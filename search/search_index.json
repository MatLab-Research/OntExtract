{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"OntExtract Documentation","text":"<p>Welcome to the OntExtract user manual.</p>"},{"location":"#about-ontextract","title":"About OntExtract","text":"<p>OntExtract provides a unified interface for document processing with integrated provenance tracking. PROV-O provenance concepts are embedded directly in the database schema, and each processing operation creates a versioned output with corresponding provenance records.</p> <p></p> <p>The system operates in two modes:</p> <ul> <li>Standalone mode uses established NLP libraries (spaCy, NLTK, sentence-transformers) without external API dependencies</li> <li>API-enhanced mode adds LLM orchestration for automated tool selection and cross-document synthesis</li> </ul> <p>Users can apply different processing strategies to the same documents and compare results while the system tracks complete analytical provenance.</p>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Getting Started - Installation and initial configuration</li> <li>First Login - Initial setup after installation</li> <li>FAQ - Frequently asked questions</li> </ul>"},{"location":"#research-workflow","title":"Research Workflow","text":"<p>OntExtract guides you through a 6-step workflow for semantic change analysis:</p> Step Task Guide 1 Define Terms - Create anchor terms to track semantic evolution Create Anchor Terms 2 Upload Sources - Add documents from different historical periods Upload Documents 3 Create Experiment - Link terms to document sets with temporal periods Create Temporal Experiment 4 LLM Orchestration - AI suggests processing pipelines LLM Orchestration 5 Execute Pipeline - Process documents with selected tools Process Documents 6 View Results - Explore semantic drift and provenance graphs View Results"},{"location":"#core-features","title":"Core Features","text":""},{"location":"#document-management","title":"Document Management","text":"<p>Upload and manage historical documents with automatic metadata extraction from Semantic Scholar and CrossRef. Supports PDF, plain text, Word, and HTML formats.</p>"},{"location":"#anchor-terms","title":"Anchor Terms","text":"<p>Define key concepts to track across your document corpus. Anchor terms serve as reference points for analyzing semantic change over time.</p>"},{"location":"#temporal-evolution-analysis","title":"Temporal Evolution Analysis","text":"<p>Track how term meanings change across historical periods using timeline visualizations and ontology-backed semantic change events.</p>"},{"location":"#document-processing","title":"Document Processing","text":"<ul> <li>LLM Text Cleanup - Fix OCR errors and formatting issues using Claude</li> <li>Segmentation - Split documents into paragraphs or sentences</li> <li>Embeddings - Generate vector representations for similarity analysis</li> <li>Entity Extraction - Identify named entities and concepts</li> <li>Definition Extraction - Extract term definitions using pattern matching with strict validation</li> <li>Temporal Extraction - Find dates, periods, and historical markers</li> </ul>"},{"location":"#llm-orchestration","title":"LLM Orchestration","text":"<p>In API-enhanced mode, the LLM analyzes your experiment and recommends processing strategies through a 5-stage workflow: Analyze \u2192 Recommend \u2192 Review \u2192 Execute \u2192 Synthesize.</p>"},{"location":"#ontology-informed-design","title":"Ontology-Informed Design","text":"<p>Event types derived from a Semantic Change Ontology with 34 classes based on existing terminology from the literature.</p>"},{"location":"#provenance-tracking","title":"Provenance Tracking","text":"<p>Complete W3C PROV-O provenance capture for all analysis steps. Every processing operation creates versioned outputs with queryable provenance chains.</p>"},{"location":"#getting-help","title":"Getting Help","text":"<ul> <li>Check the FAQ for common questions</li> <li>Report issues at GitHub</li> </ul>"},{"location":"#about-this-documentation","title":"About This Documentation","text":"<p>This manual covers installation, configuration, and usage of OntExtract features. Pages are organized by task and feature area.</p>"},{"location":"faq/","title":"Frequently Asked Questions","text":"<p>Common questions about OntExtract.</p>"},{"location":"faq/#general","title":"General","text":""},{"location":"faq/#what-is-ontextract","title":"What is OntExtract?","text":"<p>OntExtract is a document processing system with integrated provenance tracking. It operates in two modes: standalone mode uses established NLP libraries without external dependencies, while API-enhanced mode adds LLM orchestration for automated tool selection.</p>"},{"location":"faq/#do-i-need-an-api-key-to-use-ontextract","title":"Do I need an API key to use OntExtract?","text":"<p>No. Core features work without an API key:</p> <ul> <li>Document upload and management</li> <li>Segmentation (paragraph, sentence, semantic)</li> <li>Entity extraction (spaCy)</li> <li>Temporal expression extraction</li> <li>Embedding generation (local sentence-transformers)</li> <li>PROV-O provenance tracking</li> </ul> <p>LLM-enhanced features require an Anthropic API key:</p> <ul> <li>LLM text cleanup (OCR correction)</li> <li>Automated tool orchestration</li> <li>Cross-document synthesis</li> </ul>"},{"location":"faq/#what-are-the-two-operational-modes","title":"What are the two operational modes?","text":"<p>Standalone Mode: All document processing uses local NLP libraries (spaCy, NLTK, sentence-transformers). No external API calls required.</p> <p>API-Enhanced Mode: Adds LLM orchestration through a 5-stage workflow: Analyze \u2192 Recommend \u2192 Review \u2192 Execute \u2192 Synthesize. The LLM recommends tools and synthesizes results, but human review is required before execution.</p>"},{"location":"faq/#document-processing","title":"Document Processing","text":""},{"location":"faq/#what-processing-operations-are-available","title":"What processing operations are available?","text":"Operation Description Mode LLM Text Cleanup Fix OCR errors and normalize text API-enhanced Segmentation Split into paragraphs/sentences Standalone Embeddings Generate vectors for similarity Both Entity Extraction Identify people, places, orgs Standalone Temporal Extraction Find dates and periods Standalone Definition Extraction Pattern matching with strict validation for definitions and acronyms Standalone"},{"location":"faq/#does-processing-modify-my-original-documents","title":"Does processing modify my original documents?","text":"<p>No. OntExtract preserves original documents unchanged. All processing results are stored as separate ProcessingArtifacts linked to source documents through PROV-O relationships.</p>"},{"location":"faq/#what-is-prov-o-provenance","title":"What is PROV-O provenance?","text":"<p>PROV-O is the W3C standard for representing provenance information. OntExtract embeds PROV-O concepts directly in the database, tracking:</p> <ul> <li>Which tools processed each document (wasAssociatedWith)</li> <li>How artifacts were generated (wasGeneratedBy)</li> <li>What source documents were used (wasDerivedFrom)</li> </ul> <p>This enables complete reproducibility\u2014you can trace any result back to its source.</p>"},{"location":"faq/#experiments","title":"Experiments","text":""},{"location":"faq/#what-is-a-temporal-evolution-experiment","title":"What is a temporal evolution experiment?","text":"<p>Temporal evolution experiments analyze how term meanings change over time. You define anchor terms (key concepts to track) and upload historical documents spanning your time range. The system processes documents and organizes results by temporal period.</p>"},{"location":"faq/#how-are-documents-assigned-to-periods","title":"How are documents assigned to periods?","text":"<p>Documents are assigned to temporal periods based on their publication date metadata. Ensure each document has a publication date when uploading.</p>"},{"location":"faq/#troubleshooting","title":"Troubleshooting","text":""},{"location":"faq/#processing-operations-arent-running","title":"Processing operations aren't running","text":"<ul> <li>Verify Celery worker is running</li> <li>Check Redis connection</li> <li>Review application logs for errors</li> </ul>"},{"location":"faq/#no-results-after-processing","title":"No results after processing","text":"<ul> <li>Ensure document has text content (not image-only PDF)</li> <li>Verify processing completed without errors</li> <li>Check the Processing Artifacts tab on document detail page</li> </ul>"},{"location":"faq/#llm-features-not-working","title":"LLM features not working","text":"<ul> <li>Verify Anthropic API key is configured in settings</li> <li>Check API key has sufficient quota</li> <li>Review error messages in the interface</li> </ul>"},{"location":"getting-started/first-login/","title":"First Login","text":"<p>This guide covers your first login and navigating the OntExtract interface.</p>"},{"location":"getting-started/first-login/#logging-in","title":"Logging In","text":""},{"location":"getting-started/first-login/#docker-installation","title":"Docker Installation","text":"<p>If you used Docker, a default admin account is created automatically:</p> <ul> <li>Username: <code>admin</code></li> <li>Password: <code>admin123</code></li> </ul> <p>Navigate to http://localhost:8765 and click Login in the top navigation.</p> <p></p>"},{"location":"getting-started/first-login/#creating-a-new-account","title":"Creating a New Account","text":"<p>If you need to create your own account:</p> <ol> <li>Click Register in the top navigation</li> <li>Enter your username, email, and password</li> <li>Click Create Account</li> </ol>"},{"location":"getting-started/first-login/#interface-overview","title":"Interface Overview","text":"<p>After logging in, you'll see the home page with the research workflow.</p> <p></p>"},{"location":"getting-started/first-login/#main-navigation","title":"Main Navigation","text":"<p>The top navigation bar provides access to:</p> Menu Item Description Library Sources (documents and references) and Anchor Terms Experiments Your analysis experiments Provenance Timeline and graph views of processing history Linked Data Ontology and semantic resources Docs This documentation"},{"location":"getting-started/first-login/#quick-action-buttons","title":"Quick Action Buttons","text":"<p>The home page displays quick action buttons:</p> <ul> <li>Add Term - Create a new anchor term</li> <li>Add Source - Upload a document</li> <li>Experiments - View or create experiments</li> <li>Results - Access analysis results</li> </ul>"},{"location":"getting-started/first-login/#research-workflow","title":"Research Workflow","text":"<p>The 6-step workflow cards guide you through the analysis process:</p> <ol> <li>Define Terms - Create anchor terms to track</li> <li>Upload Sources - Add documents from different periods</li> <li>Create Experiment - Link terms to documents</li> <li>LLM Orchestration - AI suggests processing strategies</li> <li>Execute Pipeline - Process documents with selected tools</li> <li>View Results - Explore findings and provenance</li> </ol>"},{"location":"getting-started/first-login/#operational-modes","title":"Operational Modes","text":"<p>OntExtract operates in two modes depending on your configuration:</p>"},{"location":"getting-started/first-login/#standalone-mode","title":"Standalone Mode","text":"<p>Available without any API keys. Features include:</p> <ul> <li>Manual tool selection</li> <li>Entity extraction (spaCy)</li> <li>Temporal analysis (NLTK)</li> <li>Text segmentation</li> <li>Embedding generation (sentence-transformers)</li> <li>PROV-O provenance tracking</li> </ul>"},{"location":"getting-started/first-login/#api-enhanced-mode","title":"API-Enhanced Mode","text":"<p>Enabled when an Anthropic API key is configured. Additional features:</p> <ul> <li>LLM-powered tool recommendations</li> <li>Automated document analysis</li> <li>Cross-document synthesis</li> <li>LLM text cleanup for OCR errors</li> </ul> <p>To enable API-enhanced mode, add your API key in Settings or via environment variable.</p>"},{"location":"getting-started/first-login/#recommended-first-steps","title":"Recommended First Steps","text":""},{"location":"getting-started/first-login/#1-explore-the-sample-experiment","title":"1. Explore the Sample Experiment","text":"<p>If available, browse the pre-loaded \"Agent Temporal Evolution\" experiment to see:</p> <ul> <li>How documents are organized by period</li> <li>Processing artifacts generated</li> <li>Provenance tracking in action</li> </ul>"},{"location":"getting-started/first-login/#2-upload-a-test-document","title":"2. Upload a Test Document","text":"<ol> <li>Click Add Source or navigate to Library &gt; Documents</li> <li>Upload a PDF or text file</li> <li>Review the automatic metadata extraction</li> </ol>"},{"location":"getting-started/first-login/#3-create-an-anchor-term","title":"3. Create an Anchor Term","text":"<ol> <li>Navigate to Library &gt; Anchor Terms</li> <li>Click Add New Anchor Term</li> <li>Enter a term like \"agent\" or \"intelligence\"</li> <li>Assign a domain (e.g., \"Artificial Intelligence\")</li> </ol>"},{"location":"getting-started/first-login/#4-create-your-first-experiment","title":"4. Create Your First Experiment","text":"<ol> <li>Go to Experiments &gt; New Experiment</li> <li>Enter a name and description</li> <li>Select documents to include</li> <li>Choose your anchor terms</li> <li>Save and start processing</li> </ol>"},{"location":"getting-started/first-login/#getting-help","title":"Getting Help","text":"<ul> <li>Documentation: Click Docs in the navigation</li> <li>FAQ: Check the FAQ for common questions</li> <li>Issues: Report problems on GitHub</li> </ul>"},{"location":"getting-started/first-login/#next-steps","title":"Next Steps","text":"<ul> <li>Manage Your Library</li> <li>Upload Documents</li> <li>Create Anchor Terms</li> <li>Create Temporal Experiment</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide covers installing OntExtract on your local machine.</p>"},{"location":"getting-started/installation/#quick-start-options","title":"Quick Start Options","text":"Method Best For Time Live Demo Trying it out without installation Instant Docker Local development, most users 5 minutes Manual Contributors, custom deployments 30+ minutes"},{"location":"getting-started/installation/#option-1-live-demo","title":"Option 1: Live Demo","text":"<p>Access the live system at https://ontextract.ontorealm.net</p> <ul> <li>Demo credentials: <code>demo</code> / <code>demo123</code></li> <li>Pre-loaded experiment with sample documents</li> <li>No installation required</li> </ul>"},{"location":"getting-started/installation/#option-2-docker-recommended","title":"Option 2: Docker (Recommended)","text":"<p>Docker provides the fastest path to a working local installation.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker Engine 20.10+</li> <li>Docker Compose 2.0+</li> <li>4GB RAM minimum</li> <li>10GB free disk space</li> </ul> <p>Windows: Docker Desktop with WSL2 integration enabled</p> <p>macOS: Docker Desktop</p> <p>Linux: Docker Engine + Docker Compose plugin</p>"},{"location":"getting-started/installation/#quick-start","title":"Quick Start","text":"<pre><code>cd OntExtract\ndocker-compose up -d\n</code></pre> <p>This starts all services: - Web application on http://localhost:8765 - PostgreSQL database - Redis cache - Celery background workers</p>"},{"location":"getting-started/installation/#default-login","title":"Default Login","text":"<ul> <li>Username: <code>admin</code></li> <li>Password: <code>admin123</code></li> </ul> <p>Change the password after first login in production!</p>"},{"location":"getting-started/installation/#enable-llm-features-optional","title":"Enable LLM Features (Optional)","text":"<p>For AI-assisted orchestration, create <code>.env.local</code>:</p> <pre><code>cp .env.docker .env.local\n</code></pre> <p>Add your API key:</p> <pre><code>ANTHROPIC_API_KEY=sk-ant-your-key-here\n</code></pre> <p>Restart services:</p> <pre><code>docker-compose up -d\n</code></pre>"},{"location":"getting-started/installation/#common-commands","title":"Common Commands","text":"<pre><code># View logs\ndocker-compose logs -f\n\n# Stop services (keeps data)\ndocker-compose stop\n\n# Stop and remove containers (keeps data volumes)\ndocker-compose down\n\n# Stop and remove everything including data\ndocker-compose down -v\n\n# Rebuild after code changes\ndocker-compose up -d --build\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting-docker","title":"Troubleshooting Docker","text":"<p>Port already in use: <pre><code># Stop conflicting local services\nsudo systemctl stop postgresql redis-server\n</code></pre></p> <p>Container won't start: <pre><code>docker-compose logs web\ndocker-compose restart web\n</code></pre></p> <p>Database connection issues: <pre><code>docker-compose exec postgres pg_isready -U postgres\ndocker-compose exec web flask db upgrade\n</code></pre></p>"},{"location":"getting-started/installation/#option-3-manual-installation","title":"Option 3: Manual Installation","text":"<p>For advanced users who need to modify the code or can't use Docker.</p>"},{"location":"getting-started/installation/#system-requirements","title":"System Requirements","text":"<ul> <li>Python 3.12+</li> <li>PostgreSQL 14+ with pgvector extension</li> <li>Redis 6+</li> <li>2GB RAM minimum</li> </ul>"},{"location":"getting-started/installation/#ubuntudebian-setup","title":"Ubuntu/Debian Setup","text":"<pre><code># Install system dependencies\nsudo apt-get update\nsudo apt-get install -y postgresql-14 postgresql-14-pgvector redis-server python3.12 python3.12-venv\n\n# Create database\nsudo -u postgres psql -c \"CREATE DATABASE ontextract_db;\"\nsudo -u postgres psql -d ontextract_db -c \"CREATE EXTENSION vector;\"\n</code></pre>"},{"location":"getting-started/installation/#python-environment","title":"Python Environment","text":"<pre><code># Create virtual environment\npython3.12 -m venv venv\nsource venv/bin/activate\n\n# Install dependencies\npip install -r requirements.txt\npython -m spacy download en_core_web_sm\n</code></pre>"},{"location":"getting-started/installation/#configuration","title":"Configuration","text":"<pre><code># Copy environment template\ncp .env.example .env\n\n# Edit with your settings\nnano .env\n</code></pre> <p>Key settings in <code>.env</code>:</p> <pre><code>DATABASE_URL=postgresql://postgres:password@localhost:5432/ontextract_db\nREDIS_URL=redis://localhost:6379/0\nSECRET_KEY=your-secret-key-here\nANTHROPIC_API_KEY=sk-ant-your-key-here  # Optional\n</code></pre>"},{"location":"getting-started/installation/#initialize-database","title":"Initialize Database","text":"<pre><code>flask db upgrade\nflask seed-defaults   # Seed prompt templates and settings\npython init_admin.py  # Create admin user\n</code></pre>"},{"location":"getting-started/installation/#start-services","title":"Start Services","text":"<p>You need three terminal sessions:</p> <pre><code># Terminal 1: Redis (if not running as service)\nredis-server\n\n# Terminal 2: Celery worker\ncelery -A celery_config.celery worker --loglevel=info\n\n# Terminal 3: Flask application\npython run.py\n</code></pre> <p>Access at http://localhost:8765</p>"},{"location":"getting-started/installation/#resource-requirements","title":"Resource Requirements","text":""},{"location":"getting-started/installation/#minimum","title":"Minimum","text":"<ul> <li>CPU: 2 cores</li> <li>RAM: 4GB</li> <li>Disk: 10GB</li> </ul>"},{"location":"getting-started/installation/#recommended","title":"Recommended","text":"<ul> <li>CPU: 4+ cores</li> <li>RAM: 8GB+</li> <li>Disk: 20GB+</li> </ul>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>After installation:</p> <ol> <li>First Login - Create your account and explore the interface</li> <li>Create Anchor Terms - Define concepts to track</li> <li>Upload Documents - Add your source materials</li> <li>Create Experiment - Set up your first analysis</li> </ol>"},{"location":"how-to/create-anchor-terms/","title":"How to Create Anchor Terms","text":"<p>This guide explains how to create and manage anchor terms for tracking semantic evolution.</p>"},{"location":"how-to/create-anchor-terms/#what-are-anchor-terms","title":"What Are Anchor Terms?","text":"<p>Anchor terms are key concepts to be tracked across historical periods. They serve as current/modern reference points or \"mile markers\" for semantic change analysis.</p> <p>Examples: - \"agent\" in AI/philosophy contexts - \"machine\" in industrial/computing contexts - \"intelligence\" across cognitive science literature</p>"},{"location":"how-to/create-anchor-terms/#creating-an-anchor-term","title":"Creating an Anchor Term","text":"<p>OntExtract offers two ways to create anchor terms: using Connected Services (recommended) or Manual Entry.</p>"},{"location":"how-to/create-anchor-terms/#method-1-connected-services-recommended","title":"Method 1: Connected Services (Recommended)","text":"<p>Use authoritative lexicographic sources to establish well-documented anchor terms.</p> <p></p> <ol> <li>Navigate to Library \u2192 Terms and click Add New Anchor Term</li> <li>Select a service to search:</li> </ol> Service Description Merriam-Webster Dictionary Authoritative American English definitions Merriam-Webster Thesaurus Synonyms and antonyms for context anchoring Oxford English Dictionary Historical English definitions with temporal data WordNet Lexical database with semantic relationships <ol> <li>Enter the search term when prompted</li> <li>Browse the results and click Use This Definition</li> <li>The form auto-populates with:</li> <li>Term text</li> <li>Meaning description</li> <li>Context anchors (related terms)</li> <li>Source citation</li> <li>Add a Research Domain if desired</li> <li>Click Create Term</li> </ol>"},{"location":"how-to/create-anchor-terms/#method-2-manual-entry","title":"Method 2: Manual Entry","text":"<p>For terms not found in dictionaries or from custom sources:</p> <ol> <li>Click the Manual Entry tab</li> <li>Fill in the term details:</li> </ol> Field Description Example Term Text The word or phrase to track \"agent\" Source Type Type of reference source Corpus, Dictionary, Standard Meaning Description Current baseline meaning \"An entity that acts autonomously...\" Context Anchors Related terms (comma-separated) \"autonomous, actor, entity\" Corpus Source Lexicographic source name \"Merriam-Webster\" Source Citation Full citation for provenance \"Merriam-Webster Dictionary, 2024\" Research Domain Subject area \"Artificial Intelligence\" Notes Additional context \"Focus on autonomous agent concept\" <ol> <li>Click Create Term</li> </ol>"},{"location":"how-to/create-anchor-terms/#context-anchors","title":"Context Anchors","text":"<p>Context anchors are related terms that help define the semantic space around an anchor term. Options include:</p> <ul> <li>Letting services auto-populate them from definitions</li> <li>Clicking the Thesaurus button to find synonyms</li> <li>Entering them manually (comma-separated)</li> </ul>"},{"location":"how-to/create-anchor-terms/#term-versions","title":"Term Versions","text":"<p>Anchor terms support temporal versioning to capture meaning changes over time. Versions are created through:</p> <ul> <li>OED Timeline - Select historical waypoints showing when new senses emerged</li> <li>Manual versioning - Add versions with different temporal periods</li> </ul> <p>Coming Soon</p> <p>OED Timeline visualization for selecting historical waypoints is planned for an upcoming release.</p>"},{"location":"how-to/create-anchor-terms/#best-practices","title":"Best Practices","text":""},{"location":"how-to/create-anchor-terms/#choosing-terms","title":"Choosing Terms","text":"<ul> <li>Specificity: Choose terms specific enough to track meaningfully</li> <li>Frequency: Select terms that appear across the document corpus</li> <li>Evolution potential: Pick terms likely to show semantic change</li> </ul>"},{"location":"how-to/create-anchor-terms/#term-naming","title":"Term Naming","text":"<ul> <li>Use the canonical/base form (e.g., \"agent\" not \"agents\")</li> <li>Be consistent with capitalization</li> <li>Include phrases if tracking multi-word concepts (\"artificial intelligence\")</li> </ul>"},{"location":"how-to/create-anchor-terms/#domain-classification","title":"Domain Classification","text":"<p>Assign appropriate domains to help organize terms: - Philosophy - Computer Science - Artificial Intelligence - Cognitive Science - (Custom domains as needed)</p>"},{"location":"how-to/create-anchor-terms/#viewing-term-details","title":"Viewing Term Details","text":"<p>Click any term to see:</p> <ul> <li>Basic Information - Term text, meaning description, research domain</li> <li>Source Information - Corpus source and citation</li> <li>Context Anchors - Related terms for semantic anchoring</li> <li>Temporal Versions - Historical meaning snapshots (if created)</li> <li>Associated Experiments - Where the term is used</li> </ul> <p></p>"},{"location":"how-to/create-anchor-terms/#editing-terms","title":"Editing Terms","text":"<ol> <li>Navigate to the term detail page</li> <li>Click Edit Term</li> <li>Modify fields as needed</li> <li>Click Save Changes</li> </ol> <p>Note: Editing a term affects all experiments using it.</p>"},{"location":"how-to/create-anchor-terms/#deleting-terms","title":"Deleting Terms","text":"<p>To delete a term:</p> <ol> <li>Navigate to Library \u2192 Terms</li> <li>Find the term in the list</li> <li>Click the trash icon on the right side of the row</li> <li>Confirm the deletion in the modal</li> </ol> <p>Warning: Deleting a term removes all temporal versions and associated data permanently.</p>"},{"location":"how-to/create-anchor-terms/#term-search","title":"Term Search","text":"<p>Find existing terms using:</p> <ul> <li>Search box - Search by term text</li> <li>Domain filter - Filter by research domain</li> </ul>"},{"location":"how-to/create-anchor-terms/#related-guides","title":"Related Guides","text":"<ul> <li>Upload Documents</li> <li>Process Documents</li> <li>Create Temporal Experiment</li> </ul>"},{"location":"how-to/create-temporal-experiment/","title":"How to Create a Temporal Evolution Experiment","text":"<p>Complete guide for setting up experiments to track semantic change across historical periods.</p>"},{"location":"how-to/create-temporal-experiment/#overview","title":"Overview","text":"<p>Temporal evolution experiments analyze how term meanings change over time by combining historical documents with anchor terms across defined time periods.</p>"},{"location":"how-to/create-temporal-experiment/#prerequisites","title":"Prerequisites","text":"<p>Before creating an experiment:</p> <ul> <li>[ ] OntExtract installed and running</li> <li>[ ] User account created and logged in</li> <li>[ ] Historical documents ready for upload (spanning the target time range)</li> </ul>"},{"location":"how-to/create-temporal-experiment/#step-1-create-anchor-terms","title":"Step 1: Create Anchor Terms","text":"<p>First, create anchor terms to define the concepts to be tracked:</p> <ol> <li>Navigate to Library \u2192 Terms</li> <li>Click Add New Anchor Term</li> <li>Enter:</li> <li>Term text - The word/phrase to track (e.g., \"agent\")</li> <li>Domain - Subject area</li> <li>Notes - Research context</li> </ol> <p>See Create Anchor Terms for detailed instructions.</p>"},{"location":"how-to/create-temporal-experiment/#step-2-create-the-experiment","title":"Step 2: Create the Experiment","text":""},{"location":"how-to/create-temporal-experiment/#navigate-to-experiments","title":"Navigate to Experiments","text":"<ol> <li>Click Experiments in the main navigation</li> <li>Click New Experiment</li> </ol>"},{"location":"how-to/create-temporal-experiment/#fill-in-experiment-details","title":"Fill in Experiment Details","text":"Field Description Example Name Descriptive experiment name \"Agent Temporal Evolution 1910-2024\" Description Research goals and scope \"Tracking the semantic evolution of 'agent' in AI literature\" Start Year Beginning of time range 1910 End Year End of time range 2024 Status Experiment state draft, active, completed"},{"location":"how-to/create-temporal-experiment/#temporal-periods","title":"Temporal Periods","text":"<p>OntExtract automatically generates temporal periods based on the date range. Options include:</p> <ul> <li>Auto-generate - System creates periods based on document dates</li> <li>Manual - Define custom period boundaries</li> </ul> <p>Click Create Experiment to save.</p>"},{"location":"how-to/create-temporal-experiment/#manage-temporal-terms","title":"Manage Temporal Terms","text":"<p>After creating a temporal evolution experiment, access the Manage Temporal Terms feature to configure the timeline in detail.</p>"},{"location":"how-to/create-temporal-experiment/#accessing-the-manager","title":"Accessing the Manager","text":"<ol> <li>Go to the experiment's detail page</li> <li>Click Manage Temporal Terms button</li> </ol>"},{"location":"how-to/create-temporal-experiment/#timeline-configuration","title":"Timeline Configuration","text":"<p>The Temporal Term Manager provides two ways to set up periods:</p> Method Description Auto-generate from documents Creates artifact markers for each document's publication year Manual Entry Manually specify time period boundaries"},{"location":"how-to/create-temporal-experiment/#adding-semantic-events","title":"Adding Semantic Events","text":"<p>The timeline can be annotated with semantic change events:</p> <ol> <li>Click Add Event in the Periods &amp; Events section</li> <li>Select the event type (e.g., amelioration, pejoration, drift)</li> <li>Specify the time range (from/to periods)</li> <li>Add a description of the semantic shift</li> <li>Link related documents as evidence</li> </ol>"},{"location":"how-to/create-temporal-experiment/#period-cards","title":"Period Cards","text":"<p>The timeline displays period cards showing:</p> <ul> <li>Year - The period's date marker</li> <li>Source badge - ARTIFACT (auto-generated) or MANUAL</li> <li>Documents - Papers associated with that period</li> <li>Events - Semantic change events spanning periods</li> </ul> <p>Period boundaries are color-coded:</p> <ul> <li>Green (START) - Beginning of a defined period</li> <li>Red (END) - End of a defined period</li> </ul>"},{"location":"how-to/create-temporal-experiment/#saving-configuration","title":"Saving Configuration","text":"<p>Click Save Configuration to persist the temporal setup before proceeding to analysis.</p>"},{"location":"how-to/create-temporal-experiment/#step-3-add-documents","title":"Step 3: Add Documents","text":""},{"location":"how-to/create-temporal-experiment/#upload-documents","title":"Upload Documents","text":"<ol> <li>Go to the experiment's Document Pipeline</li> <li>Click Add Documents</li> <li>Upload files with publication dates</li> <li>Documents are automatically assigned to periods based on publication date</li> </ol>"},{"location":"how-to/create-temporal-experiment/#document-requirements","title":"Document Requirements","text":"<p>For meaningful temporal analysis:</p> <ul> <li>Multiple documents per period - More data improves accuracy</li> <li>Date coverage - Documents spanning the full time range</li> <li>Consistent domain - Documents from related subject areas</li> </ul> <p>See Upload Documents for detailed upload instructions.</p>"},{"location":"how-to/create-temporal-experiment/#step-4-process-documents","title":"Step 4: Process Documents","text":""},{"location":"how-to/create-temporal-experiment/#llm-text-cleanup-recommended","title":"LLM Text Cleanup (Recommended)","text":"<p>For scanned or OCR'd documents:</p> <ol> <li>In Document Pipeline, click the broom icon for each document</li> <li>Review suggested corrections</li> <li>Accept or modify changes</li> <li>Save cleaned version</li> </ol>"},{"location":"how-to/create-temporal-experiment/#run-processing-operations","title":"Run Processing Operations","text":"<p>From the Document Pipeline or individual document pages:</p> <ol> <li>Segmentation - Split into paragraphs/sentences</li> <li>Embeddings - Generate vector representations</li> <li>Entity Extraction - Identify named entities</li> </ol> <p>Use Run Local Tools for batch processing without API costs.</p>"},{"location":"how-to/create-temporal-experiment/#step-5-run-analysis","title":"Step 5: Run Analysis","text":""},{"location":"how-to/create-temporal-experiment/#llm-orchestration-advanced","title":"LLM Orchestration (Advanced)","text":"<p>For AI-assisted analysis:</p> <ol> <li>Go to the experiment's Document Pipeline</li> <li>Click Start LLM Orchestration</li> <li>Review the generated strategy</li> <li>Approve or modify the approach</li> <li>Execute the analysis</li> </ol>"},{"location":"how-to/create-temporal-experiment/#manual-analysis","title":"Manual Analysis","text":"<p>Explore results through:</p> <ul> <li>Timeline View - Visual evolution across periods</li> <li>Document Comparison - Side-by-side period analysis</li> <li>Term Context - Source passages for each period</li> </ul>"},{"location":"how-to/create-temporal-experiment/#step-6-review-results","title":"Step 6: Review Results","text":""},{"location":"how-to/create-temporal-experiment/#timeline-visualization","title":"Timeline Visualization","text":"<p>The timeline shows: - Term usage frequency per period - Semantic change events - Context snippets from source documents</p>"},{"location":"how-to/create-temporal-experiment/#export-options","title":"Export Options","text":"<p>Export analysis results:</p> <ul> <li>CSV - Tabular data for spreadsheets</li> <li>JSON - Structured results for further processing</li> </ul>"},{"location":"how-to/create-temporal-experiment/#troubleshooting","title":"Troubleshooting","text":""},{"location":"how-to/create-temporal-experiment/#no-documents-in-period","title":"No Documents in Period","text":"<ul> <li>Check document publication dates</li> <li>Verify period boundaries</li> <li>Upload additional documents for sparse periods</li> </ul>"},{"location":"how-to/create-temporal-experiment/#processing-errors","title":"Processing Errors","text":"<ul> <li>Ensure documents have text content</li> <li>Run LLM cleanup on problematic documents</li> <li>Check API keys for external services</li> </ul>"},{"location":"how-to/create-temporal-experiment/#missing-analysis-results","title":"Missing Analysis Results","text":"<ul> <li>Verify all processing steps completed</li> <li>Check anchor term associations</li> <li>Review experiment status</li> </ul>"},{"location":"how-to/create-temporal-experiment/#related-guides","title":"Related Guides","text":"<ul> <li>Upload Documents</li> <li>Process Documents</li> <li>Create Anchor Terms</li> </ul>"},{"location":"how-to/document-processing/","title":"How to Process Documents","text":"<p>This guide covers the document processing operations available in OntExtract.</p>"},{"location":"how-to/document-processing/#overview","title":"Overview","text":"<p>After uploading documents, various processing operations can be applied to extract structured information. OntExtract preserves original documents unchanged\u2014all results are stored as ProcessingArtifacts linked to source documents through PROV-O relationships.</p>"},{"location":"how-to/document-processing/#processing-operations","title":"Processing Operations","text":"Operation Purpose Mode LLM Text Cleanup Fix OCR errors, normalize spelling API-enhanced Segmentation Split into paragraphs or sentences Standalone Embeddings Generate vector representations Both Entity Extraction Identify people, places, organizations Standalone Temporal Extraction Find dates, periods, durations Standalone Definition Extraction Locate concept definitions Standalone"},{"location":"how-to/document-processing/#llm-text-cleanup","title":"LLM Text Cleanup","text":"<p>Use this for scanned or OCR'd historical documents with recognition errors.</p>"},{"location":"how-to/document-processing/#when-to-use","title":"When to Use","text":"<ul> <li>Documents with OCR character errors (rn \u2192 m, l \u2192 I)</li> <li>Archaic spelling that needs normalization</li> <li>Scanning artifacts (headers, page numbers in text)</li> </ul>"},{"location":"how-to/document-processing/#how-to-run","title":"How to Run","text":"<ol> <li>Navigate to the document detail page</li> <li>Click the menu button (three dots) in the top-right</li> <li>Select Clean with LLM</li> <li>Review the suggested corrections</li> <li>Accept or modify changes</li> <li>Save the cleaned version</li> </ol> <p>Note: LLM cleanup creates a new document version. The original is preserved. This operation requires an Anthropic API key configured in settings.</p>"},{"location":"how-to/document-processing/#from-document-pipeline","title":"From Document Pipeline","text":"<p>Cleanup can also be triggered from the experiment's Document Pipeline:</p> <ol> <li>Go to Experiments &gt; Select experiment &gt; Document Pipeline</li> <li>Click the broom icon next to any document</li> <li>Follow the cleanup workflow</li> </ol>"},{"location":"how-to/document-processing/#segmentation","title":"Segmentation","text":"<p>Split documents into logical sections for analysis.</p>"},{"location":"how-to/document-processing/#segmentation-methods","title":"Segmentation Methods","text":"Method Description Best For Paragraph NLTK-enhanced paragraph detection Most documents Sentence NLTK Punkt tokenizer Fine-grained analysis"},{"location":"how-to/document-processing/#how-to-run_1","title":"How to Run","text":"<ol> <li>Go to the document or experiment's Document Pipeline</li> <li>Select documents to process</li> <li>Check Segmentation in Processing Operations</li> <li>Choose a segmentation method</li> <li>Click Run Selected Tools</li> </ol>"},{"location":"how-to/document-processing/#results","title":"Results","text":"<p>Segmentation creates TextSegment artifacts with: - Segment text content - Character-level position (start/end offsets) - Segment index within document</p>"},{"location":"how-to/document-processing/#auto-dependency","title":"Auto-Dependency","text":"<p>When selecting Embeddings or Definition Extraction, the system automatically selects Paragraph Segmentation if it hasn't been run. This is because:</p> <ul> <li>Embeddings create segment-level vectors when segments exist (more granular similarity search)</li> <li>All extraction tools produce better results with structured text segments</li> </ul> <p>Segmentation can be deselected for document-level processing only.</p>"},{"location":"how-to/document-processing/#embedding-generation","title":"Embedding Generation","text":"<p>Create vector representations for semantic similarity search.</p>"},{"location":"how-to/document-processing/#embedding-methods","title":"Embedding Methods","text":"Method Description Best For Local Standard sentence-transformers model General modern text Period Aware Selects model based on document era/domain Historical or domain-specific text OpenAI text-embedding-3-large (3072 dims) Highest accuracy (requires API key)"},{"location":"how-to/document-processing/#how-to-run_2","title":"How to Run","text":"<ol> <li>Go to Document Pipeline or document detail</li> <li>Check Embeddings in Processing Operations</li> <li>Select embedding method</li> <li>Click Run Selected Tools</li> </ol>"},{"location":"how-to/document-processing/#period-aware-embeddings","title":"Period-Aware Embeddings","text":"<p>For historical documents or specialized domains, use Period Aware embeddings. This automatically selects appropriate models based on:</p> <ul> <li>Document publication date</li> <li>Domain (scientific, legal, biomedical)</li> <li>Detected archaic language patterns</li> </ul> <p>See Period-Aware Embeddings for detailed information.</p>"},{"location":"how-to/document-processing/#results_1","title":"Results","text":"<p>Embeddings enable: - Semantic similarity search across segments - Finding related passages across documents - Clustering similar content</p> <p>Vectors are stored in PostgreSQL using pgvector for efficient similarity queries.</p>"},{"location":"how-to/document-processing/#entity-extraction","title":"Entity Extraction","text":"<p>Identify named entities using spaCy NLP models.</p>"},{"location":"how-to/document-processing/#entity-types","title":"Entity Types","text":"<ul> <li>PERSON - People, including fictional</li> <li>ORG - Organizations, companies, agencies</li> <li>GPE - Geopolitical entities (countries, cities)</li> <li>DATE - Dates and periods</li> <li>WORK_OF_ART - Titles of works</li> </ul>"},{"location":"how-to/document-processing/#how-to-run_3","title":"How to Run","text":"<ol> <li>Select documents in Document Pipeline</li> <li>Check Entity Extraction in Processing Operations</li> <li>Click Run Selected Tools</li> </ol>"},{"location":"how-to/document-processing/#results_2","title":"Results","text":"<p>Entity extraction creates artifacts with: - Entity text and type - Character positions in source - Confidence scores</p> <p>Note: Accuracy depends on domain alignment with training corpora. Historical and technical texts may require validation.</p>"},{"location":"how-to/document-processing/#definition-extraction","title":"Definition Extraction","text":"<p>Extract term definitions using pattern matching with strict validation.</p>"},{"location":"how-to/document-processing/#approach","title":"Approach","text":"<p>OntExtract uses pattern matching to identify definitions in text:</p> <ol> <li>Zero-shot classification (optional, disabled by default)</li> <li>Uses <code>facebook/bart-large-mnli</code> model (~1.6GB)</li> <li>Too slow on CPU for large documents (10+ minutes per document)</li> <li>Enable with environment variable: <code>ENABLE_ZERO_SHOT_DEFINITIONS=true</code></li> <li> <p>When enabled, scores sentences for confidence boosting</p> </li> <li> <p>Pattern matching (default, fast) - Detects 8 definition types:</p> </li> <li>explicit_definition: \"X is defined as Y\"</li> <li>explicit_reference: \"X refers to Y\"</li> <li>meaning: \"X means Y\"</li> <li>copula: \"X is a Y\"</li> <li>acronym: \"IRA (Information Retrieval Agent)\" with strict validation</li> <li>also_known_as: \"X (also known as Y)\"</li> <li>ie_explanation: \"X (i.e., Y)\"</li> <li> <p>appositive: Dependency parsing for noun appositives</p> </li> <li> <p>Strict acronym validation:</p> </li> <li>Pattern: 2-6 uppercase letters with capitalized word expansion</li> <li>Requires expansion first letters to match acronym (e.g., \"IRA\" must expand to words starting with I, R, A)</li> <li>Rejects expansions containing years (likely citations)</li> <li> <p>Eliminates nonsense patterns</p> </li> <li> <p>Quality filters:</p> </li> <li>Reject academic citations (e.g., \"et al., 2015\")</li> <li>Reject reference lists (year ranges, multiple years)</li> <li>Reject terms with more than 3 words</li> <li>Length validation (10-200 characters)</li> </ol>"},{"location":"how-to/document-processing/#how-to-run_4","title":"How to Run","text":"<ol> <li>Select documents in Document Pipeline</li> <li>Check Definition Extraction in Processing Operations</li> <li>Click Run Selected Tools</li> </ol>"},{"location":"how-to/document-processing/#results_3","title":"Results","text":"<p>Definition extraction creates artifacts with: - Term being defined - Definition text - Pattern type (explicit, acronym, etc.) - Confidence score (0.65-0.90 depending on pattern) - Character positions in source document - Source sentence for context</p> <p>Results are labeled \"Auto\" in the UI with a \"Pattern\" source badge. If zero-shot is enabled, definitions may show \"ZeroShot\" badge.</p> <p>Note: Definition extraction works best on documents that explicitly define terminology, such as glossaries, textbook introductions, or standards documents. Research papers that use but do not define terms may return few or no results.</p>"},{"location":"how-to/document-processing/#batch-processing","title":"Batch Processing","text":"<p>Process multiple documents efficiently:</p> <ol> <li>Go to Experiments &gt; Select experiment &gt; Document Pipeline</li> <li>Use checkboxes to select multiple documents</li> <li>Choose operations to apply</li> <li>Click Run Selected Tools</li> </ol> <p></p> <p>Operations run in parallel where possible. Progress is tracked in the interface.</p>"},{"location":"how-to/document-processing/#processing-without-api-costs","title":"Processing Without API Costs","text":"<p>Run Local Tools processes documents using only local NLP libraries: - spaCy for entity extraction - NLTK for sentence tokenization - sentence-transformers for embeddings</p> <p>No external API calls are made, enabling offline operation.</p>"},{"location":"how-to/document-processing/#viewing-results","title":"Viewing Results","text":"<p>After processing, view results from the experiment detail page:</p> <ol> <li>Go to Experiments &gt; Select the experiment</li> <li>Expand the View Results section</li> <li>Click a result type:</li> <li>Definitions - Extracted term definitions</li> <li>Entities - Named entities and concepts</li> <li>Embeddings - Generated vectors and similarity data</li> <li>Segments - Document segments</li> <li>Temporal - Extracted dates and periods</li> </ol>"},{"location":"how-to/document-processing/#result-details","title":"Result Details","text":"<p>Each result page shows: - Extracted items grouped by document - Source text and character positions - Confidence scores and extraction method - Links back to source documents</p>"},{"location":"how-to/document-processing/#prov-o-provenance","title":"PROV-O Provenance","text":"<p>All processing operations create PROV-O provenance records:</p> <ul> <li>wasDerivedFrom - Links artifacts to source documents</li> <li>wasGeneratedBy - Connects artifacts to generating activities</li> <li>wasAssociatedWith - Maps operations to tool versions</li> </ul> <p>This enables complete reproducibility\u2014any result can be traced back to its source to understand exactly how it was generated.</p>"},{"location":"how-to/document-processing/#troubleshooting","title":"Troubleshooting","text":""},{"location":"how-to/document-processing/#processing-stuck","title":"Processing Stuck","text":"<ul> <li>Check Celery worker status</li> <li>Verify Redis connection</li> <li>Review application logs</li> </ul>"},{"location":"how-to/document-processing/#no-results-generated","title":"No Results Generated","text":"<ul> <li>Ensure document has text content</li> <li>Check that source document exists</li> <li>Verify processing completed (no errors in logs)</li> </ul>"},{"location":"how-to/document-processing/#embedding-errors","title":"Embedding Errors","text":"<ul> <li>For OpenAI: verify API key in settings</li> <li>For local: check sentence-transformers installation</li> <li>Ensure document has been segmented first</li> </ul>"},{"location":"how-to/document-processing/#related-guides","title":"Related Guides","text":"<ul> <li>Upload Documents</li> <li>Create Temporal Experiment</li> <li>Create Anchor Terms</li> </ul>"},{"location":"how-to/llm-orchestration/","title":"How to Use LLM Orchestration","text":"<p>This guide covers the AI-assisted workflow for analyzing experiments.</p>"},{"location":"how-to/llm-orchestration/#overview","title":"Overview","text":"<p>LLM Orchestration is available in API-enhanced mode (requires Anthropic API key). The system analyzes the experiment and recommends processing strategies for each document.</p>"},{"location":"how-to/llm-orchestration/#prerequisites","title":"Prerequisites","text":"<ul> <li>Anthropic API key configured in Settings</li> <li>An experiment with associated documents</li> <li>Documents should have text content extracted</li> </ul>"},{"location":"how-to/llm-orchestration/#the-5-stage-workflow","title":"The 5-Stage Workflow","text":"<p>LLM Orchestration follows a structured workflow with human review at each decision point:</p>"},{"location":"how-to/llm-orchestration/#stage-1-analyze","title":"Stage 1: Analyze","text":"<p>The LLM examines the experiment to understand:</p> <ul> <li>Research goals and scope</li> <li>Document characteristics (length, format, historical period)</li> <li>Focus terms and their domains</li> <li>Temporal range of the corpus</li> </ul>"},{"location":"how-to/llm-orchestration/#stage-2-recommend","title":"Stage 2: Recommend","text":"<p>Based on the analysis, the system recommends:</p> <ul> <li>Which tools to apply to each document</li> <li>Processing order and dependencies</li> <li>Confidence scores for each recommendation</li> <li>Rationale explaining the choices</li> </ul> <p>Example recommendations:</p> Document Recommended Tools Confidence Historical paper (1910) Entity extraction, Definition extraction 0.92 Modern technical paper Semantic segmentation, Entity extraction 0.88 Legal dictionary entry Definition extraction, Temporal extraction 0.95"},{"location":"how-to/llm-orchestration/#stage-3-review","title":"Stage 3: Review","text":"<p>Recommendations are reviewed before execution:</p> <ul> <li>Approve recommendations as-is</li> <li>Modify tool selections for specific documents</li> <li>Add processing notes</li> <li>Reject and request re-analysis</li> </ul> <p>All LLM recommendations require human approval before execution.</p>"},{"location":"how-to/llm-orchestration/#stage-4-execute","title":"Stage 4: Execute","text":"<p>After approval, the system processes documents:</p> <ul> <li>Tools run using local NLP libraries (spaCy, NLTK, sentence-transformers)</li> <li>Progress tracked in real-time</li> <li>Results stored as ProcessingArtifacts with PROV-O provenance</li> </ul> <p>Available Processing Tools:</p> <ul> <li>Entity Extraction (spaCy): Named entities (PERSON, ORG, GPE) + noun phrase concepts</li> <li>Temporal Extraction (spaCy + regex): Dates, periods, historical markers, relative expressions</li> <li>Definition Extraction (pattern matching):</li> <li>Pattern matching for 8 definition types (explicit, copula, acronym, appositive, etc.)</li> <li>Strict acronym validation requiring first-letter matching</li> <li>Quality filters to reject citations, reference lists, and nonsense patterns</li> <li>Text Segmentation: Structure-aware document splitting</li> <li>Embedding Generation (sentence-transformers): Period-aware semantic vectors</li> <li>LLM Text Cleanup (Claude): Modernize OCR errors while preserving historical terminology</li> </ul>"},{"location":"how-to/llm-orchestration/#stage-5-synthesize","title":"Stage 5: Synthesize","text":"<p>The LLM analyzes results across all documents:</p> <ul> <li>Identifies patterns and themes</li> <li>Generates term cards with frequency data</li> <li>Organizes findings by temporal period</li> <li>Does not interpret results - preserves researcher authority</li> </ul>"},{"location":"how-to/llm-orchestration/#accessing-llm-orchestration","title":"Accessing LLM Orchestration","text":"<ol> <li>Go to Experiments and select an experiment</li> <li>Click Document Pipeline</li> <li>Select LLM mode (toggle at top of page)</li> <li>Click LLM Analyze to begin Stage 1</li> </ol>"},{"location":"how-to/llm-orchestration/#workflow-states","title":"Workflow States","text":"State Description <code>not_started</code> Orchestration not yet initiated <code>analyzing</code> Stage 1 in progress <code>awaiting_approval</code> Recommendations ready for review <code>executing</code> Processing documents <code>synthesizing</code> Generating cross-document insights <code>completed</code> All stages finished <code>error</code> Processing encountered an error"},{"location":"how-to/llm-orchestration/#manual-alternative","title":"Manual Alternative","text":"<p>To process documents without LLM orchestration:</p> <ol> <li>Go to Document Pipeline in the experiment</li> <li>Select documents manually</li> <li>Choose processing operations</li> <li>Click Run Selected Tools</li> </ol> <p>Manual selections are recorded with the same PROV-O provenance structure.</p>"},{"location":"how-to/llm-orchestration/#when-to-use-llm-orchestration","title":"When to Use LLM Orchestration","text":"<ul> <li>Large document collections (10+ documents)</li> <li>Mixed document types requiring different tools</li> <li>For AI-generated synthesis of patterns across documents</li> </ul>"},{"location":"how-to/llm-orchestration/#when-to-process-manually","title":"When to Process Manually","text":"<ul> <li>Small experiments (&lt; 5 documents)</li> <li>When specific tools are already determined</li> <li>When API costs are a concern</li> </ul>"},{"location":"how-to/llm-orchestration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"how-to/llm-orchestration/#orchestration-stuck","title":"Orchestration stuck","text":"<ul> <li>Check that Celery worker is running</li> <li>Verify API key is valid and has quota</li> <li>Review application logs for errors</li> </ul>"},{"location":"how-to/llm-orchestration/#recommendations-seem-wrong","title":"Recommendations seem wrong","text":"<ul> <li>Ensure documents have metadata (especially publication date)</li> <li>Check that document text was extracted correctly</li> <li>Try providing more specific experiment description</li> </ul>"},{"location":"how-to/llm-orchestration/#related-guides","title":"Related Guides","text":"<ul> <li>Create Temporal Experiment</li> <li>Process Documents</li> <li>View Results</li> </ul>"},{"location":"how-to/manage-library/","title":"How to Manage the Library","text":"<p>This guide explains how to browse, search, and manage the document library in OntExtract.</p>"},{"location":"how-to/manage-library/#overview","title":"Overview","text":"<p>The Library section (accessible via Library \u2192 Sources in the navigation) serves as the central hub for managing all uploaded content. It displays both:</p> <ul> <li>Documents - Primary source materials for analysis (PDFs, text files, pasted content)</li> <li>References - Canonical reference materials like dictionary entries and academic papers</li> </ul>"},{"location":"how-to/manage-library/#browsing-sources","title":"Browsing Sources","text":""},{"location":"how-to/manage-library/#access-the-library","title":"Access the Library","text":"<ol> <li>Click Library in the main navigation</li> <li>Select Sources from the dropdown</li> </ol>"},{"location":"how-to/manage-library/#filter-by-type","title":"Filter by Type","text":"<p>Use the filter tabs at the top to show:</p> Tab Contents All Both documents and references Documents Primary source materials only References Reference documents only"},{"location":"how-to/manage-library/#understanding-the-list-view","title":"Understanding the List View","text":"<p>Each source card displays:</p> <ul> <li>Title - Document name (clickable to view details)</li> <li>Type badge - Document or Reference</li> <li>Version indicator - Current version number</li> <li>Bibliographic metadata - Authors, publication date, journal, DOI (if available)</li> <li>Content preview - Abstract or first few lines of content</li> </ul>"},{"location":"how-to/manage-library/#document-versions","title":"Document Versions","text":"<p>OntExtract maintains version history for processed documents.</p>"},{"location":"how-to/manage-library/#viewing-version-history","title":"Viewing Version History","text":"<ol> <li>If a document has multiple versions, click the X versions button on the card</li> <li>The version list expands showing:</li> <li>Version number</li> <li>Version type (Original, Processed)</li> <li>Creation timestamp</li> </ol>"},{"location":"how-to/manage-library/#version-types","title":"Version Types","text":"Type Description Original The initially uploaded document Processed A derived version after processing (e.g., language extraction, segmentation)"},{"location":"how-to/manage-library/#viewing-document-details","title":"Viewing Document Details","text":"<p>Click any document title or the View Latest button to see:</p> <ul> <li>Full content or file preview</li> <li>Complete bibliographic metadata</li> <li>Temporal metadata (for historical documents)</li> <li>Experiments using this document</li> <li>Processing history and artifacts</li> </ul>"},{"location":"how-to/manage-library/#document-detail-sections","title":"Document Detail Sections","text":"Section Information Overview Title, type, word count, creation date Metadata Authors, publication date, DOI, and other bibliographic fields Content Full text or file download Experiments List of experiments that include this document Processing Operations performed and their results"},{"location":"how-to/manage-library/#editing-documents","title":"Editing Documents","text":""},{"location":"how-to/manage-library/#edit-metadata","title":"Edit Metadata","text":"<ol> <li>Navigate to the document detail page</li> <li>Click Edit in the top-right</li> <li>Modify any metadata fields:</li> <li>Title</li> <li>Authors, Editor, Edition</li> <li>Publication date</li> <li>Journal, Publisher, Place</li> <li>DOI, ISBN, ISSN, URL</li> <li>Abstract, Notes</li> <li>Click Save Changes</li> </ol>"},{"location":"how-to/manage-library/#what-can-be-edited","title":"What Can Be Edited","text":"<ul> <li>All bibliographic metadata fields</li> <li>Document title</li> <li>Notes and abstract</li> </ul>"},{"location":"how-to/manage-library/#what-cannot-be-edited","title":"What Cannot Be Edited","text":"<ul> <li>Original file content (upload a new version instead)</li> <li>Processing results (re-run processing)</li> <li>Creation date and user ownership</li> </ul>"},{"location":"how-to/manage-library/#deleting-documents","title":"Deleting Documents","text":""},{"location":"how-to/manage-library/#delete-a-single-document","title":"Delete a Single Document","text":"<ol> <li>Navigate to the document detail page</li> <li>Click the menu button (three dots) in the top-right</li> <li>Select Delete</li> <li>Confirm the deletion</li> </ol> <p>Note: Documents that are part of experiments cannot be deleted directly. Remove them from experiments first, or delete the experiment.</p>"},{"location":"how-to/manage-library/#delete-all-versions","title":"Delete All Versions","text":"<p>If a document has multiple versions (original + processed):</p> <ol> <li>Navigate to any version's detail page</li> <li>Click the menu button (three dots) in the top-right</li> <li>Select Delete All Versions</li> <li>Confirm to remove all versions of this document family</li> </ol> <p>This option only appears when a document has multiple versions.</p>"},{"location":"how-to/manage-library/#delete-all-documents-admin-only","title":"Delete All Documents (Admin Only)","text":"<p>Administrators can delete all documents at once:</p> <ol> <li>Go to Library \u2192 Sources</li> <li>Click Delete All Documents</li> <li>Type <code>DELETE ALL</code> to confirm</li> <li>Click Delete Everything</li> </ol> <p>Warning: This removes all documents, files, processing results, and experiment relationships. This action cannot be undone.</p>"},{"location":"how-to/manage-library/#uploading-new-documents","title":"Uploading New Documents","text":"<p>From the Library view, click Upload Document to add new sources.</p> <p>See Upload Documents for detailed instructions.</p>"},{"location":"how-to/manage-library/#tips-for-library-management","title":"Tips for Library Management","text":""},{"location":"how-to/manage-library/#organization","title":"Organization","text":"<ul> <li>Use consistent naming conventions for document titles</li> <li>Fill in bibliographic metadata for better searchability</li> <li>Add notes to documents explaining their significance</li> </ul>"},{"location":"how-to/manage-library/#best-practices","title":"Best Practices","text":"<ul> <li>Delete test documents after experimentation</li> <li>Keep original versions for reference</li> <li>Use the type filter to focus on documents or references as needed</li> </ul>"},{"location":"how-to/manage-library/#performance","title":"Performance","text":"<ul> <li>The library paginates automatically (10 items per page)</li> <li>Large libraries may take a moment to load</li> <li>Use filters to narrow down search results</li> </ul>"},{"location":"how-to/manage-library/#related-guides","title":"Related Guides","text":"<ul> <li>Upload Documents</li> <li>Create Temporal Experiment</li> <li>Process Documents</li> </ul>"},{"location":"how-to/period-aware-embeddings/","title":"How to Use Period-Aware Embeddings","text":"<p>This guide covers OntExtract's period-aware embedding feature for historical and domain-specific text analysis.</p>"},{"location":"how-to/period-aware-embeddings/#overview","title":"Overview","text":"<p>Historical texts use different vocabulary, spelling, and linguistic patterns than contemporary texts. Using a modern embedding model on archaic text can result in poor semantic representations.</p> <p>The period-aware embedding service addresses this by:</p> <ul> <li>Selecting models trained on corpora from similar time periods</li> <li>Using domain-specific models for specialized vocabularies</li> <li>Detecting archaic language patterns when metadata is unavailable</li> </ul>"},{"location":"how-to/period-aware-embeddings/#when-to-use-period-aware-embeddings","title":"When to Use Period-Aware Embeddings","text":"<p>Use period-aware embeddings when:</p> <ul> <li>Analyzing documents spanning multiple historical periods</li> <li>Working with archaic or historical language</li> <li>Processing domain-specific texts (scientific, legal, biomedical)</li> <li>Comparing semantic similarity across time periods</li> </ul>"},{"location":"how-to/period-aware-embeddings/#model-selection","title":"Model Selection","text":"<p>The service selects embedding models based on this priority:</p> <ol> <li>Domain (if specified) - Takes precedence for specialized vocabularies</li> <li>Year (if specified) - Selects period-appropriate model</li> <li>Text Analysis (fallback) - Detects archaic/technical language patterns</li> <li>Default - Falls back to modern model</li> </ol>"},{"location":"how-to/period-aware-embeddings/#period-based-models","title":"Period-Based Models","text":"Period Era Handles Archaic Pre-1850 Pre-industrial Yes 1850-1950 Industrial Yes 1950-2000 Modern No 2000+ Contemporary No"},{"location":"how-to/period-aware-embeddings/#domain-specific-models","title":"Domain-Specific Models","text":"Domain Use Case Scientific Scientific papers, technical documentation Legal Legal documents, contracts, case law Biomedical Medical literature, clinical texts"},{"location":"how-to/period-aware-embeddings/#using-period-aware-embeddings","title":"Using Period-Aware Embeddings","text":""},{"location":"how-to/period-aware-embeddings/#from-the-document-pipeline","title":"From the Document Pipeline","text":"<ol> <li>Go to Experiments &gt; Select the experiment &gt; Document Pipeline</li> <li>Select documents to process using the checkboxes</li> <li>Under the Embeddings section, check Period-Aware Embeddings</li> <li>Click Run Selected Tools</li> </ol> <p>The service will:</p> <ul> <li>Check the document's publication date metadata</li> <li>Analyze text for archaic language patterns (if no date available)</li> <li>Select and apply the appropriate model</li> </ul>"},{"location":"how-to/period-aware-embeddings/#via-llm-orchestration","title":"Via LLM Orchestration","text":"<p>When using LLM orchestration, the system may automatically recommend period-aware embeddings for:</p> <ul> <li>Historical documents (based on publication date)</li> <li>Documents with detected archaic language</li> <li>Domain-specific technical papers</li> </ul> <p>Recommendations can be approved or modified during the Review stage.</p>"},{"location":"how-to/period-aware-embeddings/#setup-requirements","title":"Setup Requirements","text":"<p>Period-aware models must be downloaded before use. Run the download script:</p> <pre><code># Download core models (~500MB)\npython scripts/download_embedding_models.py --core\n\n# Download all models (~2GB)\npython scripts/download_embedding_models.py --all\n\n# Check download status\npython scripts/download_embedding_models.py --check\n</code></pre>"},{"location":"how-to/period-aware-embeddings/#archaic-language-detection","title":"Archaic Language Detection","text":"<p>When no publication date is available, the service uses a heuristic approach to detect archaic language, based on lexical markers established in historical linguistics research.</p>"},{"location":"how-to/period-aware-embeddings/#linguistic-basis","title":"Linguistic Basis","text":"<p>The detection approach uses two categories of markers that are well-documented in the literature on Early Modern English (c. 1500-1700):</p> <p>1. Archaic Second-Person Pronouns and Verb Forms</p> <ul> <li>thou, thee, thy, thine \u2014 The singular second-person pronoun system that fell out of standard use by the 17th century. The shift from \"thou\" to \"you\" is one of the most studied changes in English historical linguistics (see Burnley, 2000; Wales, 1996).</li> <li>hath, doth \u2014 Third-person singular verb forms with the archaic -eth ending, replaced by modern -s forms (\"has,\" \"does\") during the Early Modern period.</li> </ul> <p>2. Pronominal Adverbs</p> <ul> <li>whence, wherefore, wherein, whereby, heretofore, hereunto \u2014 These are pronominal adverbs formed from wh-/h-/th- stems combined with prepositions. They form systematic patterns (hither/thither/whither for direction-to; hence/thence/whence for direction-from) and are characteristic of both archaic and legal English.</li> </ul> <p>These markers are used in corpus normalization research for Early Modern English texts (see Archer et al., 2015, \"Guidelines for normalising Early Modern English corpora\") and are recognized as reliable indicators of historical text in computational historical linguistics.</p>"},{"location":"how-to/period-aware-embeddings/#detection-method","title":"Detection Method","text":"<p>Archaic indicators detected:</p> <ul> <li>Historical pronouns: thou, thee, thy, thine</li> <li>Archaic verbs: hath, doth</li> <li>Pronominal adverbs: whence, wherefore, wherein, whereby, heretofore, hereunto, notwithstanding</li> </ul> <p>Technical indicators detected:</p> <ul> <li>Academic vocabulary: hypothesis, methodology, parameter</li> <li>Scientific terms: coefficient, algorithm, paradigm, empirical</li> </ul> <p>If archaic language is detected, the historical model is automatically selected.</p>"},{"location":"how-to/period-aware-embeddings/#limitations","title":"Limitations","text":"<p>This is a heuristic approach based on lexical markers rather than a trained classifier. It works well for:</p> <ul> <li>Texts containing Early Modern English features (pre-1700)</li> <li>Legal documents with formal/archaic register</li> <li>Religious texts (e.g., King James Bible style)</li> </ul> <p>For more sophisticated period detection, future versions may incorporate trained classifiers on dated corpora.</p>"},{"location":"how-to/period-aware-embeddings/#references","title":"References","text":"<ul> <li>Archer, D., Kyt\u00f6, M., Baron, A., &amp; Rayson, P. (2015). Guidelines for normalising Early Modern English corpora: Decisions and justifications. ICAME Journal, 39, 5-24.</li> <li>Burnley, D. (2000). The History of the English Language: A Source Book (2nd ed.). Longman.</li> <li>Wales, K. (1996). Personal Pronouns in Present-Day English. Cambridge University Press.</li> <li>Piotrowski, M. (2012). Natural Language Processing for Historical Texts. Morgan &amp; Claypool (Synthesis Lectures on Human Language Technologies, vol. 17).</li> </ul>"},{"location":"how-to/period-aware-embeddings/#understanding-results","title":"Understanding Results","text":""},{"location":"how-to/period-aware-embeddings/#embedding-metadata","title":"Embedding Metadata","text":"<p>When period-aware embeddings are generated, the processing artifact includes metadata showing:</p> Field Description Selected Model Which embedding model was used Selection Reason Why this model was chosen Selection Confidence Confidence score (0-1) Era Detected time period category Handles Archaic Whether the model handles historical language"},{"location":"how-to/period-aware-embeddings/#semantic-drift-classification","title":"Semantic Drift Classification","text":"<p>When comparing embeddings across periods, drift is classified as:</p> Classification Drift Value Meaning Stable &lt; 0.2 Minimal semantic change Minor Change 0.2 - 0.4 Some evolution in meaning Moderate Drift 0.4 - 0.7 Notable semantic shift Major Shift \u2265 0.7 Substantial meaning change"},{"location":"how-to/period-aware-embeddings/#tips-for-best-results","title":"Tips for Best Results","text":""},{"location":"how-to/period-aware-embeddings/#document-metadata","title":"Document Metadata","text":"<ul> <li>Ensure documents have accurate publication dates for best model selection</li> <li>Add domain metadata (scientific, legal, biomedical) when applicable</li> </ul>"},{"location":"how-to/period-aware-embeddings/#corpus-considerations","title":"Corpus Considerations","text":"<ul> <li>Use consistent embedding methods within a single experiment for valid comparisons</li> <li>When comparing across periods, process all documents with period-aware embeddings</li> <li>Include multiple documents per period for reliable drift calculations</li> </ul>"},{"location":"how-to/period-aware-embeddings/#model-downloads","title":"Model Downloads","text":"<ul> <li>Download models before batch processing to avoid delays</li> <li>Core models are sufficient for most historical text analysis</li> <li>Domain-specific models (scientific, legal, biomedical) are included in the full download</li> </ul>"},{"location":"how-to/period-aware-embeddings/#related-guides","title":"Related Guides","text":"<ul> <li>Process Documents</li> <li>Create Temporal Experiment</li> <li>View Results</li> </ul>"},{"location":"how-to/provenance-tracking/","title":"How to Use Provenance Tracking","text":"<p>This guide covers OntExtract's PROV-O provenance tracking features.</p>"},{"location":"how-to/provenance-tracking/#overview","title":"Overview","text":"<p>OntExtract implements W3C PROV-O provenance tracking directly in the database schema. Every processing operation creates versioned outputs with corresponding provenance records, enabling complete reproducibility of analytical workflows.</p>"},{"location":"how-to/provenance-tracking/#why-provenance-matters","title":"Why Provenance Matters","text":"<ul> <li>Reproducibility - Recreate exact processing conditions</li> <li>Transparency - Understand how results were generated</li> <li>Debugging - Trace unexpected results to their source</li> <li>Scholarly citation - Document analytical methodology</li> </ul>"},{"location":"how-to/provenance-tracking/#prov-o-concepts","title":"PROV-O Concepts","text":"<p>OntExtract uses the PROV-O entity-activity-agent model:</p> Concept Description Examples in OntExtract Entity Artifacts created or modified Documents, text segments, extracted entities Activity Processes that generate entities Segmentation, entity extraction, embedding generation Agent Actors responsible for activities Users, NLP tools (spaCy 3.8.11), LLM orchestrator"},{"location":"how-to/provenance-tracking/#prov-o-relationships","title":"PROV-O Relationships","text":"<p>Four relationships enable workflow reconstruction:</p> Relationship Meaning Example <code>wasDerivedFrom</code> Links artifacts to source documents Text segment derived from uploaded PDF <code>wasGeneratedBy</code> Connects artifacts to generating processes Entities generated by extraction activity <code>used</code> Records which entities were consumed Segmentation used the original document <code>wasAssociatedWith</code> Maps operations to tool versions Extraction associated with spaCy 3.8.11"},{"location":"how-to/provenance-tracking/#provenance-timeline","title":"Provenance Timeline","text":"<p>Access the provenance timeline at Provenance &gt; Timeline to view a chronological audit trail of all activities.</p> <p></p>"},{"location":"how-to/provenance-tracking/#filtering-the-timeline","title":"Filtering the Timeline","text":"<p>Filter provenance records by:</p> <ul> <li>Experiment - Show only activities for a specific experiment</li> <li>Document - Show activities related to a document family</li> <li>Term - Filter by anchor term</li> <li>Activity type - Filter by operation type</li> </ul>"},{"location":"how-to/provenance-tracking/#activity-types","title":"Activity Types","text":"Activity Type Description <code>document_upload</code> Initial document upload <code>text_extraction</code> Text extracted from PDF/document <code>document_segmentation</code> Document split into segments <code>embedding_generation</code> Vector embeddings created <code>entity_extraction</code> Named entities identified <code>temporal_extraction</code> Dates and periods extracted <code>definition_extraction</code> Concept definitions located <code>orchestration_run</code> LLM orchestration workflow <code>tool_execution</code> Individual tool execution"},{"location":"how-to/provenance-tracking/#document-versioning","title":"Document Versioning","text":"<p>OntExtract preserves original documents unchanged. Processing creates new document versions linked through provenance.</p>"},{"location":"how-to/provenance-tracking/#version-types","title":"Version Types","text":"Type Description <code>original</code> The initially uploaded document (v1) <code>processed</code> Result of processing operations <code>experimental</code> Created within an experiment context <code>composite</code> Merged or combined from multiple sources"},{"location":"how-to/provenance-tracking/#document-selection-in-experiments","title":"Document Selection in Experiments","text":"<p>When creating new experiments, only original (v1) documents appear in the selection dropdown. This ensures experiments start with clean source materials. To use a processed version, reference the original experiment that created it.</p>"},{"location":"how-to/provenance-tracking/#document-selection-in-provenance","title":"Document Selection in Provenance","text":"<p>When filtering the provenance timeline by document:</p> <ol> <li>Only original documents appear in the dropdown</li> <li>Selecting a document shows provenance for the entire document family (all versions)</li> <li>A hint displays: \"Showing provenance for X versions\" when multiple versions exist</li> </ol> <p>This design enables tracing complete processing history from a single selection.</p>"},{"location":"how-to/provenance-tracking/#processing-artifacts","title":"Processing Artifacts","text":"<p>Analysis results are stored as ProcessingArtifacts - separate database entities linked to source documents through PROV-O relationships. This maintains document integrity and enables applying multiple processing strategies to identical sources.</p>"},{"location":"how-to/provenance-tracking/#artifact-contents","title":"Artifact Contents","text":"<p>Each ProcessingArtifact includes:</p> <ul> <li>Operation type - What processing was performed</li> <li>Timestamps - When the operation occurred</li> <li>Configuration parameters - Settings used</li> <li>Results - Structured output data</li> <li>Character positions - For text-based artifacts</li> </ul>"},{"location":"how-to/provenance-tracking/#viewing-artifact-provenance","title":"Viewing Artifact Provenance","text":"<p>Provenance information is accessible through the Provenance &gt; Timeline view:</p> <ol> <li>Filter by document to see all processing history</li> <li>Each activity shows its provenance chain:</li> <li>Source document (<code>wasDerivedFrom</code>)</li> <li>Generating activity (<code>wasGeneratedBy</code>)</li> <li>Tool and version (<code>wasAssociatedWith</code>)</li> </ol>"},{"location":"how-to/provenance-tracking/#reproducibility-features","title":"Reproducibility Features","text":""},{"location":"how-to/provenance-tracking/#deterministic-operations","title":"Deterministic Operations","text":"<p>Document processing operations (segmentation, extraction) produce identical outputs given:</p> <ul> <li>Identical input documents</li> <li>Same tool versions</li> <li>Same configuration parameters</li> </ul>"},{"location":"how-to/provenance-tracking/#non-deterministic-operations","title":"Non-Deterministic Operations","text":"<p>LLM orchestration recommendations vary across runs due to model non-determinism. However, the system records:</p> <ul> <li>Complete decision context</li> <li>Recommendations and confidence scores</li> <li>Human review decisions</li> <li>Execution parameters</li> </ul>"},{"location":"how-to/provenance-tracking/#settings-capture","title":"Settings Capture","text":"<p>Experiments capture their complete configuration state at creation time, including:</p> <ul> <li>Model selections (spaCy model, embedding model)</li> <li>Processing method parameters</li> <li>Output dimensions</li> <li>Similarity thresholds</li> </ul>"},{"location":"how-to/provenance-tracking/#exporting-provenance","title":"Exporting Provenance","text":"<p>Export provenance records for external analysis:</p> <ul> <li>JSON - Structured PROV-O compatible format</li> <li>Timeline view - Chronological audit trail</li> </ul>"},{"location":"how-to/provenance-tracking/#related-guides","title":"Related Guides","text":"<ul> <li>Process Documents</li> <li>View Results</li> <li>LLM Orchestration</li> </ul>"},{"location":"how-to/settings/","title":"Settings","text":"<p>OntExtract provides configurable settings to customize application behavior. Access settings via Settings in the navigation menu (admin users only).</p>"},{"location":"how-to/settings/#settings-categories","title":"Settings Categories","text":""},{"location":"how-to/settings/#llm-integration","title":"LLM Integration","text":"<p>Controls LLM-assisted features like template enhancement.</p> <p></p> Setting Type Default Description Enable LLM Enhancement Boolean On Enable LLM template enhancement for experiment descriptions Max Tokens Integer 200 Maximum response length for template enhancement (50-1000) <p>API Key Required</p> <p>LLM features require the <code>ANTHROPIC_API_KEY</code> environment variable to be set.</p>"},{"location":"how-to/settings/#nlp-tools","title":"NLP Tools","text":"<p>Configure the transformer-based definition extraction tool.</p> Setting Type Default Description Confidence Threshold Float 0.70 Minimum confidence for definition extraction (0.5-0.95). Higher = fewer but more accurate results"},{"location":"how-to/settings/#prompt-templates","title":"Prompt Templates","text":"<p>Manage Jinja2 templates for experiment descriptions. Templates can be viewed and edited, with optional LLM enhancement support.</p> Column Description Template Template key identifier Category Template category (e.g., experiment descriptions) Variables Required template variables LLM Enhancement Whether LLM can enhance template output Actions View or edit the template"},{"location":"how-to/settings/#processing","title":"Processing","text":"<p>Configure how documents are processed during text cleanup operations.</p> <p></p> Setting Type Default Description Concurrent Chunk Processing Boolean On Process document chunks in parallel for faster cleanup Max Concurrent Chunks Integer 3 Maximum simultaneous API calls (1-10)"},{"location":"how-to/settings/#how-text-cleanup-works","title":"How Text Cleanup Works","text":"<p>When cleaning large documents (&gt;8,000 characters), OntExtract:</p> <ol> <li>Chunks the document by paragraph boundaries</li> <li>Processes each chunk through Claude for OCR correction, spelling fixes, and formatting improvements</li> <li>Reassembles cleaned chunks in order</li> </ol> <p>Parallel Mode (default): Multiple chunks processed simultaneously. Faster overall but uses more concurrent API calls.</p> <p>Sequential Mode: Chunks processed one at a time. Predictable progress, lower concurrent API usage.</p> <p>Recommended Settings</p> <p>The default of 3 concurrent chunks balances speed with API rate limits. Increase to 5-10 for faster processing with higher rate limits.</p>"},{"location":"how-to/settings/#provenance","title":"Provenance","text":"<p>Configure how provenance records are handled during deletions.</p> Setting Type Default Description Purge on Delete Boolean On Permanently delete provenance records (vs. mark as invalidated) Show Deleted in Timeline Boolean Off Display invalidated items in timeline views by default <p>When Purge on Delete is disabled: - Provenance records are marked as \"invalidated\" but preserved - Complete audit trail is maintained - Deleted items can be shown in timeline views</p>"},{"location":"how-to/settings/#saving-settings","title":"Saving Settings","text":"<ol> <li>Navigate to the Settings page</li> <li>Select the appropriate tab (LLM Integration, NLP Tools, Prompt Templates, Provenance, or Processing)</li> <li>Modify settings as needed</li> <li>Click Save Changes</li> </ol> <p>Settings are stored per-user, allowing different users to have different preferences.</p>"},{"location":"how-to/settings/#related-guides","title":"Related Guides","text":"<ul> <li>LLM Orchestration - Using LLM-powered document analysis</li> <li>Process Documents - Text cleanup and processing</li> <li>Provenance Tracking - Understanding the audit trail</li> </ul>"},{"location":"how-to/upload-documents/","title":"How to Upload Documents","text":"<p>This guide covers uploading historical documents to OntExtract for analysis.</p>"},{"location":"how-to/upload-documents/#overview","title":"Overview","text":"<p>Documents are the foundation of temporal evolution analysis. OntExtract supports various document formats and captures metadata essential for period-aware processing.</p>"},{"location":"how-to/upload-documents/#supported-formats","title":"Supported Formats","text":"<ul> <li>PDF - Scanned or digital PDFs (text extracted automatically)</li> <li>Plain Text (.txt) - Raw text files</li> <li>Word Documents (.docx) - Microsoft Word format</li> <li>Markdown (.md) - Markdown-formatted text</li> </ul>"},{"location":"how-to/upload-documents/#document-versions","title":"Document Versions","text":"<p>OntExtract treats each upload as a separate document record. Multiple versions of the same work can be uploaded:</p> <ul> <li>Different formats - PDF and Word versions of the same paper</li> <li>Different versions - Preprint (v1) and final published version</li> <li>Updated copies - A cleaner scan or OCR-corrected version</li> </ul>"},{"location":"how-to/upload-documents/#how-versions-are-handled","title":"How Versions Are Handled","text":"<ul> <li>Documents with the same DOI or title are not automatically linked</li> <li>Each upload creates an independent document record</li> <li>Provenance tracking records which specific document version was processed</li> </ul>"},{"location":"how-to/upload-documents/#best-practices","title":"Best Practices","text":"Scenario Recommendation Better quality scan available Upload new version, use it for new experiments Preprint vs published Upload both if content differs significantly PDF and Word of same content Upload whichever extracts text better Duplicate by accident Delete the unwanted copy from Documents list <p>To identify duplicates, sort by DOI or title in the Documents view.</p>"},{"location":"how-to/upload-documents/#upload-methods","title":"Upload Methods","text":""},{"location":"how-to/upload-documents/#single-document-upload","title":"Single Document Upload","text":"<ol> <li>Navigate to Documents in the main menu</li> <li>Click Upload Document</li> <li>Select or drag-and-drop your file</li> <li>Review the extracted metadata (see below)</li> <li>Click Upload</li> </ol>"},{"location":"how-to/upload-documents/#automatic-metadata-extraction","title":"Automatic Metadata Extraction","text":"<p>By default, OntExtract automatically extracts metadata from uploaded PDFs using a cascade of methods:</p>"},{"location":"how-to/upload-documents/#extraction-priority","title":"Extraction Priority","text":"<ol> <li>arXiv ID - Checked first in filename, then PDF content. If found, queries Semantic Scholar.</li> <li>DOI - Extracted from PDF pages. If found, queries Semantic Scholar, then CrossRef.</li> <li>Title + Authors - Extracted from PDF text. Used to search CrossRef database.</li> <li>PDF embedded metadata - Falls back to PDF document properties.</li> <li>Filename - Used as last resort for title if nothing else matches.</li> </ol>"},{"location":"how-to/upload-documents/#database-lookups","title":"Database Lookups","text":"<p>OntExtract queries academic databases to enrich metadata:</p> Database Best For What It Provides Semantic Scholar arXiv papers, recent preprints Title, authors, year, abstract, citation count CrossRef Published journal articles, books Title, authors, journal, volume, pages, DOI <p>Note: Very recent papers (not yet indexed) or very old documents (pre-digital) may not be found in these databases.</p>"},{"location":"how-to/upload-documents/#source-indicators","title":"Source Indicators","text":"<p>After extraction, the upload form shows badges indicating where each metadata field originated:</p> <ul> <li>Green - CrossRef database match</li> <li>Blue - Semantic Scholar database match</li> <li>Yellow - Extracted from PDF analysis</li> <li>Cyan - User-provided value</li> </ul>"},{"location":"how-to/upload-documents/#disabling-automatic-extraction","title":"Disabling Automatic Extraction","text":"<p>Uncheck \"Automatic metadata extraction\" to:</p> <ul> <li>Skip database lookups entirely</li> <li>Reveal all manual entry fields</li> <li>Enter metadata directly without API calls</li> </ul> <p>This is useful for:</p> <ul> <li>Personal or unpublished documents</li> <li>Historical documents not in academic databases</li> <li>Documents where automatic extraction produces incorrect results</li> </ul>"},{"location":"how-to/upload-documents/#metadata-fields","title":"Metadata Fields","text":""},{"location":"how-to/upload-documents/#required-fields","title":"Required Fields","text":"Field Description Title Document title (only required field)"},{"location":"how-to/upload-documents/#core-fields-always-shown","title":"Core Fields (Always Shown)","text":"Field Description Title Document title for identification Authors Comma-separated author names Publication Date Year, month-year, or full date"},{"location":"how-to/upload-documents/#extended-fields-manual-entry-mode","title":"Extended Fields (Manual Entry Mode)","text":"<p>When automatic extraction is disabled, additional fields appear:</p> <p>Publication Details:</p> Field Description Journal/Publication Journal or publication name Volume Volume number Issue Issue number Pages Page range (e.g., \"123-145\") Publisher Publishing organization Container Title For chapters in edited volumes Series Book or journal series Edition Edition number or description Editor Editor name(s) <p>Identifiers:</p> Field Description DOI Digital Object Identifier URL Web address ISBN Book identifier ISSN Serial identifier <p>Additional Information:</p> Field Description Abstract Document abstract or summary Document Type Academic Paper, Book, Dictionary Entry, etc. Entry Term For dictionary/reference entries (headword) Access Date When online source was accessed Notes Additional context or comments"},{"location":"how-to/upload-documents/#publication-date-formats","title":"Publication Date Formats","text":"<p>OntExtract accepts various date formats:</p> <ul> <li>Year only: <code>1910</code>, <code>1856</code></li> <li>Month and year: <code>March 1910</code>, <code>1910-03</code></li> <li>Full date: <code>1910-03-15</code>, <code>March 15, 1910</code></li> </ul> <p>The system extracts the year for temporal period assignment.</p>"},{"location":"how-to/upload-documents/#coming-soon","title":"Coming Soon","text":"<ul> <li>Multiple authors with structured entry and Zotero-style lookup</li> <li>LLM-based metadata guessing from document content</li> </ul>"},{"location":"how-to/upload-documents/#after-upload","title":"After Upload","text":"<p>Once uploaded, documents appear in the Documents list accessible from the main menu.</p>"},{"location":"how-to/upload-documents/#processing-options","title":"Processing Options","text":"<p>After upload, documents can be processed with:</p> <ul> <li>LLM Text Cleanup - Fix OCR errors, formatting issues (recommended for scanned documents)</li> <li>Segmentation - Split into paragraphs or sentences</li> <li>Embeddings - Generate vector representations for similarity search</li> <li>Entity Extraction - Identify named entities and concepts</li> </ul>"},{"location":"how-to/upload-documents/#tips-for-historical-documents","title":"Tips for Historical Documents","text":""},{"location":"how-to/upload-documents/#ocr-quality","title":"OCR Quality","text":"<p>Scanned historical documents often have OCR errors. Use the LLM Text Cleanup feature to: - Fix character recognition mistakes (rn \u2192 m, l \u2192 I) - Correct archaic spelling normalization - Remove scanning artifacts (headers, page numbers)</p>"},{"location":"how-to/upload-documents/#temporal-periods","title":"Temporal Periods","text":"<p>Documents are automatically assigned to temporal periods based on publication date. For an experiment tracking 1910-2024: - A document from 1910 goes in the earliest period - A document from 2020 goes in the latest period</p>"},{"location":"how-to/upload-documents/#troubleshooting","title":"Troubleshooting","text":""},{"location":"how-to/upload-documents/#upload-fails","title":"Upload Fails","text":"<ul> <li>Check file size (max 50MB default)</li> <li>Verify file format is supported</li> <li>Ensure you're logged in</li> </ul>"},{"location":"how-to/upload-documents/#no-text-extracted","title":"No Text Extracted","text":"<ul> <li>PDF may be image-only (scanned without OCR)</li> <li>Try re-uploading with a different format</li> </ul>"},{"location":"how-to/upload-documents/#wrong-publication-date","title":"Wrong Publication Date","text":"<ul> <li>Edit the document metadata after upload</li> <li>Go to Documents &gt; Select document &gt; Edit</li> </ul>"},{"location":"how-to/upload-documents/#related-guides","title":"Related Guides","text":"<ul> <li>Manage Your Library</li> <li>Process Documents</li> <li>Create Temporal Experiment</li> <li>Create Anchor Terms</li> </ul>"},{"location":"how-to/view-results/","title":"How to View Results","text":"<p>This guide covers exploring analysis results and provenance in OntExtract.</p>"},{"location":"how-to/view-results/#overview","title":"Overview","text":"<p>After processing documents, OntExtract provides several ways to explore results:</p> <ul> <li>Processing artifacts for individual documents</li> <li>Experiment-level analysis and timelines</li> <li>Provenance graphs showing processing history</li> <li>Export options for further analysis</li> </ul>"},{"location":"how-to/view-results/#viewing-processing-results","title":"Viewing Processing Results","text":"<p>Processing results are accessed through experiment result pages, which aggregate findings across all documents in an experiment.</p>"},{"location":"how-to/view-results/#access-results","title":"Access Results","text":"<ol> <li>Go to Experiments &gt; Select an experiment</li> <li>Expand the View Results section</li> <li>Click a result type (Definitions, Entities, Embeddings, Segments, or Temporal)</li> </ol>"},{"location":"how-to/view-results/#artifact-types","title":"Artifact Types","text":"Type Contents Text Segments Paragraphs or sentences with character positions Entities Named entities with types and confidence scores Embeddings Vector representations (viewable as similarity scores) Temporal Expressions Dates, periods, and durations found in text Definitions Extracted concept definitions with pattern types"},{"location":"how-to/view-results/#definition-results","title":"Definition Results","text":"<p>Definition artifacts include:</p> <ul> <li>Term - The word or phrase being defined</li> <li>Definition text - The extracted definition content</li> <li>Pattern type - How it was detected (explicit_definition, explicit_reference, meaning, copula, acronym, also_known_as, ie_explanation, appositive)</li> <li>Confidence - Score from 0.65-0.90 depending on pattern reliability</li> <li>Source badge - \"Pattern\" (default) or \"ZeroShot\" (if enabled via <code>ENABLE_ZERO_SHOT_DEFINITIONS=true</code>)</li> </ul> <p>Acronym definitions show strict validation: \"IRA (Information Retrieval Agent)\" passes because expansion letters match the acronym.</p> <p>Note: Definition extraction works best on documents that explicitly define terminology, such as glossaries, textbook introductions, or standards documents. Research papers that use but do not define terms may return few or no results.</p>"},{"location":"how-to/view-results/#artifact-details","title":"Artifact Details","text":"<p>Each artifact shows:</p> <ul> <li>Operation type and timestamp</li> <li>Source document reference</li> <li>Processing parameters used</li> <li>Tool version (for reproducibility)</li> <li>Structured results</li> </ul>"},{"location":"how-to/view-results/#experiment-results","title":"Experiment Results","text":""},{"location":"how-to/view-results/#result-type-views","title":"Result Type Views","text":"<p>From the experiment page, use the View Results buttons to explore specific result types:</p> Button Shows Definitions Extracted term definitions across all documents Entities Named entities (people, organizations, places) Embeddings Semantic similarity search interface Segments Text segments with positions Temporal Dates, periods, and temporal markers <p>Each view aggregates results from all documents in the experiment, enabling comparison of findings across the corpus.</p> <p></p> <p></p> <p></p>"},{"location":"how-to/view-results/#timeline-view","title":"Timeline View","text":"<p>For temporal evolution experiments:</p> <ol> <li>Go to Experiments &gt; Select experiment</li> <li>View the timeline showing documents by period</li> <li>See term usage patterns across time</li> </ol>"},{"location":"how-to/view-results/#llm-synthesis-results","title":"LLM Synthesis Results","text":"<p>When using LLM Orchestration:</p> <ol> <li>Go to Document Pipeline</li> <li>Click View Results on the orchestration status banner (shown when orchestration completes)</li> <li>See cross-document patterns and term cards</li> </ol> <p>Note: The synthesis organizes findings but does not interpret them. Analytical conclusions remain with the researcher.</p>"},{"location":"how-to/view-results/#provenance-tracking","title":"Provenance Tracking","text":"<p>OntExtract records complete PROV-O provenance for all operations.</p>"},{"location":"how-to/view-results/#viewing-provenance","title":"Viewing Provenance","text":"<p>Each artifact links to its provenance chain showing:</p> <ul> <li>wasDerivedFrom - Source document(s)</li> <li>wasGeneratedBy - Processing activity</li> <li>wasAssociatedWith - Tool and version used</li> <li>used - Input entities consumed</li> </ul>"},{"location":"how-to/view-results/#why-provenance-matters","title":"Why Provenance Matters","text":"<ul> <li>Reproducibility - Recreate exact processing conditions</li> <li>Transparency - Understand how results were generated</li> <li>Debugging - Trace unexpected results to their source</li> <li>Scholarly citation - Document analytical methodology</li> </ul>"},{"location":"how-to/view-results/#export-options","title":"Export Options","text":""},{"location":"how-to/view-results/#export-formats","title":"Export Formats","text":"Format Use Case CSV Tabular data for spreadsheets JSON Structured data for programming"},{"location":"how-to/view-results/#what-can-be-exported","title":"What Can Be Exported","text":"<ul> <li>Processing artifacts with metadata</li> <li>Entity extraction results</li> <li>Segment text with positions</li> <li>Provenance records</li> </ul>"},{"location":"how-to/view-results/#coming-soon","title":"Coming Soon","text":"<p>Additional results features planned for future releases:</p> <ul> <li>Semantic similarity search (find similar passages across documents)</li> <li>Interactive timeline visualizations</li> <li>Semantic drift graphs</li> <li>Comparative period analysis</li> <li>Formatted report export (PDF)</li> </ul>"},{"location":"how-to/view-results/#troubleshooting","title":"Troubleshooting","text":""},{"location":"how-to/view-results/#no-results-showing","title":"No results showing","text":"<ul> <li>Verify processing operations completed</li> <li>Check the experiment's View Results section</li> <li>Review experiment status</li> </ul>"},{"location":"how-to/view-results/#missing-artifacts","title":"Missing artifacts","text":"<ul> <li>Ensure the operation was selected during processing</li> <li>Check for errors in the processing log</li> <li>Verify document had extractable content</li> </ul>"},{"location":"how-to/view-results/#related-guides","title":"Related Guides","text":"<ul> <li>Process Documents</li> <li>LLM Orchestration</li> <li>Create Temporal Experiment</li> <li>Provenance Tracking</li> </ul>"}]}