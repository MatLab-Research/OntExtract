"""Initial migration

Revision ID: 526974d6041c
Revises: 
Create Date: 2025-09-20 17:46:29.988545

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision = '526974d6041c'
down_revision = None
branch_labels = None
depends_on = None


def upgrade():
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_table('ontology_versions')
    op.drop_table('domains')
    with op.batch_alter_table('prov_activities', schema=None) as batch_op:
        batch_op.drop_index(batch_op.f('idx_prov_activities_associated'))
        batch_op.drop_index(batch_op.f('idx_prov_activities_started'))
        batch_op.drop_index(batch_op.f('idx_prov_activities_type'))

    op.drop_table('prov_activities')
    with op.batch_alter_table('ontology_entities', schema=None) as batch_op:
        batch_op.drop_index(batch_op.f('idx_entity_label'))
        batch_op.drop_index(batch_op.f('idx_entity_type'))
        batch_op.drop_index(batch_op.f('idx_ontology_entity'))
        batch_op.drop_index(batch_op.f('ix_entity_embedding_vector'), postgresql_ops={'embedding': 'vector_cosine_ops'}, postgresql_with={'lists': '100'}, postgresql_using='ivfflat')

    op.drop_table('ontology_entities')
    op.drop_table('ontologies')
    with op.batch_alter_table('document_processing_summary', schema=None) as batch_op:
        batch_op.drop_index(batch_op.f('idx_processing_summary_document'))
        batch_op.drop_index(batch_op.f('idx_processing_summary_type'))

    op.drop_table('document_processing_summary')
    op.drop_table('search_history')
    with op.batch_alter_table('prov_relationships', schema=None) as batch_op:
        batch_op.drop_index(batch_op.f('idx_prov_relationships_object'))
        batch_op.drop_index(batch_op.f('idx_prov_relationships_subject'))
        batch_op.drop_index(batch_op.f('idx_prov_relationships_type'))

    op.drop_table('prov_relationships')
    with op.batch_alter_table('prov_entities', schema=None) as batch_op:
        batch_op.drop_index(batch_op.f('idx_prov_entities_attributed'))
        batch_op.drop_index(batch_op.f('idx_prov_entities_derived'))
        batch_op.drop_index(batch_op.f('idx_prov_entities_generated'))
        batch_op.drop_index(batch_op.f('idx_prov_entities_type'))

    op.drop_table('prov_entities')
    with op.batch_alter_table('version_changelog', schema=None) as batch_op:
        batch_op.drop_index(batch_op.f('idx_version_changelog_change_type'))
        batch_op.drop_index(batch_op.f('idx_version_changelog_document_version'))

    op.drop_table('version_changelog')
    with op.batch_alter_table('prov_agents', schema=None) as batch_op:
        batch_op.drop_index(batch_op.f('idx_prov_agents_name'))
        batch_op.drop_index(batch_op.f('idx_prov_agents_type'))

    op.drop_table('prov_agents')
    with op.batch_alter_table('document_embeddings', schema=None) as batch_op:
        batch_op.drop_index(batch_op.f('idx_embeddings_document'))
        batch_op.drop_index(batch_op.f('idx_embeddings_model'))
        batch_op.drop_index(batch_op.f('idx_embeddings_term_period'))

    op.drop_table('document_embeddings')
    with op.batch_alter_table('analysis_agents', schema=None) as batch_op:
        batch_op.drop_index(batch_op.f('idx_analysis_agents_active'), postgresql_where='(is_active = true)')
        batch_op.drop_index(batch_op.f('idx_analysis_agents_type'))

    with op.batch_alter_table('context_anchors', schema=None) as batch_op:
        batch_op.drop_index(batch_op.f('idx_context_anchors_frequency'))
        batch_op.drop_index(batch_op.f('idx_context_anchors_term'))

    with op.batch_alter_table('documents', schema=None) as batch_op:
        batch_op.add_column(sa.Column('composite_sources', sa.JSON(), nullable=True))
        batch_op.add_column(sa.Column('composite_metadata', sa.JSON(), nullable=True))
        batch_op.alter_column('processing_metadata',
               existing_type=postgresql.JSON(astext_type=sa.Text()),
               comment=None,
               existing_comment='General metadata for processing info, embeddings, and document analysis',
               existing_nullable=True)
        batch_op.alter_column('version_number',
               existing_type=sa.INTEGER(),
               nullable=False,
               comment=None,
               existing_comment='Sequential version number within a document family',
               existing_server_default=sa.text('1'))
        batch_op.alter_column('version_type',
               existing_type=sa.VARCHAR(length=20),
               nullable=False,
               comment=None,
               existing_comment='Type of version: original, processed, experimental',
               existing_server_default=sa.text("'original'::character varying"))
        batch_op.alter_column('source_document_id',
               existing_type=sa.INTEGER(),
               comment=None,
               existing_comment='Original document this version derives from',
               existing_nullable=True)
        batch_op.alter_column('experiment_id',
               existing_type=sa.INTEGER(),
               comment=None,
               existing_comment='Associated experiment (for experimental versions)',
               existing_nullable=True)
        batch_op.alter_column('processing_notes',
               existing_type=sa.TEXT(),
               comment=None,
               existing_comment='Notes about processing operations that created this version',
               existing_nullable=True)
        batch_op.drop_index(batch_op.f('idx_documents_experiment_id'))
        batch_op.drop_index(batch_op.f('idx_documents_parent'))
        batch_op.drop_index(batch_op.f('idx_documents_source_document_id'))
        batch_op.drop_index(batch_op.f('idx_documents_type'))
        batch_op.drop_index(batch_op.f('idx_documents_version_number'))
        batch_op.drop_index(batch_op.f('idx_documents_version_type'))
        batch_op.create_index(batch_op.f('ix_documents_experiment_id'), ['experiment_id'], unique=False)
        batch_op.create_index(batch_op.f('ix_documents_parent_document_id'), ['parent_document_id'], unique=False)
        batch_op.create_index(batch_op.f('ix_documents_source_document_id'), ['source_document_id'], unique=False)
        batch_op.drop_constraint(batch_op.f('fk_documents_experiment'), type_='foreignkey')
        batch_op.drop_constraint(batch_op.f('fk_documents_source'), type_='foreignkey')
        batch_op.drop_constraint(batch_op.f('fk_documents_parent_document_id'), type_='foreignkey')
        batch_op.create_foreign_key(None, 'documents', ['source_document_id'], ['id'])
        batch_op.create_foreign_key(None, 'experiments', ['experiment_id'], ['id'])
        batch_op.create_foreign_key(None, 'documents', ['parent_document_id'], ['id'])

    with op.batch_alter_table('experiment_documents', schema=None) as batch_op:
        batch_op.drop_index(batch_op.f('idx_experiment_documents_status'))
        batch_op.drop_index(batch_op.f('idx_experiment_documents_updated'))
        batch_op.drop_column('processing_status')
        batch_op.drop_column('nlp_analysis_completed')
        batch_op.drop_column('segments_metadata')
        batch_op.drop_column('embeddings_metadata')
        batch_op.drop_column('processed_at')
        batch_op.drop_column('processing_metadata')
        batch_op.drop_column('segments_created')
        batch_op.drop_column('embeddings_applied')
        batch_op.drop_column('updated_at')
        batch_op.drop_column('nlp_results')

    with op.batch_alter_table('experiment_references', schema=None) as batch_op:
        batch_op.drop_index(batch_op.f('idx_experiment_references_experiment'))
        batch_op.drop_index(batch_op.f('idx_experiment_references_reference'))

    with op.batch_alter_table('fuzziness_adjustments', schema=None) as batch_op:
        batch_op.drop_index(batch_op.f('idx_fuzziness_adjustments_user'))
        batch_op.drop_index(batch_op.f('idx_fuzziness_adjustments_version'))

    with op.batch_alter_table('learning_patterns', schema=None) as batch_op:
        batch_op.alter_column('context_signature',
               existing_type=sa.VARCHAR(length=200),
               comment=None,
               existing_comment='Signature for matching similar decision contexts',
               existing_nullable=False)
        batch_op.alter_column('conditions',
               existing_type=postgresql.JSONB(astext_type=sa.Text()),
               type_=postgresql.JSON(astext_type=sa.Text()),
               existing_nullable=False)
        batch_op.alter_column('recommendations',
               existing_type=postgresql.JSONB(astext_type=sa.Text()),
               type_=postgresql.JSON(astext_type=sa.Text()),
               existing_nullable=False)
        batch_op.alter_column('researcher_authority',
               existing_type=postgresql.JSONB(astext_type=sa.Text()),
               type_=postgresql.JSON(astext_type=sa.Text()),
               comment=None,
               existing_comment='Authority assessment of source researcher for weighting',
               existing_nullable=True)
        batch_op.drop_index(batch_op.f('idx_learning_patterns_context_type'))
        batch_op.drop_index(batch_op.f('idx_learning_patterns_status'))
        batch_op.drop_index(batch_op.f('idx_learning_patterns_success_rate'))
        batch_op.create_index('idx_pattern_context_type', ['context_signature', 'pattern_type'], unique=False)
        batch_op.create_index(batch_op.f('ix_learning_patterns_context_signature'), ['context_signature'], unique=False)
        batch_op.create_index(batch_op.f('ix_learning_patterns_pattern_status'), ['pattern_status'], unique=False)
        batch_op.create_index(batch_op.f('ix_learning_patterns_pattern_type'), ['pattern_type'], unique=False)
        batch_op.drop_table_comment(
        existing_comment='Codified learning patterns derived from researcher feedback'
    )

    with op.batch_alter_table('multi_model_consensus', schema=None) as batch_op:
        batch_op.alter_column('models_involved',
               existing_type=postgresql.ARRAY(sa.TEXT()),
               type_=postgresql.ARRAY(sa.String()),
               existing_nullable=True)
        batch_op.alter_column('model_responses',
               existing_type=postgresql.JSONB(astext_type=sa.Text()),
               type_=postgresql.JSON(astext_type=sa.Text()),
               existing_nullable=True)
        batch_op.alter_column('model_confidence_scores',
               existing_type=postgresql.JSONB(astext_type=sa.Text()),
               type_=postgresql.JSON(astext_type=sa.Text()),
               existing_nullable=True)
        batch_op.alter_column('model_agreement_matrix',
               existing_type=postgresql.JSONB(astext_type=sa.Text()),
               type_=postgresql.JSON(astext_type=sa.Text()),
               comment=None,
               existing_comment='Pairwise agreement scores between models',
               existing_nullable=True)
        batch_op.alter_column('final_decision',
               existing_type=postgresql.JSONB(astext_type=sa.Text()),
               type_=postgresql.JSON(astext_type=sa.Text()),
               existing_nullable=True)
        batch_op.alter_column('disagreement_areas',
               existing_type=postgresql.JSONB(astext_type=sa.Text()),
               type_=postgresql.JSON(astext_type=sa.Text()),
               comment=None,
               existing_comment='Specific areas where models disagreed',
               existing_nullable=True)
        batch_op.drop_index(batch_op.f('idx_multi_model_consensus_decision'))
        batch_op.drop_index(batch_op.f('idx_multi_model_consensus_reached'))
        batch_op.create_index(batch_op.f('ix_multi_model_consensus_orchestration_decision_id'), ['orchestration_decision_id'], unique=False)
        batch_op.drop_constraint(batch_op.f('multi_model_consensus_orchestration_decision_id_fkey'), type_='foreignkey')
        batch_op.create_foreign_key(None, 'orchestration_decisions', ['orchestration_decision_id'], ['id'])
        batch_op.drop_table_comment(
        existing_comment='Multi-model validation and consensus decision logging'
    )

    with op.batch_alter_table('orchestration_decisions', schema=None) as batch_op:
        batch_op.alter_column('input_metadata',
               existing_type=postgresql.JSONB(astext_type=sa.Text()),
               type_=postgresql.JSON(astext_type=sa.Text()),
               comment=None,
               existing_comment='Document metadata that influenced tool selection (year, domain, format, length)',
               existing_nullable=True)
        batch_op.alter_column('document_characteristics',
               existing_type=postgresql.JSONB(astext_type=sa.Text()),
               type_=postgresql.JSON(astext_type=sa.Text()),
               existing_nullable=True)
        batch_op.alter_column('selected_tools',
               existing_type=postgresql.ARRAY(sa.TEXT()),
               type_=postgresql.ARRAY(sa.String()),
               existing_nullable=True)
        batch_op.alter_column('decision_factors',
               existing_type=postgresql.JSONB(astext_type=sa.Text()),
               type_=postgresql.JSON(astext_type=sa.Text()),
               comment=None,
               existing_comment='Structured reasoning components for decision analysis',
               existing_nullable=True)
        batch_op.alter_column('tool_execution_success',
               existing_type=postgresql.JSONB(astext_type=sa.Text()),
               type_=postgresql.JSON(astext_type=sa.Text()),
               comment=None,
               existing_comment='Per-tool success rates and validation results',
               existing_nullable=True)
        batch_op.drop_index(batch_op.f('idx_orchestration_decisions_agent'))
        batch_op.drop_index(batch_op.f('idx_orchestration_decisions_document'))
        batch_op.drop_index(batch_op.f('idx_orchestration_decisions_experiment'))
        batch_op.drop_index(batch_op.f('idx_orchestration_decisions_status'))
        batch_op.drop_index(batch_op.f('idx_orchestration_decisions_term_time'))
        batch_op.create_index('idx_orchestration_experiment', ['experiment_id', 'created_at'], unique=False)
        batch_op.create_index('idx_orchestration_term_time', ['term_text', 'created_at'], unique=False)
        batch_op.create_index(batch_op.f('ix_orchestration_decisions_activity_status'), ['activity_status'], unique=False)
        batch_op.create_index(batch_op.f('ix_orchestration_decisions_created_at'), ['created_at'], unique=False)
        batch_op.create_index(batch_op.f('ix_orchestration_decisions_document_id'), ['document_id'], unique=False)
        batch_op.create_index(batch_op.f('ix_orchestration_decisions_experiment_id'), ['experiment_id'], unique=False)
        batch_op.create_index(batch_op.f('ix_orchestration_decisions_term_text'), ['term_text'], unique=False)
        batch_op.drop_table_comment(
        existing_comment='PROV-O compliant logging of LLM orchestration decisions for tool selection and coordination'
    )

    with op.batch_alter_table('orchestration_feedback', schema=None) as batch_op:
        batch_op.alter_column('researcher_expertise',
               existing_type=postgresql.JSONB(astext_type=sa.Text()),
               type_=postgresql.JSON(astext_type=sa.Text()),
               comment=None,
               existing_comment='Researcher expertise profile for weighting feedback authority',
               existing_nullable=True)
        batch_op.alter_column('original_decision',
               existing_type=postgresql.JSONB(astext_type=sa.Text()),
               type_=postgresql.JSON(astext_type=sa.Text()),
               existing_nullable=True)
        batch_op.alter_column('researcher_preference',
               existing_type=postgresql.JSONB(astext_type=sa.Text()),
               type_=postgresql.JSON(astext_type=sa.Text()),
               existing_nullable=True)
        batch_op.alter_column('domain_specific_factors',
               existing_type=postgresql.JSONB(astext_type=sa.Text()),
               type_=postgresql.JSON(astext_type=sa.Text()),
               comment=None,
               existing_comment='Domain knowledge that LLM missed in original decision',
               existing_nullable=True)
        batch_op.alter_column('suggested_tools',
               existing_type=postgresql.ARRAY(sa.TEXT()),
               type_=postgresql.ARRAY(sa.String()),
               existing_nullable=True)
        batch_op.drop_index(batch_op.f('idx_orchestration_feedback_decision_researcher'))
        batch_op.drop_index(batch_op.f('idx_orchestration_feedback_provided_at'))
        batch_op.drop_index(batch_op.f('idx_orchestration_feedback_type_status'))
        batch_op.create_index('idx_feedback_decision_researcher', ['orchestration_decision_id', 'researcher_id'], unique=False)
        batch_op.create_index('idx_feedback_type_status', ['feedback_type', 'feedback_status'], unique=False)
        batch_op.create_index(batch_op.f('ix_orchestration_feedback_feedback_status'), ['feedback_status'], unique=False)
        batch_op.create_index(batch_op.f('ix_orchestration_feedback_feedback_type'), ['feedback_type'], unique=False)
        batch_op.create_index(batch_op.f('ix_orchestration_feedback_orchestration_decision_id'), ['orchestration_decision_id'], unique=False)
        batch_op.create_index(batch_op.f('ix_orchestration_feedback_provided_at'), ['provided_at'], unique=False)
        batch_op.drop_constraint(batch_op.f('orchestration_feedback_orchestration_decision_id_fkey'), type_='foreignkey')
        batch_op.create_foreign_key(None, 'orchestration_decisions', ['orchestration_decision_id'], ['id'])
        batch_op.drop_table_comment(
        existing_comment='Researcher feedback on orchestration decisions for continuous improvement'
    )

    with op.batch_alter_table('orchestration_overrides', schema=None) as batch_op:
        batch_op.alter_column('original_decision',
               existing_type=postgresql.JSONB(astext_type=sa.Text()),
               type_=postgresql.JSON(astext_type=sa.Text()),
               existing_nullable=False)
        batch_op.alter_column('overridden_decision',
               existing_type=postgresql.JSONB(astext_type=sa.Text()),
               type_=postgresql.JSON(astext_type=sa.Text()),
               existing_nullable=False)
        batch_op.alter_column('expert_knowledge_applied',
               existing_type=postgresql.JSONB(astext_type=sa.Text()),
               type_=postgresql.JSON(astext_type=sa.Text()),
               existing_nullable=True)
        batch_op.alter_column('execution_results',
               existing_type=postgresql.JSONB(astext_type=sa.Text()),
               type_=postgresql.JSON(astext_type=sa.Text()),
               existing_nullable=True)
        batch_op.alter_column('performance_comparison',
               existing_type=postgresql.JSONB(astext_type=sa.Text()),
               type_=postgresql.JSON(astext_type=sa.Text()),
               existing_nullable=True)
        batch_op.drop_index(batch_op.f('idx_orchestration_overrides_applied_at'))
        batch_op.drop_index(batch_op.f('idx_orchestration_overrides_decision_researcher'))
        batch_op.create_index('idx_override_decision_researcher', ['orchestration_decision_id', 'researcher_id'], unique=False)
        batch_op.create_index(batch_op.f('ix_orchestration_overrides_orchestration_decision_id'), ['orchestration_decision_id'], unique=False)
        batch_op.drop_constraint(batch_op.f('orchestration_overrides_orchestration_decision_id_fkey'), type_='foreignkey')
        batch_op.create_foreign_key(None, 'orchestration_decisions', ['orchestration_decision_id'], ['id'])
        batch_op.drop_table_comment(
        existing_comment='Manual overrides applied by researchers to specific orchestration decisions'
    )

    with op.batch_alter_table('provenance_activities', schema=None) as batch_op:
        batch_op.alter_column('prov_id',
               existing_type=sa.VARCHAR(length=255),
               comment=None,
               existing_comment='PROV-O Activity identifier (e.g., activity_embeddings_456)',
               existing_nullable=False)
        batch_op.alter_column('prov_type',
               existing_type=sa.VARCHAR(length=100),
               comment=None,
               existing_comment='PROV-O Activity type (e.g., ont:EmbeddingsProcessing)',
               existing_nullable=False)
        batch_op.alter_column('was_associated_with',
               existing_type=sa.VARCHAR(length=255),
               comment=None,
               existing_comment='PROV-O wasAssociatedWith agent',
               existing_nullable=True)
        batch_op.alter_column('used_plan',
               existing_type=sa.VARCHAR(length=255),
               comment=None,
               existing_comment='PROV-O used plan/protocol',
               existing_nullable=True)
        batch_op.alter_column('activity_metadata',
               existing_type=postgresql.JSONB(astext_type=sa.Text()),
               type_=sa.JSON(),
               existing_nullable=True)
        batch_op.drop_index(batch_op.f('idx_provenance_activities_activity_type'))
        batch_op.drop_index(batch_op.f('idx_provenance_activities_experiment_id'))
        batch_op.drop_index(batch_op.f('idx_provenance_activities_processing_job_id'))
        batch_op.drop_index(batch_op.f('idx_provenance_activities_prov_id'))
        batch_op.drop_index(batch_op.f('idx_provenance_activities_prov_type'))
        batch_op.drop_constraint(batch_op.f('provenance_activities_prov_id_key'), type_='unique')
        batch_op.create_index(batch_op.f('ix_provenance_activities_experiment_id'), ['experiment_id'], unique=False)
        batch_op.create_index(batch_op.f('ix_provenance_activities_processing_job_id'), ['processing_job_id'], unique=False)
        batch_op.create_index(batch_op.f('ix_provenance_activities_prov_id'), ['prov_id'], unique=True)
        batch_op.drop_constraint(batch_op.f('fk_provenance_activities_experiment'), type_='foreignkey')
        batch_op.drop_constraint(batch_op.f('fk_provenance_activities_processing_job'), type_='foreignkey')
        batch_op.create_foreign_key(None, 'processing_jobs', ['processing_job_id'], ['id'])
        batch_op.create_foreign_key(None, 'experiments', ['experiment_id'], ['id'])
        batch_op.drop_table_comment(
        existing_comment='PROV-O Activity model representing processing activities'
    )

    with op.batch_alter_table('provenance_entities', schema=None) as batch_op:
        batch_op.alter_column('prov_id',
               existing_type=sa.VARCHAR(length=255),
               comment=None,
               existing_comment='PROV-O Entity identifier (e.g., document_123_v2)',
               existing_nullable=False)
        batch_op.alter_column('prov_type',
               existing_type=sa.VARCHAR(length=100),
               comment=None,
               existing_comment='PROV-O Entity type (e.g., ont:Document, ont:ProcessedDocument)',
               existing_nullable=False)
        batch_op.alter_column('derived_from_entity',
               existing_type=sa.VARCHAR(length=255),
               comment=None,
               existing_comment='PROV-O wasDerivedFrom relationship',
               existing_nullable=True)
        batch_op.alter_column('generated_by_activity',
               existing_type=sa.VARCHAR(length=255),
               comment=None,
               existing_comment='PROV-O wasGeneratedBy relationship',
               existing_nullable=True)
        batch_op.alter_column('prov_metadata',
               existing_type=postgresql.JSONB(astext_type=sa.Text()),
               type_=sa.JSON(),
               existing_nullable=True)
        batch_op.drop_index(batch_op.f('idx_provenance_entities_derived_from'))
        batch_op.drop_index(batch_op.f('idx_provenance_entities_document_id'))
        batch_op.drop_index(batch_op.f('idx_provenance_entities_experiment_id'))
        batch_op.drop_index(batch_op.f('idx_provenance_entities_generated_by'))
        batch_op.drop_index(batch_op.f('idx_provenance_entities_prov_id'))
        batch_op.drop_index(batch_op.f('idx_provenance_entities_prov_type'))
        batch_op.drop_constraint(batch_op.f('provenance_entities_prov_id_key'), type_='unique')
        batch_op.create_index(batch_op.f('ix_provenance_entities_derived_from_entity'), ['derived_from_entity'], unique=False)
        batch_op.create_index(batch_op.f('ix_provenance_entities_document_id'), ['document_id'], unique=False)
        batch_op.create_index(batch_op.f('ix_provenance_entities_experiment_id'), ['experiment_id'], unique=False)
        batch_op.create_index(batch_op.f('ix_provenance_entities_generated_by_activity'), ['generated_by_activity'], unique=False)
        batch_op.create_index(batch_op.f('ix_provenance_entities_prov_id'), ['prov_id'], unique=True)
        batch_op.drop_constraint(batch_op.f('fk_provenance_entities_document'), type_='foreignkey')
        batch_op.drop_constraint(batch_op.f('fk_provenance_entities_experiment'), type_='foreignkey')
        batch_op.create_foreign_key(None, 'documents', ['document_id'], ['id'])
        batch_op.create_foreign_key(None, 'experiments', ['experiment_id'], ['id'])
        batch_op.drop_table_comment(
        existing_comment='PROV-O Entity model representing first-class provenance entities'
    )

    with op.batch_alter_table('semantic_drift_activities', schema=None) as batch_op:
        batch_op.drop_index(batch_op.f('idx_drift_activities_agent'))
        batch_op.drop_index(batch_op.f('idx_drift_activities_generated_entity'))
        batch_op.drop_index(batch_op.f('idx_drift_activities_periods'))
        batch_op.drop_index(batch_op.f('idx_drift_activities_status'))
        batch_op.drop_index(batch_op.f('idx_drift_activities_used_entity'))

    with op.batch_alter_table('term_version_anchors', schema=None) as batch_op:
        batch_op.drop_index(batch_op.f('idx_term_version_anchors_anchor'))
        batch_op.drop_index(batch_op.f('idx_term_version_anchors_similarity'))
        batch_op.drop_index(batch_op.f('idx_term_version_anchors_version'))

    with op.batch_alter_table('term_versions', schema=None) as batch_op:
        batch_op.alter_column('source_citation',
               existing_type=sa.TEXT(),
               comment=None,
               existing_comment='Academic citation for this temporal version meaning (e.g., dictionary reference, paper, etc.)',
               existing_nullable=True)
        batch_op.drop_index(batch_op.f('idx_term_versions_corpus'))
        batch_op.drop_index(batch_op.f('idx_term_versions_current'), postgresql_where='(is_current = true)')
        batch_op.drop_index(batch_op.f('idx_term_versions_fuzziness'))
        batch_op.drop_index(batch_op.f('idx_term_versions_temporal_period'))
        batch_op.drop_index(batch_op.f('idx_term_versions_temporal_years'))
        batch_op.drop_index(batch_op.f('idx_term_versions_term_id'))

    with op.batch_alter_table('terms', schema=None) as batch_op:
        batch_op.drop_index(batch_op.f('idx_terms_created_by'))
        batch_op.drop_index(batch_op.f('idx_terms_research_domain'))
        batch_op.drop_index(batch_op.f('idx_terms_status'))
        batch_op.drop_index(batch_op.f('idx_terms_text'))

    with op.batch_alter_table('text_segments', schema=None) as batch_op:
        batch_op.drop_index(batch_op.f('idx_text_segments_doc_method'))
        batch_op.drop_index(batch_op.f('idx_text_segments_job_id'))
        batch_op.drop_index(batch_op.f('idx_text_segments_segmentation_method'))
        batch_op.drop_constraint(batch_op.f('text_segments_segmentation_job_id_fkey'), type_='foreignkey')
        batch_op.drop_column('segmentation_method')
        batch_op.drop_column('segmentation_job_id')

    with op.batch_alter_table('tool_execution_logs', schema=None) as batch_op:
        batch_op.alter_column('execution_order',
               existing_type=sa.INTEGER(),
               comment=None,
               existing_comment='Order in processing pipeline (0 = first, higher = later)',
               existing_nullable=True)
        batch_op.alter_column('output_data',
               existing_type=postgresql.JSONB(astext_type=sa.Text()),
               type_=postgresql.JSON(astext_type=sa.Text()),
               existing_nullable=True)
        batch_op.alter_column('output_quality_score',
               existing_type=sa.NUMERIC(precision=4, scale=3),
               comment=None,
               existing_comment='Quality assessment of tool output (0.0 = poor, 1.0 = excellent)',
               existing_nullable=True)
        batch_op.drop_index(batch_op.f('idx_tool_execution_logs_decision_order'))
        batch_op.drop_index(batch_op.f('idx_tool_execution_logs_status'))
        batch_op.drop_index(batch_op.f('idx_tool_execution_logs_tool_name'))
        batch_op.create_index('idx_tool_execution_decision_order', ['orchestration_decision_id', 'execution_order'], unique=False)
        batch_op.create_index(batch_op.f('ix_tool_execution_logs_execution_status'), ['execution_status'], unique=False)
        batch_op.create_index(batch_op.f('ix_tool_execution_logs_orchestration_decision_id'), ['orchestration_decision_id'], unique=False)
        batch_op.create_index(batch_op.f('ix_tool_execution_logs_tool_name'), ['tool_name'], unique=False)
        batch_op.drop_constraint(batch_op.f('tool_execution_logs_orchestration_decision_id_fkey'), type_='foreignkey')
        batch_op.create_foreign_key(None, 'orchestration_decisions', ['orchestration_decision_id'], ['id'])
        batch_op.drop_table_comment(
        existing_comment='Detailed logs of individual NLP tool execution with performance metrics'
    )

    # ### end Alembic commands ###


def downgrade():
    # ### commands auto generated by Alembic - please adjust! ###
    with op.batch_alter_table('tool_execution_logs', schema=None) as batch_op:
        batch_op.create_table_comment(
        'Detailed logs of individual NLP tool execution with performance metrics',
        existing_comment=None
    )
        batch_op.drop_constraint(None, type_='foreignkey')
        batch_op.create_foreign_key(batch_op.f('tool_execution_logs_orchestration_decision_id_fkey'), 'orchestration_decisions', ['orchestration_decision_id'], ['id'], ondelete='CASCADE')
        batch_op.drop_index(batch_op.f('ix_tool_execution_logs_tool_name'))
        batch_op.drop_index(batch_op.f('ix_tool_execution_logs_orchestration_decision_id'))
        batch_op.drop_index(batch_op.f('ix_tool_execution_logs_execution_status'))
        batch_op.drop_index('idx_tool_execution_decision_order')
        batch_op.create_index(batch_op.f('idx_tool_execution_logs_tool_name'), ['tool_name'], unique=False)
        batch_op.create_index(batch_op.f('idx_tool_execution_logs_status'), ['execution_status'], unique=False)
        batch_op.create_index(batch_op.f('idx_tool_execution_logs_decision_order'), ['orchestration_decision_id', 'execution_order'], unique=False)
        batch_op.alter_column('output_quality_score',
               existing_type=sa.NUMERIC(precision=4, scale=3),
               comment='Quality assessment of tool output (0.0 = poor, 1.0 = excellent)',
               existing_nullable=True)
        batch_op.alter_column('output_data',
               existing_type=postgresql.JSON(astext_type=sa.Text()),
               type_=postgresql.JSONB(astext_type=sa.Text()),
               existing_nullable=True)
        batch_op.alter_column('execution_order',
               existing_type=sa.INTEGER(),
               comment='Order in processing pipeline (0 = first, higher = later)',
               existing_nullable=True)

    with op.batch_alter_table('text_segments', schema=None) as batch_op:
        batch_op.add_column(sa.Column('segmentation_job_id', sa.INTEGER(), autoincrement=False, nullable=True))
        batch_op.add_column(sa.Column('segmentation_method', sa.VARCHAR(length=50), server_default=sa.text("'manual'::character varying"), autoincrement=False, nullable=True))
        batch_op.create_foreign_key(batch_op.f('text_segments_segmentation_job_id_fkey'), 'processing_jobs', ['segmentation_job_id'], ['id'], ondelete='SET NULL')
        batch_op.create_index(batch_op.f('idx_text_segments_segmentation_method'), ['segmentation_method'], unique=False)
        batch_op.create_index(batch_op.f('idx_text_segments_job_id'), ['segmentation_job_id'], unique=False)
        batch_op.create_index(batch_op.f('idx_text_segments_doc_method'), ['document_id', 'segmentation_method'], unique=False)

    with op.batch_alter_table('terms', schema=None) as batch_op:
        batch_op.create_index(batch_op.f('idx_terms_text'), ['term_text'], unique=False)
        batch_op.create_index(batch_op.f('idx_terms_status'), ['status'], unique=False)
        batch_op.create_index(batch_op.f('idx_terms_research_domain'), ['research_domain'], unique=False)
        batch_op.create_index(batch_op.f('idx_terms_created_by'), ['created_by'], unique=False)

    with op.batch_alter_table('term_versions', schema=None) as batch_op:
        batch_op.create_index(batch_op.f('idx_term_versions_term_id'), ['term_id'], unique=False)
        batch_op.create_index(batch_op.f('idx_term_versions_temporal_years'), ['temporal_start_year', 'temporal_end_year'], unique=False)
        batch_op.create_index(batch_op.f('idx_term_versions_temporal_period'), ['temporal_period'], unique=False)
        batch_op.create_index(batch_op.f('idx_term_versions_fuzziness'), ['fuzziness_score'], unique=False)
        batch_op.create_index(batch_op.f('idx_term_versions_current'), ['is_current'], unique=False, postgresql_where='(is_current = true)')
        batch_op.create_index(batch_op.f('idx_term_versions_corpus'), ['corpus_source'], unique=False)
        batch_op.alter_column('source_citation',
               existing_type=sa.TEXT(),
               comment='Academic citation for this temporal version meaning (e.g., dictionary reference, paper, etc.)',
               existing_nullable=True)

    with op.batch_alter_table('term_version_anchors', schema=None) as batch_op:
        batch_op.create_index(batch_op.f('idx_term_version_anchors_version'), ['term_version_id'], unique=False)
        batch_op.create_index(batch_op.f('idx_term_version_anchors_similarity'), [sa.literal_column('similarity_score DESC')], unique=False)
        batch_op.create_index(batch_op.f('idx_term_version_anchors_anchor'), ['context_anchor_id'], unique=False)

    with op.batch_alter_table('semantic_drift_activities', schema=None) as batch_op:
        batch_op.create_index(batch_op.f('idx_drift_activities_used_entity'), ['used_entity'], unique=False)
        batch_op.create_index(batch_op.f('idx_drift_activities_status'), ['activity_status'], unique=False)
        batch_op.create_index(batch_op.f('idx_drift_activities_periods'), ['start_period', 'end_period'], unique=False)
        batch_op.create_index(batch_op.f('idx_drift_activities_generated_entity'), ['generated_entity'], unique=False)
        batch_op.create_index(batch_op.f('idx_drift_activities_agent'), ['was_associated_with'], unique=False)

    with op.batch_alter_table('provenance_entities', schema=None) as batch_op:
        batch_op.create_table_comment(
        'PROV-O Entity model representing first-class provenance entities',
        existing_comment=None
    )
        batch_op.drop_constraint(None, type_='foreignkey')
        batch_op.drop_constraint(None, type_='foreignkey')
        batch_op.create_foreign_key(batch_op.f('fk_provenance_entities_experiment'), 'experiments', ['experiment_id'], ['id'], ondelete='SET NULL')
        batch_op.create_foreign_key(batch_op.f('fk_provenance_entities_document'), 'documents', ['document_id'], ['id'], ondelete='CASCADE')
        batch_op.drop_index(batch_op.f('ix_provenance_entities_prov_id'))
        batch_op.drop_index(batch_op.f('ix_provenance_entities_generated_by_activity'))
        batch_op.drop_index(batch_op.f('ix_provenance_entities_experiment_id'))
        batch_op.drop_index(batch_op.f('ix_provenance_entities_document_id'))
        batch_op.drop_index(batch_op.f('ix_provenance_entities_derived_from_entity'))
        batch_op.create_unique_constraint(batch_op.f('provenance_entities_prov_id_key'), ['prov_id'], postgresql_nulls_not_distinct=False)
        batch_op.create_index(batch_op.f('idx_provenance_entities_prov_type'), ['prov_type'], unique=False)
        batch_op.create_index(batch_op.f('idx_provenance_entities_prov_id'), ['prov_id'], unique=False)
        batch_op.create_index(batch_op.f('idx_provenance_entities_generated_by'), ['generated_by_activity'], unique=False)
        batch_op.create_index(batch_op.f('idx_provenance_entities_experiment_id'), ['experiment_id'], unique=False)
        batch_op.create_index(batch_op.f('idx_provenance_entities_document_id'), ['document_id'], unique=False)
        batch_op.create_index(batch_op.f('idx_provenance_entities_derived_from'), ['derived_from_entity'], unique=False)
        batch_op.alter_column('prov_metadata',
               existing_type=sa.JSON(),
               type_=postgresql.JSONB(astext_type=sa.Text()),
               existing_nullable=True)
        batch_op.alter_column('generated_by_activity',
               existing_type=sa.VARCHAR(length=255),
               comment='PROV-O wasGeneratedBy relationship',
               existing_nullable=True)
        batch_op.alter_column('derived_from_entity',
               existing_type=sa.VARCHAR(length=255),
               comment='PROV-O wasDerivedFrom relationship',
               existing_nullable=True)
        batch_op.alter_column('prov_type',
               existing_type=sa.VARCHAR(length=100),
               comment='PROV-O Entity type (e.g., ont:Document, ont:ProcessedDocument)',
               existing_nullable=False)
        batch_op.alter_column('prov_id',
               existing_type=sa.VARCHAR(length=255),
               comment='PROV-O Entity identifier (e.g., document_123_v2)',
               existing_nullable=False)

    with op.batch_alter_table('provenance_activities', schema=None) as batch_op:
        batch_op.create_table_comment(
        'PROV-O Activity model representing processing activities',
        existing_comment=None
    )
        batch_op.drop_constraint(None, type_='foreignkey')
        batch_op.drop_constraint(None, type_='foreignkey')
        batch_op.create_foreign_key(batch_op.f('fk_provenance_activities_processing_job'), 'processing_jobs', ['processing_job_id'], ['id'], ondelete='CASCADE')
        batch_op.create_foreign_key(batch_op.f('fk_provenance_activities_experiment'), 'experiments', ['experiment_id'], ['id'], ondelete='SET NULL')
        batch_op.drop_index(batch_op.f('ix_provenance_activities_prov_id'))
        batch_op.drop_index(batch_op.f('ix_provenance_activities_processing_job_id'))
        batch_op.drop_index(batch_op.f('ix_provenance_activities_experiment_id'))
        batch_op.create_unique_constraint(batch_op.f('provenance_activities_prov_id_key'), ['prov_id'], postgresql_nulls_not_distinct=False)
        batch_op.create_index(batch_op.f('idx_provenance_activities_prov_type'), ['prov_type'], unique=False)
        batch_op.create_index(batch_op.f('idx_provenance_activities_prov_id'), ['prov_id'], unique=False)
        batch_op.create_index(batch_op.f('idx_provenance_activities_processing_job_id'), ['processing_job_id'], unique=False)
        batch_op.create_index(batch_op.f('idx_provenance_activities_experiment_id'), ['experiment_id'], unique=False)
        batch_op.create_index(batch_op.f('idx_provenance_activities_activity_type'), ['activity_type'], unique=False)
        batch_op.alter_column('activity_metadata',
               existing_type=sa.JSON(),
               type_=postgresql.JSONB(astext_type=sa.Text()),
               existing_nullable=True)
        batch_op.alter_column('used_plan',
               existing_type=sa.VARCHAR(length=255),
               comment='PROV-O used plan/protocol',
               existing_nullable=True)
        batch_op.alter_column('was_associated_with',
               existing_type=sa.VARCHAR(length=255),
               comment='PROV-O wasAssociatedWith agent',
               existing_nullable=True)
        batch_op.alter_column('prov_type',
               existing_type=sa.VARCHAR(length=100),
               comment='PROV-O Activity type (e.g., ont:EmbeddingsProcessing)',
               existing_nullable=False)
        batch_op.alter_column('prov_id',
               existing_type=sa.VARCHAR(length=255),
               comment='PROV-O Activity identifier (e.g., activity_embeddings_456)',
               existing_nullable=False)

    with op.batch_alter_table('orchestration_overrides', schema=None) as batch_op:
        batch_op.create_table_comment(
        'Manual overrides applied by researchers to specific orchestration decisions',
        existing_comment=None
    )
        batch_op.drop_constraint(None, type_='foreignkey')
        batch_op.create_foreign_key(batch_op.f('orchestration_overrides_orchestration_decision_id_fkey'), 'orchestration_decisions', ['orchestration_decision_id'], ['id'], ondelete='CASCADE')
        batch_op.drop_index(batch_op.f('ix_orchestration_overrides_orchestration_decision_id'))
        batch_op.drop_index('idx_override_decision_researcher')
        batch_op.create_index(batch_op.f('idx_orchestration_overrides_decision_researcher'), ['orchestration_decision_id', 'researcher_id'], unique=False)
        batch_op.create_index(batch_op.f('idx_orchestration_overrides_applied_at'), ['applied_at'], unique=False)
        batch_op.alter_column('performance_comparison',
               existing_type=postgresql.JSON(astext_type=sa.Text()),
               type_=postgresql.JSONB(astext_type=sa.Text()),
               existing_nullable=True)
        batch_op.alter_column('execution_results',
               existing_type=postgresql.JSON(astext_type=sa.Text()),
               type_=postgresql.JSONB(astext_type=sa.Text()),
               existing_nullable=True)
        batch_op.alter_column('expert_knowledge_applied',
               existing_type=postgresql.JSON(astext_type=sa.Text()),
               type_=postgresql.JSONB(astext_type=sa.Text()),
               existing_nullable=True)
        batch_op.alter_column('overridden_decision',
               existing_type=postgresql.JSON(astext_type=sa.Text()),
               type_=postgresql.JSONB(astext_type=sa.Text()),
               existing_nullable=False)
        batch_op.alter_column('original_decision',
               existing_type=postgresql.JSON(astext_type=sa.Text()),
               type_=postgresql.JSONB(astext_type=sa.Text()),
               existing_nullable=False)

    with op.batch_alter_table('orchestration_feedback', schema=None) as batch_op:
        batch_op.create_table_comment(
        'Researcher feedback on orchestration decisions for continuous improvement',
        existing_comment=None
    )
        batch_op.drop_constraint(None, type_='foreignkey')
        batch_op.create_foreign_key(batch_op.f('orchestration_feedback_orchestration_decision_id_fkey'), 'orchestration_decisions', ['orchestration_decision_id'], ['id'], ondelete='CASCADE')
        batch_op.drop_index(batch_op.f('ix_orchestration_feedback_provided_at'))
        batch_op.drop_index(batch_op.f('ix_orchestration_feedback_orchestration_decision_id'))
        batch_op.drop_index(batch_op.f('ix_orchestration_feedback_feedback_type'))
        batch_op.drop_index(batch_op.f('ix_orchestration_feedback_feedback_status'))
        batch_op.drop_index('idx_feedback_type_status')
        batch_op.drop_index('idx_feedback_decision_researcher')
        batch_op.create_index(batch_op.f('idx_orchestration_feedback_type_status'), ['feedback_type', 'feedback_status'], unique=False)
        batch_op.create_index(batch_op.f('idx_orchestration_feedback_provided_at'), ['provided_at'], unique=False)
        batch_op.create_index(batch_op.f('idx_orchestration_feedback_decision_researcher'), ['orchestration_decision_id', 'researcher_id'], unique=False)
        batch_op.alter_column('suggested_tools',
               existing_type=postgresql.ARRAY(sa.String()),
               type_=postgresql.ARRAY(sa.TEXT()),
               existing_nullable=True)
        batch_op.alter_column('domain_specific_factors',
               existing_type=postgresql.JSON(astext_type=sa.Text()),
               type_=postgresql.JSONB(astext_type=sa.Text()),
               comment='Domain knowledge that LLM missed in original decision',
               existing_nullable=True)
        batch_op.alter_column('researcher_preference',
               existing_type=postgresql.JSON(astext_type=sa.Text()),
               type_=postgresql.JSONB(astext_type=sa.Text()),
               existing_nullable=True)
        batch_op.alter_column('original_decision',
               existing_type=postgresql.JSON(astext_type=sa.Text()),
               type_=postgresql.JSONB(astext_type=sa.Text()),
               existing_nullable=True)
        batch_op.alter_column('researcher_expertise',
               existing_type=postgresql.JSON(astext_type=sa.Text()),
               type_=postgresql.JSONB(astext_type=sa.Text()),
               comment='Researcher expertise profile for weighting feedback authority',
               existing_nullable=True)

    with op.batch_alter_table('orchestration_decisions', schema=None) as batch_op:
        batch_op.create_table_comment(
        'PROV-O compliant logging of LLM orchestration decisions for tool selection and coordination',
        existing_comment=None
    )
        batch_op.drop_index(batch_op.f('ix_orchestration_decisions_term_text'))
        batch_op.drop_index(batch_op.f('ix_orchestration_decisions_experiment_id'))
        batch_op.drop_index(batch_op.f('ix_orchestration_decisions_document_id'))
        batch_op.drop_index(batch_op.f('ix_orchestration_decisions_created_at'))
        batch_op.drop_index(batch_op.f('ix_orchestration_decisions_activity_status'))
        batch_op.drop_index('idx_orchestration_term_time')
        batch_op.drop_index('idx_orchestration_experiment')
        batch_op.create_index(batch_op.f('idx_orchestration_decisions_term_time'), ['term_text', 'created_at'], unique=False)
        batch_op.create_index(batch_op.f('idx_orchestration_decisions_status'), ['activity_status'], unique=False)
        batch_op.create_index(batch_op.f('idx_orchestration_decisions_experiment'), ['experiment_id', 'created_at'], unique=False)
        batch_op.create_index(batch_op.f('idx_orchestration_decisions_document'), ['document_id'], unique=False)
        batch_op.create_index(batch_op.f('idx_orchestration_decisions_agent'), ['was_associated_with'], unique=False)
        batch_op.alter_column('tool_execution_success',
               existing_type=postgresql.JSON(astext_type=sa.Text()),
               type_=postgresql.JSONB(astext_type=sa.Text()),
               comment='Per-tool success rates and validation results',
               existing_nullable=True)
        batch_op.alter_column('decision_factors',
               existing_type=postgresql.JSON(astext_type=sa.Text()),
               type_=postgresql.JSONB(astext_type=sa.Text()),
               comment='Structured reasoning components for decision analysis',
               existing_nullable=True)
        batch_op.alter_column('selected_tools',
               existing_type=postgresql.ARRAY(sa.String()),
               type_=postgresql.ARRAY(sa.TEXT()),
               existing_nullable=True)
        batch_op.alter_column('document_characteristics',
               existing_type=postgresql.JSON(astext_type=sa.Text()),
               type_=postgresql.JSONB(astext_type=sa.Text()),
               existing_nullable=True)
        batch_op.alter_column('input_metadata',
               existing_type=postgresql.JSON(astext_type=sa.Text()),
               type_=postgresql.JSONB(astext_type=sa.Text()),
               comment='Document metadata that influenced tool selection (year, domain, format, length)',
               existing_nullable=True)

    with op.batch_alter_table('multi_model_consensus', schema=None) as batch_op:
        batch_op.create_table_comment(
        'Multi-model validation and consensus decision logging',
        existing_comment=None
    )
        batch_op.drop_constraint(None, type_='foreignkey')
        batch_op.create_foreign_key(batch_op.f('multi_model_consensus_orchestration_decision_id_fkey'), 'orchestration_decisions', ['orchestration_decision_id'], ['id'], ondelete='CASCADE')
        batch_op.drop_index(batch_op.f('ix_multi_model_consensus_orchestration_decision_id'))
        batch_op.create_index(batch_op.f('idx_multi_model_consensus_reached'), ['consensus_reached'], unique=False)
        batch_op.create_index(batch_op.f('idx_multi_model_consensus_decision'), ['orchestration_decision_id'], unique=False)
        batch_op.alter_column('disagreement_areas',
               existing_type=postgresql.JSON(astext_type=sa.Text()),
               type_=postgresql.JSONB(astext_type=sa.Text()),
               comment='Specific areas where models disagreed',
               existing_nullable=True)
        batch_op.alter_column('final_decision',
               existing_type=postgresql.JSON(astext_type=sa.Text()),
               type_=postgresql.JSONB(astext_type=sa.Text()),
               existing_nullable=True)
        batch_op.alter_column('model_agreement_matrix',
               existing_type=postgresql.JSON(astext_type=sa.Text()),
               type_=postgresql.JSONB(astext_type=sa.Text()),
               comment='Pairwise agreement scores between models',
               existing_nullable=True)
        batch_op.alter_column('model_confidence_scores',
               existing_type=postgresql.JSON(astext_type=sa.Text()),
               type_=postgresql.JSONB(astext_type=sa.Text()),
               existing_nullable=True)
        batch_op.alter_column('model_responses',
               existing_type=postgresql.JSON(astext_type=sa.Text()),
               type_=postgresql.JSONB(astext_type=sa.Text()),
               existing_nullable=True)
        batch_op.alter_column('models_involved',
               existing_type=postgresql.ARRAY(sa.String()),
               type_=postgresql.ARRAY(sa.TEXT()),
               existing_nullable=True)

    with op.batch_alter_table('learning_patterns', schema=None) as batch_op:
        batch_op.create_table_comment(
        'Codified learning patterns derived from researcher feedback',
        existing_comment=None
    )
        batch_op.drop_index(batch_op.f('ix_learning_patterns_pattern_type'))
        batch_op.drop_index(batch_op.f('ix_learning_patterns_pattern_status'))
        batch_op.drop_index(batch_op.f('ix_learning_patterns_context_signature'))
        batch_op.drop_index('idx_pattern_context_type')
        batch_op.create_index(batch_op.f('idx_learning_patterns_success_rate'), [sa.literal_column('success_rate DESC')], unique=False)
        batch_op.create_index(batch_op.f('idx_learning_patterns_status'), ['pattern_status'], unique=False)
        batch_op.create_index(batch_op.f('idx_learning_patterns_context_type'), ['context_signature', 'pattern_type'], unique=False)
        batch_op.alter_column('researcher_authority',
               existing_type=postgresql.JSON(astext_type=sa.Text()),
               type_=postgresql.JSONB(astext_type=sa.Text()),
               comment='Authority assessment of source researcher for weighting',
               existing_nullable=True)
        batch_op.alter_column('recommendations',
               existing_type=postgresql.JSON(astext_type=sa.Text()),
               type_=postgresql.JSONB(astext_type=sa.Text()),
               existing_nullable=False)
        batch_op.alter_column('conditions',
               existing_type=postgresql.JSON(astext_type=sa.Text()),
               type_=postgresql.JSONB(astext_type=sa.Text()),
               existing_nullable=False)
        batch_op.alter_column('context_signature',
               existing_type=sa.VARCHAR(length=200),
               comment='Signature for matching similar decision contexts',
               existing_nullable=False)

    with op.batch_alter_table('fuzziness_adjustments', schema=None) as batch_op:
        batch_op.create_index(batch_op.f('idx_fuzziness_adjustments_version'), ['term_version_id'], unique=False)
        batch_op.create_index(batch_op.f('idx_fuzziness_adjustments_user'), ['adjusted_by'], unique=False)

    with op.batch_alter_table('experiment_references', schema=None) as batch_op:
        batch_op.create_index(batch_op.f('idx_experiment_references_reference'), ['reference_id'], unique=False)
        batch_op.create_index(batch_op.f('idx_experiment_references_experiment'), ['experiment_id'], unique=False)

    with op.batch_alter_table('experiment_documents', schema=None) as batch_op:
        batch_op.add_column(sa.Column('nlp_results', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True, comment='Experiment-specific NLP analysis results'))
        batch_op.add_column(sa.Column('updated_at', postgresql.TIMESTAMP(), server_default=sa.text('CURRENT_TIMESTAMP'), autoincrement=False, nullable=True))
        batch_op.add_column(sa.Column('embeddings_applied', sa.BOOLEAN(), server_default=sa.text('false'), autoincrement=False, nullable=True, comment='Whether embeddings have been generated for this experiment'))
        batch_op.add_column(sa.Column('segments_created', sa.BOOLEAN(), server_default=sa.text('false'), autoincrement=False, nullable=True, comment='Whether document has been segmented for this experiment'))
        batch_op.add_column(sa.Column('processing_metadata', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True, comment='General experiment-specific processing metadata'))
        batch_op.add_column(sa.Column('processed_at', postgresql.TIMESTAMP(), autoincrement=False, nullable=True))
        batch_op.add_column(sa.Column('embeddings_metadata', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True, comment='Embedding model info and metrics for this experiment'))
        batch_op.add_column(sa.Column('segments_metadata', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True, comment='Segmentation parameters and results'))
        batch_op.add_column(sa.Column('nlp_analysis_completed', sa.BOOLEAN(), server_default=sa.text('false'), autoincrement=False, nullable=True, comment='Whether NLP analysis is complete for this experiment'))
        batch_op.add_column(sa.Column('processing_status', sa.VARCHAR(length=20), server_default=sa.text("'pending'::character varying"), autoincrement=False, nullable=True, comment='Status: pending, processing, completed, error'))
        batch_op.create_index(batch_op.f('idx_experiment_documents_updated'), ['updated_at'], unique=False)
        batch_op.create_index(batch_op.f('idx_experiment_documents_status'), ['processing_status'], unique=False)

    with op.batch_alter_table('documents', schema=None) as batch_op:
        batch_op.drop_constraint(None, type_='foreignkey')
        batch_op.drop_constraint(None, type_='foreignkey')
        batch_op.drop_constraint(None, type_='foreignkey')
        batch_op.create_foreign_key(batch_op.f('fk_documents_parent_document_id'), 'documents', ['parent_document_id'], ['id'], ondelete='CASCADE')
        batch_op.create_foreign_key(batch_op.f('fk_documents_source'), 'documents', ['source_document_id'], ['id'], ondelete='CASCADE')
        batch_op.create_foreign_key(batch_op.f('fk_documents_experiment'), 'experiments', ['experiment_id'], ['id'], ondelete='SET NULL')
        batch_op.drop_index(batch_op.f('ix_documents_source_document_id'))
        batch_op.drop_index(batch_op.f('ix_documents_parent_document_id'))
        batch_op.drop_index(batch_op.f('ix_documents_experiment_id'))
        batch_op.create_index(batch_op.f('idx_documents_version_type'), ['version_type'], unique=False)
        batch_op.create_index(batch_op.f('idx_documents_version_number'), ['version_number'], unique=False)
        batch_op.create_index(batch_op.f('idx_documents_type'), ['document_type'], unique=False)
        batch_op.create_index(batch_op.f('idx_documents_source_document_id'), ['source_document_id'], unique=False)
        batch_op.create_index(batch_op.f('idx_documents_parent'), ['parent_document_id'], unique=False)
        batch_op.create_index(batch_op.f('idx_documents_experiment_id'), ['experiment_id'], unique=False)
        batch_op.alter_column('processing_notes',
               existing_type=sa.TEXT(),
               comment='Notes about processing operations that created this version',
               existing_nullable=True)
        batch_op.alter_column('experiment_id',
               existing_type=sa.INTEGER(),
               comment='Associated experiment (for experimental versions)',
               existing_nullable=True)
        batch_op.alter_column('source_document_id',
               existing_type=sa.INTEGER(),
               comment='Original document this version derives from',
               existing_nullable=True)
        batch_op.alter_column('version_type',
               existing_type=sa.VARCHAR(length=20),
               nullable=True,
               comment='Type of version: original, processed, experimental',
               existing_server_default=sa.text("'original'::character varying"))
        batch_op.alter_column('version_number',
               existing_type=sa.INTEGER(),
               nullable=True,
               comment='Sequential version number within a document family',
               existing_server_default=sa.text('1'))
        batch_op.alter_column('processing_metadata',
               existing_type=postgresql.JSON(astext_type=sa.Text()),
               comment='General metadata for processing info, embeddings, and document analysis',
               existing_nullable=True)
        batch_op.drop_column('composite_metadata')
        batch_op.drop_column('composite_sources')

    with op.batch_alter_table('context_anchors', schema=None) as batch_op:
        batch_op.create_index(batch_op.f('idx_context_anchors_term'), ['anchor_term'], unique=False)
        batch_op.create_index(batch_op.f('idx_context_anchors_frequency'), [sa.literal_column('frequency DESC')], unique=False)

    with op.batch_alter_table('analysis_agents', schema=None) as batch_op:
        batch_op.create_index(batch_op.f('idx_analysis_agents_type'), ['agent_type'], unique=False)
        batch_op.create_index(batch_op.f('idx_analysis_agents_active'), ['is_active'], unique=False, postgresql_where='(is_active = true)')

    op.create_table('document_embeddings',
    sa.Column('id', sa.INTEGER(), autoincrement=True, nullable=False),
    sa.Column('document_id', sa.INTEGER(), autoincrement=False, nullable=True),
    sa.Column('term', sa.VARCHAR(length=200), autoincrement=False, nullable=False),
    sa.Column('period', sa.INTEGER(), autoincrement=False, nullable=True),
    sa.Column('embedding', sa.NullType(), autoincrement=False, nullable=True),
    sa.Column('model_name', sa.VARCHAR(length=100), autoincrement=False, nullable=True),
    sa.Column('context_window', sa.TEXT(), autoincrement=False, nullable=True),
    sa.Column('extraction_method', sa.VARCHAR(length=50), autoincrement=False, nullable=True),
    sa.Column('metadata', postgresql.JSONB(astext_type=sa.Text()), autoincrement=False, nullable=True),
    sa.Column('created_at', postgresql.TIMESTAMP(), server_default=sa.text('now()'), autoincrement=False, nullable=True),
    sa.Column('updated_at', postgresql.TIMESTAMP(), server_default=sa.text('now()'), autoincrement=False, nullable=True),
    sa.ForeignKeyConstraint(['document_id'], ['documents.id'], name=op.f('document_embeddings_document_id_fkey')),
    sa.PrimaryKeyConstraint('id', name=op.f('document_embeddings_pkey'))
    )
    with op.batch_alter_table('document_embeddings', schema=None) as batch_op:
        batch_op.create_index(batch_op.f('idx_embeddings_term_period'), ['term', 'period'], unique=False)
        batch_op.create_index(batch_op.f('idx_embeddings_model'), ['model_name'], unique=False)
        batch_op.create_index(batch_op.f('idx_embeddings_document'), ['document_id'], unique=False)

    op.create_table('prov_agents',
    sa.Column('agent_id', sa.UUID(), server_default=sa.text('gen_random_uuid()'), autoincrement=False, nullable=False),
    sa.Column('agent_type', sa.VARCHAR(length=20), autoincrement=False, nullable=False),
    sa.Column('foaf_name', sa.VARCHAR(length=255), autoincrement=False, nullable=True),
    sa.Column('foaf_givenname', sa.VARCHAR(length=255), autoincrement=False, nullable=True),
    sa.Column('foaf_mbox', sa.VARCHAR(length=255), autoincrement=False, nullable=True),
    sa.Column('foaf_homepage', sa.VARCHAR(length=500), autoincrement=False, nullable=True),
    sa.Column('agent_metadata', postgresql.JSONB(astext_type=sa.Text()), server_default=sa.text("'{}'::jsonb"), autoincrement=False, nullable=True),
    sa.Column('created_at', postgresql.TIMESTAMP(timezone=True), server_default=sa.text('CURRENT_TIMESTAMP'), autoincrement=False, nullable=True),
    sa.Column('updated_at', postgresql.TIMESTAMP(timezone=True), server_default=sa.text('CURRENT_TIMESTAMP'), autoincrement=False, nullable=True),
    sa.CheckConstraint("agent_type::text = ANY (ARRAY['Person'::character varying, 'Organization'::character varying, 'SoftwareAgent'::character varying]::text[])", name='prov_agents_agent_type_check'),
    sa.PrimaryKeyConstraint('agent_id', name='prov_agents_pkey'),
    postgresql_ignore_search_path=False
    )
    with op.batch_alter_table('prov_agents', schema=None) as batch_op:
        batch_op.create_index(batch_op.f('idx_prov_agents_type'), ['agent_type'], unique=False)
        batch_op.create_index(batch_op.f('idx_prov_agents_name'), ['foaf_name'], unique=False)

    op.create_table('version_changelog',
    sa.Column('id', sa.INTEGER(), autoincrement=True, nullable=False),
    sa.Column('document_id', sa.INTEGER(), autoincrement=False, nullable=False),
    sa.Column('version_number', sa.INTEGER(), autoincrement=False, nullable=False),
    sa.Column('change_type', sa.VARCHAR(length=50), autoincrement=False, nullable=False),
    sa.Column('change_description', sa.TEXT(), autoincrement=False, nullable=True),
    sa.Column('previous_version', sa.INTEGER(), autoincrement=False, nullable=True),
    sa.Column('created_at', postgresql.TIMESTAMP(), server_default=sa.text('CURRENT_TIMESTAMP'), autoincrement=False, nullable=False),
    sa.Column('created_by', sa.INTEGER(), autoincrement=False, nullable=False),
    sa.Column('processing_metadata', postgresql.JSONB(astext_type=sa.Text()), autoincrement=False, nullable=True),
    sa.ForeignKeyConstraint(['created_by'], ['users.id'], name=op.f('version_changelog_created_by_fkey')),
    sa.ForeignKeyConstraint(['document_id'], ['documents.id'], name=op.f('version_changelog_document_id_fkey'), ondelete='CASCADE'),
    sa.PrimaryKeyConstraint('id', name=op.f('version_changelog_pkey')),
    sa.UniqueConstraint('document_id', 'version_number', 'change_type', name=op.f('unique_document_version_change'), postgresql_include=[], postgresql_nulls_not_distinct=False)
    )
    with op.batch_alter_table('version_changelog', schema=None) as batch_op:
        batch_op.create_index(batch_op.f('idx_version_changelog_document_version'), ['document_id', 'version_number'], unique=False)
        batch_op.create_index(batch_op.f('idx_version_changelog_change_type'), ['change_type'], unique=False)

    op.create_table('prov_entities',
    sa.Column('entity_id', sa.UUID(), server_default=sa.text('gen_random_uuid()'), autoincrement=False, nullable=False),
    sa.Column('entity_type', sa.VARCHAR(length=100), autoincrement=False, nullable=False),
    sa.Column('generatedattime', postgresql.TIMESTAMP(timezone=True), autoincrement=False, nullable=True),
    sa.Column('invalidatedattime', postgresql.TIMESTAMP(timezone=True), autoincrement=False, nullable=True),
    sa.Column('wasgeneratedby', sa.UUID(), autoincrement=False, nullable=True),
    sa.Column('wasattributedto', sa.UUID(), autoincrement=False, nullable=True),
    sa.Column('wasderivedfrom', sa.UUID(), autoincrement=False, nullable=True),
    sa.Column('entity_value', postgresql.JSONB(astext_type=sa.Text()), server_default=sa.text("'{}'::jsonb"), autoincrement=False, nullable=False),
    sa.Column('entity_metadata', postgresql.JSONB(astext_type=sa.Text()), server_default=sa.text("'{}'::jsonb"), autoincrement=False, nullable=True),
    sa.Column('character_start', sa.INTEGER(), autoincrement=False, nullable=True),
    sa.Column('character_end', sa.INTEGER(), autoincrement=False, nullable=True),
    sa.Column('created_at', postgresql.TIMESTAMP(timezone=True), server_default=sa.text('CURRENT_TIMESTAMP'), autoincrement=False, nullable=True),
    sa.CheckConstraint('character_start IS NULL AND character_end IS NULL OR character_start IS NOT NULL AND character_end IS NOT NULL AND character_start <= character_end', name=op.f('valid_character_positions')),
    sa.CheckConstraint('wasgeneratedby IS NOT NULL', name=op.f('must_have_generation_provenance')),
    sa.ForeignKeyConstraint(['wasattributedto'], ['prov_agents.agent_id'], name=op.f('prov_entities_wasattributedto_fkey')),
    sa.ForeignKeyConstraint(['wasderivedfrom'], ['prov_entities.entity_id'], name=op.f('prov_entities_wasderivedfrom_fkey')),
    sa.ForeignKeyConstraint(['wasgeneratedby'], ['prov_activities.activity_id'], name=op.f('prov_entities_wasgeneratedby_fkey')),
    sa.PrimaryKeyConstraint('entity_id', name=op.f('prov_entities_pkey'))
    )
    with op.batch_alter_table('prov_entities', schema=None) as batch_op:
        batch_op.create_index(batch_op.f('idx_prov_entities_type'), ['entity_type'], unique=False)
        batch_op.create_index(batch_op.f('idx_prov_entities_generated'), ['wasgeneratedby'], unique=False)
        batch_op.create_index(batch_op.f('idx_prov_entities_derived'), ['wasderivedfrom'], unique=False)
        batch_op.create_index(batch_op.f('idx_prov_entities_attributed'), ['wasattributedto'], unique=False)

    op.create_table('prov_relationships',
    sa.Column('relationship_id', sa.UUID(), server_default=sa.text('gen_random_uuid()'), autoincrement=False, nullable=False),
    sa.Column('relationship_type', sa.VARCHAR(length=50), autoincrement=False, nullable=False),
    sa.Column('subject_id', sa.UUID(), autoincrement=False, nullable=False),
    sa.Column('subject_type', sa.VARCHAR(length=20), autoincrement=False, nullable=False),
    sa.Column('object_id', sa.UUID(), autoincrement=False, nullable=False),
    sa.Column('object_type', sa.VARCHAR(length=20), autoincrement=False, nullable=False),
    sa.Column('relationship_metadata', postgresql.JSONB(astext_type=sa.Text()), server_default=sa.text("'{}'::jsonb"), autoincrement=False, nullable=True),
    sa.Column('created_at', postgresql.TIMESTAMP(timezone=True), server_default=sa.text('CURRENT_TIMESTAMP'), autoincrement=False, nullable=True),
    sa.CheckConstraint("object_type::text = ANY (ARRAY['Agent'::character varying, 'Activity'::character varying, 'Entity'::character varying]::text[])", name=op.f('prov_relationships_object_type_check')),
    sa.CheckConstraint("relationship_type::text = ANY (ARRAY['wasGeneratedBy'::character varying::text, 'wasAssociatedWith'::character varying::text, 'wasDerivedFrom'::character varying::text, 'wasInformedBy'::character varying::text, 'actedOnBehalfOf'::character varying::text, 'wasAttributedTo'::character varying::text, 'used'::character varying::text, 'wasStartedBy'::character varying::text, 'wasEndedBy'::character varying::text, 'wasQuotedFrom'::character varying::text, 'wasRevisionOf'::character varying::text, 'hadPrimarySource'::character varying::text, 'alternateOf'::character varying::text, 'specializationOf'::character varying::text])", name=op.f('prov_relationships_relationship_type_check')),
    sa.CheckConstraint("subject_type::text = ANY (ARRAY['Agent'::character varying, 'Activity'::character varying, 'Entity'::character varying]::text[])", name=op.f('prov_relationships_subject_type_check')),
    sa.PrimaryKeyConstraint('relationship_id', name=op.f('prov_relationships_pkey'))
    )
    with op.batch_alter_table('prov_relationships', schema=None) as batch_op:
        batch_op.create_index(batch_op.f('idx_prov_relationships_type'), ['relationship_type'], unique=False)
        batch_op.create_index(batch_op.f('idx_prov_relationships_subject'), ['subject_id', 'subject_type'], unique=False)
        batch_op.create_index(batch_op.f('idx_prov_relationships_object'), ['object_id', 'object_type'], unique=False)

    op.create_table('search_history',
    sa.Column('id', sa.INTEGER(), autoincrement=True, nullable=False),
    sa.Column('query', sa.TEXT(), autoincrement=False, nullable=False),
    sa.Column('query_type', sa.VARCHAR(length=50), autoincrement=False, nullable=True),
    sa.Column('results_count', sa.INTEGER(), autoincrement=False, nullable=True),
    sa.Column('execution_time', sa.DOUBLE_PRECISION(precision=53), autoincrement=False, nullable=True),
    sa.Column('user_id', sa.VARCHAR(length=255), autoincrement=False, nullable=True),
    sa.Column('ip_address', sa.VARCHAR(length=45), autoincrement=False, nullable=True),
    sa.Column('created_at', postgresql.TIMESTAMP(), autoincrement=False, nullable=True),
    sa.PrimaryKeyConstraint('id', name=op.f('search_history_pkey'))
    )
    op.create_table('document_processing_summary',
    sa.Column('id', sa.INTEGER(), autoincrement=True, nullable=False),
    sa.Column('document_id', sa.INTEGER(), autoincrement=False, nullable=False),
    sa.Column('processing_type', sa.VARCHAR(length=50), autoincrement=False, nullable=False),
    sa.Column('status', sa.VARCHAR(length=20), autoincrement=False, nullable=False),
    sa.Column('source_document_id', sa.INTEGER(), autoincrement=False, nullable=True),
    sa.Column('job_id', sa.INTEGER(), autoincrement=False, nullable=True),
    sa.Column('priority', sa.INTEGER(), server_default=sa.text('1'), autoincrement=False, nullable=True),
    sa.Column('created_at', postgresql.TIMESTAMP(), server_default=sa.text('now()'), autoincrement=False, nullable=True),
    sa.Column('updated_at', postgresql.TIMESTAMP(), server_default=sa.text('now()'), autoincrement=False, nullable=True),
    sa.ForeignKeyConstraint(['document_id'], ['documents.id'], name=op.f('document_processing_summary_document_id_fkey'), ondelete='CASCADE'),
    sa.ForeignKeyConstraint(['job_id'], ['processing_jobs.id'], name=op.f('document_processing_summary_job_id_fkey')),
    sa.ForeignKeyConstraint(['source_document_id'], ['documents.id'], name=op.f('document_processing_summary_source_document_id_fkey')),
    sa.PrimaryKeyConstraint('id', name=op.f('document_processing_summary_pkey')),
    sa.UniqueConstraint('document_id', 'processing_type', 'source_document_id', name=op.f('document_processing_summary_document_id_processing_type_sou_key'), postgresql_include=[], postgresql_nulls_not_distinct=False),
    comment='Efficient summary of processing capabilities available per document'
    )
    with op.batch_alter_table('document_processing_summary', schema=None) as batch_op:
        batch_op.create_index(batch_op.f('idx_processing_summary_type'), ['processing_type'], unique=False)
        batch_op.create_index(batch_op.f('idx_processing_summary_document'), ['document_id'], unique=False)

    op.create_table('ontologies',
    sa.Column('id', sa.INTEGER(), server_default=sa.text("nextval('ontologies_id_seq'::regclass)"), autoincrement=True, nullable=False),
    sa.Column('uuid', sa.UUID(), autoincrement=False, nullable=False),
    sa.Column('domain_id', sa.INTEGER(), autoincrement=False, nullable=True),
    sa.Column('name', sa.VARCHAR(length=255), autoincrement=False, nullable=False),
    sa.Column('base_uri', sa.TEXT(), autoincrement=False, nullable=False),
    sa.Column('description', sa.TEXT(), autoincrement=False, nullable=True),
    sa.Column('is_base', sa.BOOLEAN(), autoincrement=False, nullable=True),
    sa.Column('is_editable', sa.BOOLEAN(), autoincrement=False, nullable=True),
    sa.Column('parent_ontology_id', sa.INTEGER(), autoincrement=False, nullable=True),
    sa.Column('ontology_type', sa.VARCHAR(length=20), autoincrement=False, nullable=True),
    sa.Column('metadata', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True),
    sa.Column('created_at', postgresql.TIMESTAMP(timezone=True), autoincrement=False, nullable=False),
    sa.Column('updated_at', postgresql.TIMESTAMP(timezone=True), autoincrement=False, nullable=True),
    sa.ForeignKeyConstraint(['domain_id'], ['domains.id'], name='ontologies_domain_id_fkey'),
    sa.ForeignKeyConstraint(['parent_ontology_id'], ['ontologies.id'], name='ontologies_parent_ontology_id_fkey'),
    sa.PrimaryKeyConstraint('id', name='ontologies_pkey'),
    sa.UniqueConstraint('uuid', name='ontologies_uuid_key', postgresql_include=[], postgresql_nulls_not_distinct=False),
    postgresql_ignore_search_path=False
    )
    op.create_table('ontology_entities',
    sa.Column('id', sa.INTEGER(), autoincrement=True, nullable=False),
    sa.Column('ontology_id', sa.INTEGER(), autoincrement=False, nullable=False),
    sa.Column('entity_type', sa.VARCHAR(length=50), autoincrement=False, nullable=False),
    sa.Column('uri', sa.TEXT(), autoincrement=False, nullable=False),
    sa.Column('label', sa.VARCHAR(length=255), autoincrement=False, nullable=True),
    sa.Column('comment', sa.TEXT(), autoincrement=False, nullable=True),
    sa.Column('parent_uri', sa.TEXT(), autoincrement=False, nullable=True),
    sa.Column('domain', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True),
    sa.Column('range', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True),
    sa.Column('properties', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True),
    sa.Column('embedding', sa.NullType(), autoincrement=False, nullable=True),
    sa.Column('created_at', postgresql.TIMESTAMP(), autoincrement=False, nullable=True),
    sa.ForeignKeyConstraint(['ontology_id'], ['ontologies.id'], name=op.f('ontology_entities_ontology_id_fkey')),
    sa.PrimaryKeyConstraint('id', name=op.f('ontology_entities_pkey'))
    )
    with op.batch_alter_table('ontology_entities', schema=None) as batch_op:
        batch_op.create_index(batch_op.f('ix_entity_embedding_vector'), ['embedding'], unique=False, postgresql_ops={'embedding': 'vector_cosine_ops'}, postgresql_with={'lists': '100'}, postgresql_using='ivfflat')
        batch_op.create_index(batch_op.f('idx_ontology_entity'), ['ontology_id', 'entity_type'], unique=False)
        batch_op.create_index(batch_op.f('idx_entity_type'), ['entity_type'], unique=False)
        batch_op.create_index(batch_op.f('idx_entity_label'), ['label'], unique=False)

    op.create_table('prov_activities',
    sa.Column('activity_id', sa.UUID(), server_default=sa.text('gen_random_uuid()'), autoincrement=False, nullable=False),
    sa.Column('activity_type', sa.VARCHAR(length=100), autoincrement=False, nullable=False),
    sa.Column('startedattime', postgresql.TIMESTAMP(timezone=True), autoincrement=False, nullable=True),
    sa.Column('endedattime', postgresql.TIMESTAMP(timezone=True), autoincrement=False, nullable=True),
    sa.Column('wasassociatedwith', sa.UUID(), autoincrement=False, nullable=True),
    sa.Column('activity_parameters', postgresql.JSONB(astext_type=sa.Text()), server_default=sa.text("'{}'::jsonb"), autoincrement=False, nullable=True),
    sa.Column('activity_status', sa.VARCHAR(length=20), server_default=sa.text("'active'::character varying"), autoincrement=False, nullable=True),
    sa.Column('activity_metadata', postgresql.JSONB(astext_type=sa.Text()), server_default=sa.text("'{}'::jsonb"), autoincrement=False, nullable=True),
    sa.Column('created_at', postgresql.TIMESTAMP(timezone=True), server_default=sa.text('CURRENT_TIMESTAMP'), autoincrement=False, nullable=True),
    sa.CheckConstraint("activity_status::text = ANY (ARRAY['active'::character varying, 'completed'::character varying, 'failed'::character varying]::text[])", name=op.f('prov_activities_activity_status_check')),
    sa.CheckConstraint('startedattime IS NULL OR endedattime IS NULL OR startedattime <= endedattime', name=op.f('valid_activity_duration')),
    sa.ForeignKeyConstraint(['wasassociatedwith'], ['prov_agents.agent_id'], name=op.f('prov_activities_wasassociatedwith_fkey')),
    sa.PrimaryKeyConstraint('activity_id', name=op.f('prov_activities_pkey'))
    )
    with op.batch_alter_table('prov_activities', schema=None) as batch_op:
        batch_op.create_index(batch_op.f('idx_prov_activities_type'), ['activity_type'], unique=False)
        batch_op.create_index(batch_op.f('idx_prov_activities_started'), ['startedattime'], unique=False)
        batch_op.create_index(batch_op.f('idx_prov_activities_associated'), ['wasassociatedwith'], unique=False)

    op.create_table('domains',
    sa.Column('id', sa.INTEGER(), server_default=sa.text("nextval('domains_id_seq'::regclass)"), autoincrement=True, nullable=False),
    sa.Column('uuid', sa.UUID(), autoincrement=False, nullable=False),
    sa.Column('name', sa.VARCHAR(length=255), autoincrement=False, nullable=False),
    sa.Column('display_name', sa.VARCHAR(length=255), autoincrement=False, nullable=True),
    sa.Column('namespace_uri', sa.TEXT(), autoincrement=False, nullable=False),
    sa.Column('description', sa.TEXT(), autoincrement=False, nullable=True),
    sa.Column('metadata', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True),
    sa.Column('is_active', sa.BOOLEAN(), autoincrement=False, nullable=True),
    sa.Column('created_at', postgresql.TIMESTAMP(timezone=True), autoincrement=False, nullable=True),
    sa.Column('updated_at', postgresql.TIMESTAMP(timezone=True), autoincrement=False, nullable=True),
    sa.PrimaryKeyConstraint('id', name='domains_pkey'),
    sa.UniqueConstraint('name', name='domains_name_key', postgresql_include=[], postgresql_nulls_not_distinct=False),
    sa.UniqueConstraint('namespace_uri', name='domains_namespace_uri_key', postgresql_include=[], postgresql_nulls_not_distinct=False),
    sa.UniqueConstraint('uuid', name='domains_uuid_key', postgresql_include=[], postgresql_nulls_not_distinct=False),
    postgresql_ignore_search_path=False
    )
    op.create_table('ontology_versions',
    sa.Column('id', sa.INTEGER(), autoincrement=True, nullable=False),
    sa.Column('ontology_id', sa.INTEGER(), autoincrement=False, nullable=False),
    sa.Column('version_number', sa.INTEGER(), autoincrement=False, nullable=False),
    sa.Column('version_tag', sa.VARCHAR(length=50), autoincrement=False, nullable=True),
    sa.Column('content', sa.TEXT(), autoincrement=False, nullable=False),
    sa.Column('content_hash', sa.VARCHAR(length=64), autoincrement=False, nullable=True),
    sa.Column('change_summary', sa.TEXT(), autoincrement=False, nullable=True),
    sa.Column('created_by', sa.VARCHAR(length=255), autoincrement=False, nullable=True),
    sa.Column('created_at', postgresql.TIMESTAMP(timezone=True), autoincrement=False, nullable=False),
    sa.Column('is_current', sa.BOOLEAN(), autoincrement=False, nullable=True),
    sa.Column('is_draft', sa.BOOLEAN(), autoincrement=False, nullable=True),
    sa.Column('workflow_status', sa.VARCHAR(length=20), autoincrement=False, nullable=True),
    sa.Column('metadata', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True),
    sa.ForeignKeyConstraint(['ontology_id'], ['ontologies.id'], name=op.f('ontology_versions_ontology_id_fkey')),
    sa.PrimaryKeyConstraint('id', name=op.f('ontology_versions_pkey')),
    sa.UniqueConstraint('ontology_id', 'version_number', name=op.f('uq_ontology_version'), postgresql_include=[], postgresql_nulls_not_distinct=False)
    )
    # ### end Alembic commands ###
