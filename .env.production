# OntExtract Production Environment Configuration
# Copy this file to .env and fill in your actual API keys

# ==============================================================================
# DATABASE CONFIGURATION
# ==============================================================================
# Docker Compose will use this automatically
DATABASE_URL=postgresql://postgres:PASS@db:5432/ontextract_db

# ==============================================================================
# FLASK CONFIGURATION
# ==============================================================================
# IMPORTANT: Generate a new secret key for production!
# You can generate one with: python -c "import secrets; print(secrets.token_hex(32))"
SECRET_KEY=your_production_secret_key_change_this_immediately

FLASK_ENV=production
FLASK_APP=run.py
WTF_CSRF_ENABLED=True
DEBUG=False

# ==============================================================================
# LLM PROVIDER API KEYS (REQUIRED)
# ==============================================================================
# Anthropic Claude API Key (Required for Claude models)
# Get your key from: https://console.anthropic.com/
ANTHROPIC_API_KEY=sk-ant-api03-YOUR_ACTUAL_KEY_HERE

# OpenAI API Key (Required for GPT models)
# Get your key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-YOUR_ACTUAL_KEY_HERE

# Default LLM Provider (anthropic or openai)
DEFAULT_LLM_PROVIDER=anthropic

# ==============================================================================
# CLAUDE MODEL CONFIGURATION
# ==============================================================================
# Latest Claude models (as of January 2025):
# claude-opus-4-1-20250805 - Most capable and intelligent model
# claude-sonnet-4-20250514 - High-performance model with exceptional reasoning
# claude-3-7-sonnet-20250219 - High intelligence with extended thinking
# claude-3-5-haiku-20241022 - Fastest model for quick responses
CLAUDE_DEFAULT_MODEL=claude-sonnet-4-20250514
CLAUDE_API_VERSION=2023-06-01
CLAUDE_EMBEDDING_MODEL=claude-3-embedding

# ==============================================================================
# OPENAI MODEL CONFIGURATION
# ==============================================================================
# Latest OpenAI models (as of January 2025):
# gpt-4o - Latest multimodal model (text + vision)
# gpt-4o-mini - Cost-effective multimodal model
# gpt-4-turbo - Latest GPT-4 Turbo model
# gpt-3.5-turbo - Fast and cost-effective
OPENAI_DEFAULT_MODEL=gpt-4o
OPENAI_EMBEDDING_MODEL=text-embedding-3-small
# Alternative embedding models:
# text-embedding-3-large - Higher quality embeddings
# text-embedding-ada-002 - Legacy model

# ==============================================================================
# LLM PROVIDER CONFIGURATION
# ==============================================================================
# Priority order for LLM providers (comma-separated)
LLM_PROVIDER_PRIORITY=anthropic,openai

# Temperature settings for different use cases
LLM_TEMPERATURE_DEFAULT=0.7
LLM_TEMPERATURE_CREATIVE=0.9
LLM_TEMPERATURE_ANALYTICAL=0.3

# Max tokens for different use cases
LLM_MAX_TOKENS_DEFAULT=1000
LLM_MAX_TOKENS_SUMMARY=500
LLM_MAX_TOKENS_EXTENDED=4000

# ==============================================================================
# GOOGLE CLOUD CONFIGURATION (OPTIONAL)
# ==============================================================================
# Only required if using Google Language/Translation services
# Leave commented out if not using Google Cloud services
# GOOGLE_APPLICATION_CREDENTIALS=/app/google-creds.json
# GOOGLE_PROJECT_ID=your-google-cloud-project-id
# GOOGLE_LOCATION=us-central1

# Google LangExtract Configuration
GOOGLE_LANGUAGE_SERVICE_ENABLED=False
GOOGLE_TRANSLATE_SERVICE_ENABLED=False

# ==============================================================================
# FILE UPLOAD CONFIGURATION
# ==============================================================================
ONTEXTRACT_UPLOAD_FOLDER=uploads
MAX_CONTENT_LENGTH=16777216  # 16MB max file size
ALLOWED_EXTENSIONS=txt,pdf,docx,html,md

# ==============================================================================
# PROCESSING CONFIGURATION
# ==============================================================================
ENABLE_BATCH_PROCESSING=True
MAX_CONCURRENT_JOBS=3

# ==============================================================================
# RAG CONFIGURATION
# ==============================================================================
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
VECTOR_DIMENSION=384
SIMILARITY_THRESHOLD=0.7

# ==============================================================================
# REDIS CONFIGURATION (OPTIONAL)
# ==============================================================================
# Only needed if using Celery for background tasks
# REDIS_URL=redis://localhost:6379/0

# ==============================================================================
# LOGGING CONFIGURATION
# ==============================================================================
LOG_LEVEL=INFO
LOG_FILE=logs/ontextract.log

# ==============================================================================
# DOCKER-SPECIFIC SETTINGS
# ==============================================================================
# Enable automatic git updates on container restart
CHECK_GIT_UPDATES=true
GIT_BRANCH=main

# ==============================================================================
# DEVELOPMENT SETTINGS (DO NOT ENABLE IN PRODUCTION)
# ==============================================================================
TESTING=False
