--
-- PostgreSQL database dump
--

\restrict DoZfftHuLcZl42Z2BEYmR0jEblCPc5geIueVggwcJiQtTkSj1x2Sj6S8kbKW1GB

-- Dumped from database version 17.7 (Ubuntu 17.7-3.pgdg24.04+1)
-- Dumped by pg_dump version 17.7 (Ubuntu 17.7-3.pgdg24.04+1)

SET statement_timeout = 0;
SET lock_timeout = 0;
SET idle_in_transaction_session_timeout = 0;
SET transaction_timeout = 0;
SET client_encoding = 'UTF8';
SET standard_conforming_strings = on;
SELECT pg_catalog.set_config('search_path', '', false);
SET check_function_bodies = false;
SET xmloption = content;
SET client_min_messages = warning;
SET row_security = off;

--
-- Name: pg_trgm; Type: EXTENSION; Schema: -; Owner: -
--

CREATE EXTENSION IF NOT EXISTS pg_trgm WITH SCHEMA public;


--
-- Name: EXTENSION pg_trgm; Type: COMMENT; Schema: -; Owner: 
--

COMMENT ON EXTENSION pg_trgm IS 'text similarity measurement and index searching based on trigrams';


--
-- Name: pgcrypto; Type: EXTENSION; Schema: -; Owner: -
--

CREATE EXTENSION IF NOT EXISTS pgcrypto WITH SCHEMA public;


--
-- Name: EXTENSION pgcrypto; Type: COMMENT; Schema: -; Owner: 
--

COMMENT ON EXTENSION pgcrypto IS 'cryptographic functions';


--
-- Name: uuid-ossp; Type: EXTENSION; Schema: -; Owner: -
--

CREATE EXTENSION IF NOT EXISTS "uuid-ossp" WITH SCHEMA public;


--
-- Name: EXTENSION "uuid-ossp"; Type: COMMENT; Schema: -; Owner: 
--

COMMENT ON EXTENSION "uuid-ossp" IS 'generate universally unique identifiers (UUIDs)';


--
-- Name: vector; Type: EXTENSION; Schema: -; Owner: -
--

CREATE EXTENSION IF NOT EXISTS vector WITH SCHEMA public;


--
-- Name: EXTENSION vector; Type: COMMENT; Schema: -; Owner: 
--

COMMENT ON EXTENSION vector IS 'vector data type and ivfflat and hnsw access methods';


--
-- Name: get_document_version_id(integer, integer); Type: FUNCTION; Schema: public; Owner: ontextract_user
--

CREATE FUNCTION public.get_document_version_id(base_document_id integer, target_version integer) RETURNS integer
    LANGUAGE plpgsql
    AS $$
DECLARE
    version_document_id INTEGER;
BEGIN
    -- Find document ID for specific version
    SELECT d.id INTO version_document_id
    FROM documents d 
    WHERE ((d.id = base_document_id AND d.version_number = target_version)
        OR (d.source_document_id = base_document_id AND d.version_number = target_version))
    LIMIT 1;
    
    RETURN version_document_id;
END;
$$;


ALTER FUNCTION public.get_document_version_id(base_document_id integer, target_version integer) OWNER TO ontextract_user;

--
-- Name: get_latest_document_version(integer); Type: FUNCTION; Schema: public; Owner: ontextract_user
--

CREATE FUNCTION public.get_latest_document_version(base_document_id integer) RETURNS integer
    LANGUAGE plpgsql
    AS $$
DECLARE
    latest_version INTEGER;
BEGIN
    -- Find the highest version number for documents with same base ID
    SELECT MAX(d.version_number) INTO latest_version
    FROM documents d 
    WHERE (d.id = base_document_id OR d.source_document_id = base_document_id)
    OR (d.source_document_id IN (
        SELECT source_document_id FROM documents WHERE id = base_document_id
    ));
    
    RETURN COALESCE(latest_version, 1);
END;
$$;


ALTER FUNCTION public.get_latest_document_version(base_document_id integer) OWNER TO ontextract_user;

--
-- Name: inherit_processing_data(integer, integer); Type: FUNCTION; Schema: public; Owner: ontextract_user
--

CREATE FUNCTION public.inherit_processing_data(source_document_id integer, target_document_id integer) RETURNS void
    LANGUAGE plpgsql
    AS $$
BEGIN
    -- Copy embeddings from source to target
    INSERT INTO document_embeddings (
        document_id, term, period, embedding, 
        model_name, context_window, extraction_method, metadata, created_at
    )
    SELECT 
        target_document_id, term, period, embedding,
        model_name, context_window, extraction_method, metadata, CURRENT_TIMESTAMP
    FROM document_embeddings 
    WHERE document_id = source_document_id;

    -- Copy text segments from source to target  
    INSERT INTO text_segments (
        document_id, content, segment_type, segment_number,
        start_position, end_position, parent_segment_id, level,
        word_count, character_count, sentence_count, language, language_confidence,
        embedding, embedding_model, processed, processing_notes, topics, keywords,
        sentiment_score, complexity_score, created_at
    )
    SELECT 
        target_document_id, content, segment_type, segment_number,
        start_position, end_position, parent_segment_id, level,
        word_count, character_count, sentence_count, language, language_confidence,
        embedding, embedding_model, processed, processing_notes, topics, keywords,
        sentiment_score, complexity_score, CURRENT_TIMESTAMP
    FROM text_segments
    WHERE document_id = source_document_id;
    
END;
$$;


ALTER FUNCTION public.inherit_processing_data(source_document_id integer, target_document_id integer) OWNER TO ontextract_user;

--
-- Name: update_context_anchor_frequency(); Type: FUNCTION; Schema: public; Owner: ontextract_user
--

CREATE FUNCTION public.update_context_anchor_frequency() RETURNS trigger
    LANGUAGE plpgsql
    AS $$
BEGIN
    IF TG_OP = 'INSERT' THEN
        INSERT INTO context_anchors (anchor_term, frequency, first_used_in, last_used_in)
        VALUES ((SELECT anchor_term FROM context_anchors WHERE id = NEW.context_anchor_id), 1, NEW.term_version_id, NEW.term_version_id)
        ON CONFLICT (anchor_term) DO UPDATE SET
            frequency = context_anchors.frequency + 1,
            last_used_in = NEW.term_version_id;
        RETURN NEW;
    ELSIF TG_OP = 'DELETE' THEN
        UPDATE context_anchors SET frequency = frequency - 1 
        WHERE id = OLD.context_anchor_id;
        RETURN OLD;
    END IF;
    RETURN NULL;
END;
$$;


ALTER FUNCTION public.update_context_anchor_frequency() OWNER TO ontextract_user;

--
-- Name: update_terms_updated_at(); Type: FUNCTION; Schema: public; Owner: ontextract_user
--

CREATE FUNCTION public.update_terms_updated_at() RETURNS trigger
    LANGUAGE plpgsql
    AS $$
BEGIN
    NEW.updated_at = CURRENT_TIMESTAMP;
    RETURN NEW;
END;
$$;


ALTER FUNCTION public.update_terms_updated_at() OWNER TO ontextract_user;

--
-- Name: update_updated_at_column(); Type: FUNCTION; Schema: public; Owner: ontextract_user
--

CREATE FUNCTION public.update_updated_at_column() RETURNS trigger
    LANGUAGE plpgsql
    AS $$
BEGIN
    NEW.updated_at = CURRENT_TIMESTAMP;
    RETURN NEW;
END;
$$;


ALTER FUNCTION public.update_updated_at_column() OWNER TO ontextract_user;

SET default_tablespace = '';

SET default_table_access_method = heap;

--
-- Name: alembic_version; Type: TABLE; Schema: public; Owner: ontextract_user
--

CREATE TABLE public.alembic_version (
    version_num character varying(32) NOT NULL
);


ALTER TABLE public.alembic_version OWNER TO ontextract_user;

--
-- Name: analysis_agents; Type: TABLE; Schema: public; Owner: ontextract_user
--

CREATE TABLE public.analysis_agents (
    id uuid DEFAULT gen_random_uuid() NOT NULL,
    agent_type character varying(20) NOT NULL,
    name character varying(200) NOT NULL,
    description text,
    version character varying(50),
    algorithm_type character varying(100),
    model_parameters json,
    training_data character varying(200),
    expertise_domain character varying(100),
    institutional_affiliation character varying(200),
    created_at timestamp with time zone,
    is_active boolean,
    user_id integer,
    CONSTRAINT analysis_agents_agent_type_check CHECK (((agent_type)::text = ANY ((ARRAY['SoftwareAgent'::character varying, 'Person'::character varying, 'Organization'::character varying])::text[])))
);


ALTER TABLE public.analysis_agents OWNER TO ontextract_user;

--
-- Name: app_settings; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.app_settings (
    id integer NOT NULL,
    setting_key character varying(100) NOT NULL,
    setting_value jsonb NOT NULL,
    category character varying(50) NOT NULL,
    data_type character varying(20) NOT NULL,
    description text,
    default_value jsonb,
    requires_llm boolean DEFAULT false,
    user_id integer,
    created_at timestamp without time zone DEFAULT now(),
    updated_at timestamp without time zone DEFAULT now()
);


ALTER TABLE public.app_settings OWNER TO postgres;

--
-- Name: TABLE app_settings; Type: COMMENT; Schema: public; Owner: postgres
--

COMMENT ON TABLE public.app_settings IS 'Application settings with support for system-wide and user-specific configurations';


--
-- Name: COLUMN app_settings.setting_key; Type: COMMENT; Schema: public; Owner: postgres
--

COMMENT ON COLUMN public.app_settings.setting_key IS 'Unique setting identifier (e.g., spacy_model)';


--
-- Name: COLUMN app_settings.setting_value; Type: COMMENT; Schema: public; Owner: postgres
--

COMMENT ON COLUMN public.app_settings.setting_value IS 'Setting value stored as JSON';


--
-- Name: COLUMN app_settings.category; Type: COMMENT; Schema: public; Owner: postgres
--

COMMENT ON COLUMN public.app_settings.category IS 'Setting category: prompts, nlp, processing, llm, ui';


--
-- Name: COLUMN app_settings.data_type; Type: COMMENT; Schema: public; Owner: postgres
--

COMMENT ON COLUMN public.app_settings.data_type IS 'Data type hint: string, integer, boolean, json';


--
-- Name: COLUMN app_settings.user_id; Type: COMMENT; Schema: public; Owner: postgres
--

COMMENT ON COLUMN public.app_settings.user_id IS 'User ID for user-specific settings, NULL for system-wide';


--
-- Name: app_settings_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.app_settings_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE public.app_settings_id_seq OWNER TO postgres;

--
-- Name: app_settings_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.app_settings_id_seq OWNED BY public.app_settings.id;


--
-- Name: context_anchors; Type: TABLE; Schema: public; Owner: ontextract_user
--

CREATE TABLE public.context_anchors (
    id uuid DEFAULT gen_random_uuid() NOT NULL,
    anchor_term character varying(255) NOT NULL,
    frequency integer,
    first_used_in uuid,
    last_used_in uuid,
    created_at timestamp with time zone
);


ALTER TABLE public.context_anchors OWNER TO ontextract_user;

--
-- Name: document_embeddings; Type: TABLE; Schema: public; Owner: ontextract_user
--

CREATE TABLE public.document_embeddings (
    id integer NOT NULL,
    document_id integer,
    term character varying(200) NOT NULL,
    period integer,
    embedding public.vector,
    model_name character varying(100),
    context_window text,
    extraction_method character varying(50),
    metadata jsonb,
    created_at timestamp without time zone DEFAULT now(),
    updated_at timestamp without time zone DEFAULT now()
);


ALTER TABLE public.document_embeddings OWNER TO ontextract_user;

--
-- Name: document_embeddings_id_seq; Type: SEQUENCE; Schema: public; Owner: ontextract_user
--

CREATE SEQUENCE public.document_embeddings_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE public.document_embeddings_id_seq OWNER TO ontextract_user;

--
-- Name: document_embeddings_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: ontextract_user
--

ALTER SEQUENCE public.document_embeddings_id_seq OWNED BY public.document_embeddings.id;


--
-- Name: document_processing_index; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.document_processing_index (
    id uuid DEFAULT gen_random_uuid() NOT NULL,
    document_id integer NOT NULL,
    experiment_id integer NOT NULL,
    processing_id uuid NOT NULL,
    processing_type character varying(50) NOT NULL,
    processing_method character varying(50) NOT NULL,
    status character varying(20) NOT NULL,
    created_at timestamp without time zone DEFAULT now()
);


ALTER TABLE public.document_processing_index OWNER TO postgres;

--
-- Name: document_processing_summary; Type: TABLE; Schema: public; Owner: ontextract_user
--

CREATE TABLE public.document_processing_summary (
    id integer NOT NULL,
    document_id integer NOT NULL,
    processing_type character varying(50) NOT NULL,
    status character varying(20) NOT NULL,
    source_document_id integer,
    job_id integer,
    priority integer DEFAULT 1,
    created_at timestamp without time zone DEFAULT now(),
    updated_at timestamp without time zone DEFAULT now()
);


ALTER TABLE public.document_processing_summary OWNER TO ontextract_user;

--
-- Name: TABLE document_processing_summary; Type: COMMENT; Schema: public; Owner: ontextract_user
--

COMMENT ON TABLE public.document_processing_summary IS 'Efficient summary of processing capabilities available per document';


--
-- Name: document_processing_summary_id_seq; Type: SEQUENCE; Schema: public; Owner: ontextract_user
--

CREATE SEQUENCE public.document_processing_summary_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE public.document_processing_summary_id_seq OWNER TO ontextract_user;

--
-- Name: document_processing_summary_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: ontextract_user
--

ALTER SEQUENCE public.document_processing_summary_id_seq OWNED BY public.document_processing_summary.id;


--
-- Name: document_temporal_metadata; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.document_temporal_metadata (
    id uuid DEFAULT gen_random_uuid() NOT NULL,
    document_id integer NOT NULL,
    experiment_id integer,
    temporal_period character varying(100),
    temporal_start_year integer,
    temporal_end_year integer,
    publication_year integer,
    discipline character varying(100),
    subdiscipline character varying(100),
    key_definition text,
    semantic_features jsonb,
    semantic_shift_type character varying(50),
    timeline_position integer,
    timeline_track character varying(50),
    marker_color character varying(20),
    extraction_method character varying(50),
    extraction_confidence numeric(3,2),
    reviewed_by integer,
    reviewed_at timestamp with time zone,
    created_at timestamp with time zone DEFAULT now(),
    updated_at timestamp with time zone DEFAULT now()
);


ALTER TABLE public.document_temporal_metadata OWNER TO postgres;

--
-- Name: TABLE document_temporal_metadata; Type: COMMENT; Schema: public; Owner: postgres
--

COMMENT ON TABLE public.document_temporal_metadata IS 'Temporal and disciplinary metadata for documents in semantic change experiments';


--
-- Name: documents; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.documents (
    id integer NOT NULL,
    title character varying(200) NOT NULL,
    content_type character varying(20) NOT NULL,
    document_type character varying(20) NOT NULL,
    reference_subtype character varying(30),
    file_type character varying(10),
    original_filename character varying(255),
    file_path character varying(500),
    file_size integer,
    source_metadata json,
    content text,
    content_preview text,
    detected_language character varying(10),
    language_confidence double precision,
    status character varying(20) NOT NULL,
    word_count integer,
    character_count integer,
    created_at timestamp without time zone NOT NULL,
    updated_at timestamp without time zone,
    processed_at timestamp without time zone,
    user_id integer NOT NULL,
    embedding character varying,
    parent_document_id integer,
    processing_metadata json,
    version_number integer DEFAULT 1,
    version_type character varying(20) DEFAULT 'original'::character varying,
    experiment_id integer,
    source_document_id integer,
    processing_notes text,
    composite_sources jsonb,
    composite_metadata jsonb,
    metadata_provenance jsonb DEFAULT '{}'::jsonb,
    uuid uuid DEFAULT gen_random_uuid() NOT NULL,
    CONSTRAINT check_version_number_positive CHECK ((version_number > 0)),
    CONSTRAINT check_version_type CHECK (((version_type)::text = ANY ((ARRAY['original'::character varying, 'processed'::character varying, 'experimental'::character varying, 'composite'::character varying])::text[])))
);


ALTER TABLE public.documents OWNER TO postgres;

--
-- Name: COLUMN documents.processing_metadata; Type: COMMENT; Schema: public; Owner: postgres
--

COMMENT ON COLUMN public.documents.processing_metadata IS 'General metadata for processing info, embeddings, and document analysis';


--
-- Name: COLUMN documents.version_number; Type: COMMENT; Schema: public; Owner: postgres
--

COMMENT ON COLUMN public.documents.version_number IS 'Sequential version number within a document family';


--
-- Name: COLUMN documents.version_type; Type: COMMENT; Schema: public; Owner: postgres
--

COMMENT ON COLUMN public.documents.version_type IS 'Type of version: original, processed, experimental';


--
-- Name: COLUMN documents.experiment_id; Type: COMMENT; Schema: public; Owner: postgres
--

COMMENT ON COLUMN public.documents.experiment_id IS 'Associated experiment (for experimental versions)';


--
-- Name: COLUMN documents.source_document_id; Type: COMMENT; Schema: public; Owner: postgres
--

COMMENT ON COLUMN public.documents.source_document_id IS 'Original document this version derives from';


--
-- Name: COLUMN documents.processing_notes; Type: COMMENT; Schema: public; Owner: postgres
--

COMMENT ON COLUMN public.documents.processing_notes IS 'Notes about processing operations that created this version';


--
-- Name: COLUMN documents.metadata_provenance; Type: COMMENT; Schema: public; Owner: postgres
--

COMMENT ON COLUMN public.documents.metadata_provenance IS 'Tracks provenance for each metadata field. Structure: {field_name: {source: str, confidence: float, timestamp: str, raw_value: any}}';


--
-- Name: experiments; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.experiments (
    id integer NOT NULL,
    name character varying(200) NOT NULL,
    description text,
    experiment_type character varying(50) NOT NULL,
    configuration text,
    status character varying(20) NOT NULL,
    results text,
    results_summary text,
    created_at timestamp without time zone NOT NULL,
    updated_at timestamp without time zone,
    started_at timestamp without time zone,
    completed_at timestamp without time zone,
    user_id integer NOT NULL,
    term_id uuid
);


ALTER TABLE public.experiments OWNER TO postgres;

--
-- Name: document_version_chains; Type: VIEW; Schema: public; Owner: postgres
--

CREATE VIEW public.document_version_chains AS
 SELECT COALESCE(d.source_document_id, d.id) AS root_document_id,
    d.id AS document_id,
    d.title,
    d.version_number,
    d.version_type,
    d.experiment_id,
    d.created_at,
    d.status,
    e.name AS experiment_name
   FROM (public.documents d
     LEFT JOIN public.experiments e ON ((d.experiment_id = e.id)))
  ORDER BY COALESCE(d.source_document_id, d.id), d.version_number;


ALTER VIEW public.document_version_chains OWNER TO postgres;

--
-- Name: text_segments; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.text_segments (
    id integer NOT NULL,
    content text NOT NULL,
    segment_type character varying(50),
    segment_number integer,
    start_position integer,
    end_position integer,
    parent_segment_id integer,
    level integer,
    word_count integer,
    character_count integer,
    sentence_count integer,
    language character varying(10),
    language_confidence double precision,
    embedding character varying,
    embedding_model character varying(100),
    processed boolean,
    processing_notes text,
    topics text,
    keywords text,
    sentiment_score double precision,
    complexity_score double precision,
    created_at timestamp without time zone NOT NULL,
    updated_at timestamp without time zone,
    processed_at timestamp without time zone,
    document_id integer NOT NULL,
    segmentation_method character varying(50) DEFAULT 'manual'::character varying,
    segmentation_job_id integer,
    processing_method character varying(100),
    group_id integer
);


ALTER TABLE public.text_segments OWNER TO postgres;

--
-- Name: version_changelog; Type: TABLE; Schema: public; Owner: ontextract_user
--

CREATE TABLE public.version_changelog (
    id integer NOT NULL,
    document_id integer NOT NULL,
    version_number integer NOT NULL,
    change_type character varying(50) NOT NULL,
    change_description text,
    previous_version integer,
    created_at timestamp without time zone DEFAULT CURRENT_TIMESTAMP NOT NULL,
    created_by integer NOT NULL,
    processing_metadata jsonb
);


ALTER TABLE public.version_changelog OWNER TO ontextract_user;

--
-- Name: document_version_history; Type: VIEW; Schema: public; Owner: ontextract_user
--

CREATE VIEW public.document_version_history AS
 SELECT d.id,
    d.title,
    d.version_number,
    d.version_type,
    d.created_at AS version_created,
    d.source_document_id,
    COALESCE(d.source_document_id, d.id) AS base_document_id,
    ((d.version_number = 1) AND ((d.version_type)::text = 'original'::text)) AS is_base_document,
    COALESCE(e.embedding_count, (0)::bigint) AS embedding_count,
    COALESCE(s.segment_count, (0)::bigint) AS segment_count,
    array_agg(DISTINCT vc.change_type) FILTER (WHERE (vc.change_type IS NOT NULL)) AS changes_in_version,
    vc.change_description
   FROM (((public.documents d
     LEFT JOIN ( SELECT document_embeddings.document_id,
            count(*) AS embedding_count
           FROM public.document_embeddings
          GROUP BY document_embeddings.document_id) e ON ((d.id = e.document_id)))
     LEFT JOIN ( SELECT text_segments.document_id,
            count(*) AS segment_count
           FROM public.text_segments
          GROUP BY text_segments.document_id) s ON ((d.id = s.document_id)))
     LEFT JOIN public.version_changelog vc ON (((d.id = vc.document_id) AND (d.version_number = vc.version_number))))
  GROUP BY d.id, d.title, d.version_number, d.version_type, d.created_at, d.source_document_id, e.embedding_count, s.segment_count, vc.change_description
  ORDER BY COALESCE(d.source_document_id, d.id), d.version_number;


ALTER VIEW public.document_version_history OWNER TO ontextract_user;

--
-- Name: documents_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.documents_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE public.documents_id_seq OWNER TO postgres;

--
-- Name: documents_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.documents_id_seq OWNED BY public.documents.id;


--
-- Name: domains; Type: TABLE; Schema: public; Owner: ontextract_user
--

CREATE TABLE public.domains (
    id integer NOT NULL,
    uuid uuid NOT NULL,
    name character varying(255) NOT NULL,
    display_name character varying(255),
    namespace_uri text NOT NULL,
    description text,
    metadata json,
    is_active boolean,
    created_at timestamp with time zone,
    updated_at timestamp with time zone
);


ALTER TABLE public.domains OWNER TO ontextract_user;

--
-- Name: domains_id_seq; Type: SEQUENCE; Schema: public; Owner: ontextract_user
--

CREATE SEQUENCE public.domains_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE public.domains_id_seq OWNER TO ontextract_user;

--
-- Name: domains_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: ontextract_user
--

ALTER SEQUENCE public.domains_id_seq OWNED BY public.domains.id;


--
-- Name: experiment_document_processing; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.experiment_document_processing (
    id uuid DEFAULT gen_random_uuid() NOT NULL,
    experiment_document_id integer NOT NULL,
    processing_type character varying(50) NOT NULL,
    processing_method character varying(50) NOT NULL,
    status character varying(20) DEFAULT 'pending'::character varying NOT NULL,
    configuration_json text,
    results_summary_json text,
    error_message text,
    created_at timestamp without time zone DEFAULT now(),
    started_at timestamp without time zone,
    completed_at timestamp without time zone
);


ALTER TABLE public.experiment_document_processing OWNER TO postgres;

--
-- Name: experiment_documents; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.experiment_documents (
    experiment_id integer NOT NULL,
    document_id integer NOT NULL,
    added_at timestamp without time zone,
    processing_status character varying(20) DEFAULT 'pending'::character varying,
    processing_metadata json,
    embeddings_applied boolean DEFAULT false,
    embeddings_metadata json,
    segments_created boolean DEFAULT false,
    segments_metadata json,
    nlp_analysis_completed boolean DEFAULT false,
    nlp_results json,
    processed_at timestamp without time zone,
    updated_at timestamp without time zone DEFAULT CURRENT_TIMESTAMP
);


ALTER TABLE public.experiment_documents OWNER TO postgres;

--
-- Name: COLUMN experiment_documents.processing_status; Type: COMMENT; Schema: public; Owner: postgres
--

COMMENT ON COLUMN public.experiment_documents.processing_status IS 'Status: pending, processing, completed, error';


--
-- Name: COLUMN experiment_documents.processing_metadata; Type: COMMENT; Schema: public; Owner: postgres
--

COMMENT ON COLUMN public.experiment_documents.processing_metadata IS 'General experiment-specific processing metadata';


--
-- Name: COLUMN experiment_documents.embeddings_applied; Type: COMMENT; Schema: public; Owner: postgres
--

COMMENT ON COLUMN public.experiment_documents.embeddings_applied IS 'Whether embeddings have been generated for this experiment';


--
-- Name: COLUMN experiment_documents.embeddings_metadata; Type: COMMENT; Schema: public; Owner: postgres
--

COMMENT ON COLUMN public.experiment_documents.embeddings_metadata IS 'Embedding model info and metrics for this experiment';


--
-- Name: COLUMN experiment_documents.segments_created; Type: COMMENT; Schema: public; Owner: postgres
--

COMMENT ON COLUMN public.experiment_documents.segments_created IS 'Whether document has been segmented for this experiment';


--
-- Name: COLUMN experiment_documents.segments_metadata; Type: COMMENT; Schema: public; Owner: postgres
--

COMMENT ON COLUMN public.experiment_documents.segments_metadata IS 'Segmentation parameters and results';


--
-- Name: COLUMN experiment_documents.nlp_analysis_completed; Type: COMMENT; Schema: public; Owner: postgres
--

COMMENT ON COLUMN public.experiment_documents.nlp_analysis_completed IS 'Whether NLP analysis is complete for this experiment';


--
-- Name: COLUMN experiment_documents.nlp_results; Type: COMMENT; Schema: public; Owner: postgres
--

COMMENT ON COLUMN public.experiment_documents.nlp_results IS 'Experiment-specific NLP analysis results';


--
-- Name: experiment_documents_v2; Type: TABLE; Schema: public; Owner: ontextract_user
--

CREATE TABLE public.experiment_documents_v2 (
    id integer NOT NULL,
    experiment_id integer NOT NULL,
    document_id integer NOT NULL,
    processing_status character varying(50) NOT NULL,
    embedding_model character varying(100),
    embedding_dimension integer,
    embeddings_applied boolean,
    embedding_metadata text,
    segmentation_method character varying(50),
    segment_size integer,
    segments_created boolean,
    segmentation_metadata text,
    nlp_analysis_completed boolean,
    nlp_tools_used text,
    processing_started_at timestamp without time zone,
    processing_completed_at timestamp without time zone,
    embeddings_generated_at timestamp without time zone,
    segmentation_completed_at timestamp without time zone,
    added_at timestamp without time zone,
    updated_at timestamp without time zone
);


ALTER TABLE public.experiment_documents_v2 OWNER TO ontextract_user;

--
-- Name: experiment_documents_v2_id_seq; Type: SEQUENCE; Schema: public; Owner: ontextract_user
--

CREATE SEQUENCE public.experiment_documents_v2_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE public.experiment_documents_v2_id_seq OWNER TO ontextract_user;

--
-- Name: experiment_documents_v2_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: ontextract_user
--

ALTER SEQUENCE public.experiment_documents_v2_id_seq OWNED BY public.experiment_documents_v2.id;


--
-- Name: experiment_orchestration_runs; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.experiment_orchestration_runs (
    id uuid DEFAULT gen_random_uuid() NOT NULL,
    experiment_id integer NOT NULL,
    user_id integer NOT NULL,
    started_at timestamp without time zone DEFAULT now() NOT NULL,
    completed_at timestamp without time zone,
    status character varying(50) NOT NULL,
    current_stage character varying(50),
    error_message text,
    experiment_goal text,
    term_context character varying(200),
    recommended_strategy jsonb,
    strategy_reasoning text,
    confidence double precision,
    strategy_approved boolean DEFAULT false,
    modified_strategy jsonb,
    review_notes text,
    reviewed_by integer,
    reviewed_at timestamp without time zone,
    processing_results jsonb,
    execution_trace jsonb,
    cross_document_insights text,
    term_evolution_analysis text,
    comparative_summary text
);


ALTER TABLE public.experiment_orchestration_runs OWNER TO postgres;

--
-- Name: experiment_references; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.experiment_references (
    experiment_id integer NOT NULL,
    reference_id integer NOT NULL,
    include_in_analysis boolean,
    added_at timestamp without time zone,
    notes text
);


ALTER TABLE public.experiment_references OWNER TO postgres;

--
-- Name: experiments_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.experiments_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE public.experiments_id_seq OWNER TO postgres;

--
-- Name: experiments_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.experiments_id_seq OWNED BY public.experiments.id;


--
-- Name: extracted_entities; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.extracted_entities (
    id integer NOT NULL,
    entity_text character varying(500) NOT NULL,
    entity_type character varying(100) NOT NULL,
    entity_subtype character varying(100),
    context_before character varying(200),
    context_after character varying(200),
    sentence text,
    start_position integer,
    end_position integer,
    paragraph_number integer,
    sentence_number integer,
    confidence_score double precision,
    extraction_method character varying(50),
    properties text,
    language character varying(10),
    normalized_form character varying(500),
    created_at timestamp without time zone NOT NULL,
    updated_at timestamp without time zone,
    processing_job_id integer NOT NULL,
    text_segment_id integer
);


ALTER TABLE public.extracted_entities OWNER TO postgres;

--
-- Name: extracted_entities_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.extracted_entities_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE public.extracted_entities_id_seq OWNER TO postgres;

--
-- Name: extracted_entities_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.extracted_entities_id_seq OWNED BY public.extracted_entities.id;


--
-- Name: fuzziness_adjustments; Type: TABLE; Schema: public; Owner: ontextract_user
--

CREATE TABLE public.fuzziness_adjustments (
    id uuid DEFAULT gen_random_uuid() NOT NULL,
    term_version_id uuid NOT NULL,
    original_score numeric(4,3) NOT NULL,
    adjusted_score numeric(4,3) NOT NULL,
    adjustment_reason text NOT NULL,
    adjusted_by integer,
    created_at timestamp with time zone
);


ALTER TABLE public.fuzziness_adjustments OWNER TO ontextract_user;

--
-- Name: learning_patterns; Type: TABLE; Schema: public; Owner: ontextract_user
--

CREATE TABLE public.learning_patterns (
    id uuid DEFAULT gen_random_uuid() NOT NULL,
    pattern_name character varying(100) NOT NULL,
    pattern_type character varying(50) NOT NULL,
    context_signature character varying(200) NOT NULL,
    conditions jsonb NOT NULL,
    recommendations jsonb NOT NULL,
    confidence numeric(4,3) NOT NULL,
    derived_from_feedback uuid,
    researcher_authority jsonb,
    times_applied integer DEFAULT 0,
    success_rate numeric(4,3),
    last_applied timestamp with time zone,
    pattern_status character varying(20) DEFAULT 'active'::character varying,
    created_at timestamp with time zone DEFAULT CURRENT_TIMESTAMP,
    updated_at timestamp with time zone DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT learning_patterns_confidence_check CHECK (((confidence >= (0)::numeric) AND (confidence <= (1)::numeric))),
    CONSTRAINT learning_patterns_success_rate_check CHECK (((success_rate >= (0)::numeric) AND (success_rate <= (1)::numeric))),
    CONSTRAINT pattern_status_check CHECK (((pattern_status)::text = ANY ((ARRAY['active'::character varying, 'deprecated'::character varying, 'under_review'::character varying, 'experimental'::character varying])::text[]))),
    CONSTRAINT pattern_type_check CHECK (((pattern_type)::text = ANY ((ARRAY['avoidance'::character varying, 'preference'::character varying, 'enhancement'::character varying, 'domain_specific'::character varying])::text[])))
);


ALTER TABLE public.learning_patterns OWNER TO ontextract_user;

--
-- Name: TABLE learning_patterns; Type: COMMENT; Schema: public; Owner: ontextract_user
--

COMMENT ON TABLE public.learning_patterns IS 'Codified learning patterns derived from researcher feedback';


--
-- Name: COLUMN learning_patterns.context_signature; Type: COMMENT; Schema: public; Owner: ontextract_user
--

COMMENT ON COLUMN public.learning_patterns.context_signature IS 'Signature for matching similar decision contexts';


--
-- Name: COLUMN learning_patterns.researcher_authority; Type: COMMENT; Schema: public; Owner: ontextract_user
--

COMMENT ON COLUMN public.learning_patterns.researcher_authority IS 'Authority assessment of source researcher for weighting';


--
-- Name: multi_model_consensus; Type: TABLE; Schema: public; Owner: ontextract_user
--

CREATE TABLE public.multi_model_consensus (
    id uuid DEFAULT gen_random_uuid() NOT NULL,
    orchestration_decision_id uuid NOT NULL,
    validation_type character varying(50) DEFAULT 'multi_model_consensus'::character varying,
    models_involved text[],
    consensus_method character varying(50),
    model_responses jsonb,
    model_confidence_scores jsonb,
    model_agreement_matrix jsonb,
    consensus_reached boolean,
    consensus_confidence numeric(4,3),
    final_decision jsonb,
    disagreement_areas jsonb,
    started_at timestamp with time zone DEFAULT CURRENT_TIMESTAMP,
    completed_at timestamp with time zone,
    total_processing_time_ms integer,
    CONSTRAINT multi_model_consensus_consensus_confidence_check CHECK (((consensus_confidence >= (0)::numeric) AND (consensus_confidence <= (1)::numeric)))
);


ALTER TABLE public.multi_model_consensus OWNER TO ontextract_user;

--
-- Name: TABLE multi_model_consensus; Type: COMMENT; Schema: public; Owner: ontextract_user
--

COMMENT ON TABLE public.multi_model_consensus IS 'Multi-model validation and consensus decision logging';


--
-- Name: COLUMN multi_model_consensus.model_agreement_matrix; Type: COMMENT; Schema: public; Owner: ontextract_user
--

COMMENT ON COLUMN public.multi_model_consensus.model_agreement_matrix IS 'Pairwise agreement scores between models';


--
-- Name: COLUMN multi_model_consensus.disagreement_areas; Type: COMMENT; Schema: public; Owner: ontextract_user
--

COMMENT ON COLUMN public.multi_model_consensus.disagreement_areas IS 'Specific areas where models disagreed';


--
-- Name: oed_definitions; Type: TABLE; Schema: public; Owner: ontextract_user
--

CREATE TABLE public.oed_definitions (
    id uuid NOT NULL,
    term_id uuid NOT NULL,
    definition_number character varying(10),
    first_cited_year integer,
    last_cited_year integer,
    part_of_speech character varying(30),
    domain_label character varying(100),
    status character varying(20),
    quotation_count integer,
    sense_frequency_rank integer,
    historical_period character varying(50),
    period_start_year integer,
    period_end_year integer,
    generated_at_time timestamp with time zone,
    was_attributed_to character varying(100),
    was_derived_from character varying(200),
    derivation_type character varying(50),
    definition_confidence character varying(20),
    created_at timestamp with time zone,
    updated_at timestamp with time zone,
    definition_excerpt character varying(300),
    oed_sense_id character varying(100),
    oed_url character varying(500),
    CONSTRAINT oed_definitions_definition_confidence_check CHECK (((definition_confidence)::text = ANY ((ARRAY['high'::character varying, 'medium'::character varying, 'low'::character varying])::text[]))),
    CONSTRAINT oed_definitions_status_check CHECK (((status)::text = ANY ((ARRAY['current'::character varying, 'historical'::character varying, 'obsolete'::character varying])::text[])))
);


ALTER TABLE public.oed_definitions OWNER TO ontextract_user;

--
-- Name: oed_etymology; Type: TABLE; Schema: public; Owner: ontextract_user
--

CREATE TABLE public.oed_etymology (
    id uuid NOT NULL,
    term_id uuid NOT NULL,
    etymology_text text,
    origin_language character varying(50),
    first_recorded_year integer,
    etymology_confidence character varying(20),
    language_family json,
    root_analysis json,
    morphology json,
    generated_at_time timestamp with time zone,
    was_attributed_to character varying(100),
    was_derived_from character varying(200),
    derivation_type character varying(50),
    source_version character varying(50),
    created_at timestamp with time zone,
    updated_at timestamp with time zone,
    CONSTRAINT oed_etymology_etymology_confidence_check CHECK (((etymology_confidence)::text = ANY ((ARRAY['high'::character varying, 'medium'::character varying, 'low'::character varying])::text[])))
);


ALTER TABLE public.oed_etymology OWNER TO ontextract_user;

--
-- Name: oed_historical_stats; Type: TABLE; Schema: public; Owner: ontextract_user
--

CREATE TABLE public.oed_historical_stats (
    id uuid NOT NULL,
    term_id uuid NOT NULL,
    time_period character varying(50) NOT NULL,
    start_year integer NOT NULL,
    end_year integer NOT NULL,
    definition_count integer,
    sense_count integer,
    quotation_span_years integer,
    earliest_quotation_year integer,
    latest_quotation_year integer,
    semantic_stability_score numeric(4,3),
    domain_shift_indicator boolean,
    part_of_speech_changes json,
    started_at_time timestamp with time zone,
    ended_at_time timestamp with time zone,
    was_associated_with character varying(100),
    used_entity json,
    generated_entity character varying(200),
    oed_edition character varying(50),
    created_at timestamp with time zone,
    updated_at timestamp with time zone,
    CONSTRAINT oed_historical_stats_semantic_stability_score_check CHECK (((semantic_stability_score >= (0)::numeric) AND (semantic_stability_score <= (1)::numeric)))
);


ALTER TABLE public.oed_historical_stats OWNER TO ontextract_user;

--
-- Name: oed_quotation_summaries; Type: TABLE; Schema: public; Owner: ontextract_user
--

CREATE TABLE public.oed_quotation_summaries (
    id uuid NOT NULL,
    term_id uuid NOT NULL,
    oed_definition_id uuid,
    quotation_year integer,
    author_name character varying(200),
    work_title character varying(300),
    domain_context character varying(100),
    usage_type character varying(50),
    has_technical_usage boolean,
    represents_semantic_shift boolean,
    chronological_rank integer,
    generated_at_time timestamp with time zone,
    was_attributed_to character varying(100),
    was_derived_from character varying(200),
    derivation_type character varying(50),
    created_at timestamp with time zone,
    updated_at timestamp with time zone
);


ALTER TABLE public.oed_quotation_summaries OWNER TO ontextract_user;

--
-- Name: oed_timeline_markers; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.oed_timeline_markers (
    id uuid DEFAULT gen_random_uuid() NOT NULL,
    term_id uuid NOT NULL,
    year integer,
    period_label character varying(100),
    century integer,
    sense_number character varying(20),
    definition text NOT NULL,
    definition_short text,
    first_recorded_use text,
    quotation_date character varying(50),
    quotation_author character varying(200),
    quotation_work character varying(200),
    semantic_category character varying(100),
    etymology_note text,
    marker_type character varying(50),
    display_order integer,
    oed_entry_id character varying(100),
    extraction_date timestamp with time zone DEFAULT now(),
    extracted_by character varying(50),
    created_at timestamp with time zone DEFAULT now()
);


ALTER TABLE public.oed_timeline_markers OWNER TO postgres;

--
-- Name: TABLE oed_timeline_markers; Type: COMMENT; Schema: public; Owner: postgres
--

COMMENT ON TABLE public.oed_timeline_markers IS 'Historical timeline data extracted from OED entries for anchor terms';


--
-- Name: ontologies; Type: TABLE; Schema: public; Owner: ontextract_user
--

CREATE TABLE public.ontologies (
    id integer NOT NULL,
    uuid uuid NOT NULL,
    domain_id integer,
    name character varying(255) NOT NULL,
    base_uri text NOT NULL,
    description text,
    is_base boolean,
    is_editable boolean,
    parent_ontology_id integer,
    ontology_type character varying(20),
    metadata json,
    created_at timestamp with time zone NOT NULL,
    updated_at timestamp with time zone
);


ALTER TABLE public.ontologies OWNER TO ontextract_user;

--
-- Name: ontologies_id_seq; Type: SEQUENCE; Schema: public; Owner: ontextract_user
--

CREATE SEQUENCE public.ontologies_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE public.ontologies_id_seq OWNER TO ontextract_user;

--
-- Name: ontologies_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: ontextract_user
--

ALTER SEQUENCE public.ontologies_id_seq OWNED BY public.ontologies.id;


--
-- Name: ontology_entities; Type: TABLE; Schema: public; Owner: ontextract_user
--

CREATE TABLE public.ontology_entities (
    id integer NOT NULL,
    ontology_id integer NOT NULL,
    entity_type character varying(50) NOT NULL,
    uri text NOT NULL,
    label character varying(255),
    comment text,
    parent_uri text,
    domain json,
    range json,
    properties json,
    embedding public.vector(384),
    created_at timestamp without time zone
);


ALTER TABLE public.ontology_entities OWNER TO ontextract_user;

--
-- Name: ontology_entities_id_seq; Type: SEQUENCE; Schema: public; Owner: ontextract_user
--

CREATE SEQUENCE public.ontology_entities_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE public.ontology_entities_id_seq OWNER TO ontextract_user;

--
-- Name: ontology_entities_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: ontextract_user
--

ALTER SEQUENCE public.ontology_entities_id_seq OWNED BY public.ontology_entities.id;


--
-- Name: ontology_mappings; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.ontology_mappings (
    id integer NOT NULL,
    ontology_uri character varying(500) NOT NULL,
    concept_label character varying(200) NOT NULL,
    concept_definition text,
    parent_concepts text,
    child_concepts text,
    related_concepts text,
    mapping_confidence double precision,
    mapping_method character varying(50),
    mapping_source character varying(100),
    semantic_type character varying(100),
    domain character varying(100),
    properties text,
    is_verified boolean,
    verified_by character varying(100),
    verification_notes text,
    alternative_mappings text,
    created_at timestamp without time zone NOT NULL,
    updated_at timestamp without time zone,
    verified_at timestamp without time zone,
    extracted_entity_id integer NOT NULL
);


ALTER TABLE public.ontology_mappings OWNER TO postgres;

--
-- Name: ontology_mappings_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.ontology_mappings_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE public.ontology_mappings_id_seq OWNER TO postgres;

--
-- Name: ontology_mappings_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.ontology_mappings_id_seq OWNED BY public.ontology_mappings.id;


--
-- Name: ontology_versions; Type: TABLE; Schema: public; Owner: ontextract_user
--

CREATE TABLE public.ontology_versions (
    id integer NOT NULL,
    ontology_id integer NOT NULL,
    version_number integer NOT NULL,
    version_tag character varying(50),
    content text NOT NULL,
    content_hash character varying(64),
    change_summary text,
    created_by character varying(255),
    created_at timestamp with time zone NOT NULL,
    is_current boolean,
    is_draft boolean,
    workflow_status character varying(20),
    metadata json
);


ALTER TABLE public.ontology_versions OWNER TO ontextract_user;

--
-- Name: ontology_versions_id_seq; Type: SEQUENCE; Schema: public; Owner: ontextract_user
--

CREATE SEQUENCE public.ontology_versions_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE public.ontology_versions_id_seq OWNER TO ontextract_user;

--
-- Name: ontology_versions_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: ontextract_user
--

ALTER SEQUENCE public.ontology_versions_id_seq OWNED BY public.ontology_versions.id;


--
-- Name: orchestration_decisions; Type: TABLE; Schema: public; Owner: ontextract_user
--

CREATE TABLE public.orchestration_decisions (
    id uuid DEFAULT gen_random_uuid() NOT NULL,
    activity_type character varying(50) DEFAULT 'llm_orchestration'::character varying NOT NULL,
    started_at_time timestamp with time zone DEFAULT CURRENT_TIMESTAMP,
    ended_at_time timestamp with time zone,
    activity_status character varying(20) DEFAULT 'completed'::character varying,
    document_id integer,
    experiment_id integer,
    term_text character varying(255),
    input_metadata jsonb,
    document_characteristics jsonb,
    orchestrator_provider character varying(50),
    orchestrator_model character varying(100),
    orchestrator_prompt text,
    orchestrator_response text,
    orchestrator_response_time_ms integer,
    selected_tools text[],
    embedding_model character varying(100),
    processing_strategy character varying(50),
    expected_runtime_seconds integer,
    decision_confidence numeric(4,3),
    reasoning_summary text,
    decision_factors jsonb,
    decision_validated boolean,
    actual_runtime_seconds integer,
    tool_execution_success jsonb,
    was_associated_with uuid,
    used_entity uuid,
    created_at timestamp with time zone DEFAULT CURRENT_TIMESTAMP,
    created_by integer,
    CONSTRAINT orchestration_decisions_activity_status_check CHECK (((activity_status)::text = ANY ((ARRAY['running'::character varying, 'completed'::character varying, 'error'::character varying, 'timeout'::character varying])::text[]))),
    CONSTRAINT orchestration_decisions_decision_confidence_check CHECK (((decision_confidence >= (0)::numeric) AND (decision_confidence <= (1)::numeric)))
);


ALTER TABLE public.orchestration_decisions OWNER TO ontextract_user;

--
-- Name: TABLE orchestration_decisions; Type: COMMENT; Schema: public; Owner: ontextract_user
--

COMMENT ON TABLE public.orchestration_decisions IS 'PROV-O compliant logging of LLM orchestration decisions for tool selection and coordination';


--
-- Name: COLUMN orchestration_decisions.input_metadata; Type: COMMENT; Schema: public; Owner: ontextract_user
--

COMMENT ON COLUMN public.orchestration_decisions.input_metadata IS 'Document metadata that influenced tool selection (year, domain, format, length)';


--
-- Name: COLUMN orchestration_decisions.decision_factors; Type: COMMENT; Schema: public; Owner: ontextract_user
--

COMMENT ON COLUMN public.orchestration_decisions.decision_factors IS 'Structured reasoning components for decision analysis';


--
-- Name: COLUMN orchestration_decisions.tool_execution_success; Type: COMMENT; Schema: public; Owner: ontextract_user
--

COMMENT ON COLUMN public.orchestration_decisions.tool_execution_success IS 'Per-tool success rates and validation results';


--
-- Name: orchestration_feedback; Type: TABLE; Schema: public; Owner: ontextract_user
--

CREATE TABLE public.orchestration_feedback (
    id uuid DEFAULT gen_random_uuid() NOT NULL,
    orchestration_decision_id uuid NOT NULL,
    researcher_id integer NOT NULL,
    researcher_expertise jsonb,
    feedback_type character varying(50) NOT NULL,
    feedback_scope character varying(50),
    original_decision jsonb,
    researcher_preference jsonb,
    agreement_level character varying(20),
    confidence_assessment numeric(4,3),
    reasoning text NOT NULL,
    domain_specific_factors jsonb,
    suggested_tools text[],
    suggested_embedding_model character varying(100),
    suggested_processing_strategy character varying(50),
    alternative_reasoning text,
    feedback_status character varying(20) DEFAULT 'pending'::character varying,
    integration_notes text,
    subsequent_decisions_influenced integer DEFAULT 0,
    improvement_verified boolean,
    verification_notes text,
    provided_at timestamp with time zone DEFAULT CURRENT_TIMESTAMP,
    reviewed_at timestamp with time zone,
    integrated_at timestamp with time zone,
    CONSTRAINT agreement_level_check CHECK (((agreement_level)::text = ANY ((ARRAY['strongly_agree'::character varying, 'agree'::character varying, 'neutral'::character varying, 'disagree'::character varying, 'strongly_disagree'::character varying])::text[]))),
    CONSTRAINT feedback_status_check CHECK (((feedback_status)::text = ANY ((ARRAY['pending'::character varying, 'reviewed'::character varying, 'integrated'::character varying, 'rejected'::character varying, 'obsolete'::character varying])::text[]))),
    CONSTRAINT feedback_type_check CHECK (((feedback_type)::text = ANY ((ARRAY['correction'::character varying, 'enhancement'::character varying, 'validation'::character varying, 'clarification'::character varying])::text[]))),
    CONSTRAINT orchestration_feedback_confidence_assessment_check CHECK (((confidence_assessment >= (0)::numeric) AND (confidence_assessment <= (1)::numeric)))
);


ALTER TABLE public.orchestration_feedback OWNER TO ontextract_user;

--
-- Name: TABLE orchestration_feedback; Type: COMMENT; Schema: public; Owner: ontextract_user
--

COMMENT ON TABLE public.orchestration_feedback IS 'Researcher feedback on orchestration decisions for continuous improvement';


--
-- Name: COLUMN orchestration_feedback.researcher_expertise; Type: COMMENT; Schema: public; Owner: ontextract_user
--

COMMENT ON COLUMN public.orchestration_feedback.researcher_expertise IS 'Researcher expertise profile for weighting feedback authority';


--
-- Name: COLUMN orchestration_feedback.domain_specific_factors; Type: COMMENT; Schema: public; Owner: ontextract_user
--

COMMENT ON COLUMN public.orchestration_feedback.domain_specific_factors IS 'Domain knowledge that LLM missed in original decision';


--
-- Name: orchestration_overrides; Type: TABLE; Schema: public; Owner: ontextract_user
--

CREATE TABLE public.orchestration_overrides (
    id uuid DEFAULT gen_random_uuid() NOT NULL,
    orchestration_decision_id uuid NOT NULL,
    researcher_id integer NOT NULL,
    override_type character varying(50) NOT NULL,
    original_decision jsonb NOT NULL,
    overridden_decision jsonb NOT NULL,
    justification text NOT NULL,
    expert_knowledge_applied jsonb,
    override_applied boolean DEFAULT false,
    execution_results jsonb,
    performance_comparison jsonb,
    applied_at timestamp with time zone DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT override_type_check CHECK (((override_type)::text = ANY ((ARRAY['full_replacement'::character varying, 'tool_addition'::character varying, 'tool_removal'::character varying, 'model_change'::character varying, 'strategy_change'::character varying])::text[])))
);


ALTER TABLE public.orchestration_overrides OWNER TO ontextract_user;

--
-- Name: TABLE orchestration_overrides; Type: COMMENT; Schema: public; Owner: ontextract_user
--

COMMENT ON TABLE public.orchestration_overrides IS 'Manual overrides applied by researchers to specific orchestration decisions';


--
-- Name: processing_artifact_groups; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.processing_artifact_groups (
    id integer NOT NULL,
    document_id integer NOT NULL,
    artifact_type character varying(40) NOT NULL,
    method_key character varying(100) NOT NULL,
    processing_job_id integer,
    parent_method_keys json,
    metadata json,
    include_in_composite boolean DEFAULT true NOT NULL,
    status character varying(20) DEFAULT 'completed'::character varying NOT NULL,
    created_at timestamp without time zone DEFAULT CURRENT_TIMESTAMP NOT NULL,
    updated_at timestamp without time zone DEFAULT CURRENT_TIMESTAMP NOT NULL
);


ALTER TABLE public.processing_artifact_groups OWNER TO postgres;

--
-- Name: processing_artifact_groups_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.processing_artifact_groups_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE public.processing_artifact_groups_id_seq OWNER TO postgres;

--
-- Name: processing_artifact_groups_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.processing_artifact_groups_id_seq OWNED BY public.processing_artifact_groups.id;


--
-- Name: processing_artifacts; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.processing_artifacts (
    id uuid DEFAULT gen_random_uuid() NOT NULL,
    processing_id uuid NOT NULL,
    document_id integer NOT NULL,
    artifact_type character varying(50) NOT NULL,
    artifact_index integer,
    content_json text,
    metadata_json text,
    created_at timestamp without time zone DEFAULT now()
);


ALTER TABLE public.processing_artifacts OWNER TO postgres;

--
-- Name: processing_jobs; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.processing_jobs (
    id integer NOT NULL,
    job_type character varying(50) NOT NULL,
    job_name character varying(100),
    provider character varying(20),
    model character varying(50),
    parameters text,
    status character varying(20) NOT NULL,
    progress_percent integer,
    current_step character varying(100),
    total_steps integer,
    result_data text,
    result_summary text,
    error_message text,
    error_details text,
    retry_count integer,
    max_retries integer,
    tokens_used integer,
    processing_time double precision,
    cost_estimate double precision,
    created_at timestamp without time zone NOT NULL,
    started_at timestamp without time zone,
    completed_at timestamp without time zone,
    updated_at timestamp without time zone,
    user_id integer NOT NULL,
    document_id integer NOT NULL,
    parent_job_id integer
);


ALTER TABLE public.processing_jobs OWNER TO postgres;

--
-- Name: processing_jobs_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.processing_jobs_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE public.processing_jobs_id_seq OWNER TO postgres;

--
-- Name: processing_jobs_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.processing_jobs_id_seq OWNED BY public.processing_jobs.id;


--
-- Name: prompt_templates; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.prompt_templates (
    id integer NOT NULL,
    template_key character varying(100) NOT NULL,
    template_text text NOT NULL,
    category character varying(50) NOT NULL,
    variables jsonb DEFAULT '{}'::jsonb NOT NULL,
    supports_llm_enhancement boolean DEFAULT true,
    llm_enhancement_prompt text,
    is_active boolean DEFAULT true,
    created_at timestamp without time zone DEFAULT now(),
    updated_at timestamp without time zone DEFAULT now()
);


ALTER TABLE public.prompt_templates OWNER TO postgres;

--
-- Name: TABLE prompt_templates; Type: COMMENT; Schema: public; Owner: postgres
--

COMMENT ON TABLE public.prompt_templates IS 'Jinja2 templates for generating descriptions and prompts with optional LLM enhancement';


--
-- Name: COLUMN prompt_templates.template_key; Type: COMMENT; Schema: public; Owner: postgres
--

COMMENT ON COLUMN public.prompt_templates.template_key IS 'Unique template identifier (e.g., experiment_description_single_document)';


--
-- Name: COLUMN prompt_templates.template_text; Type: COMMENT; Schema: public; Owner: postgres
--

COMMENT ON COLUMN public.prompt_templates.template_text IS 'Jinja2 template text with {{ variable }} syntax';


--
-- Name: COLUMN prompt_templates.category; Type: COMMENT; Schema: public; Owner: postgres
--

COMMENT ON COLUMN public.prompt_templates.category IS 'Template category: experiment_description, analysis_summary, etc.';


--
-- Name: COLUMN prompt_templates.variables; Type: COMMENT; Schema: public; Owner: postgres
--

COMMENT ON COLUMN public.prompt_templates.variables IS 'Required variables with types: {"document_title": "string", "word_count": "int"}';


--
-- Name: COLUMN prompt_templates.supports_llm_enhancement; Type: COMMENT; Schema: public; Owner: postgres
--

COMMENT ON COLUMN public.prompt_templates.supports_llm_enhancement IS 'Whether this template can be enhanced by LLM';


--
-- Name: COLUMN prompt_templates.llm_enhancement_prompt; Type: COMMENT; Schema: public; Owner: postgres
--

COMMENT ON COLUMN public.prompt_templates.llm_enhancement_prompt IS 'Prompt for LLM to enhance the rendered template output';


--
-- Name: COLUMN prompt_templates.is_active; Type: COMMENT; Schema: public; Owner: postgres
--

COMMENT ON COLUMN public.prompt_templates.is_active IS 'Whether template is currently active/enabled';


--
-- Name: prompt_templates_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.prompt_templates_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE public.prompt_templates_id_seq OWNER TO postgres;

--
-- Name: prompt_templates_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.prompt_templates_id_seq OWNED BY public.prompt_templates.id;


--
-- Name: prov_activities; Type: TABLE; Schema: public; Owner: ontextract_user
--

CREATE TABLE public.prov_activities (
    activity_id uuid DEFAULT gen_random_uuid() NOT NULL,
    activity_type character varying(100) NOT NULL,
    startedattime timestamp with time zone,
    endedattime timestamp with time zone,
    wasassociatedwith uuid,
    activity_parameters jsonb DEFAULT '{}'::jsonb,
    activity_status character varying(20) DEFAULT 'active'::character varying,
    activity_metadata jsonb DEFAULT '{}'::jsonb,
    created_at timestamp with time zone DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT prov_activities_activity_status_check CHECK (((activity_status)::text = ANY ((ARRAY['active'::character varying, 'completed'::character varying, 'failed'::character varying])::text[]))),
    CONSTRAINT valid_activity_duration CHECK (((startedattime IS NULL) OR (endedattime IS NULL) OR (startedattime <= endedattime)))
);


ALTER TABLE public.prov_activities OWNER TO ontextract_user;

--
-- Name: prov_agents; Type: TABLE; Schema: public; Owner: ontextract_user
--

CREATE TABLE public.prov_agents (
    agent_id uuid DEFAULT gen_random_uuid() NOT NULL,
    agent_type character varying(20) NOT NULL,
    foaf_name character varying(255),
    foaf_givenname character varying(255),
    foaf_mbox character varying(255),
    foaf_homepage character varying(500),
    agent_metadata jsonb DEFAULT '{}'::jsonb,
    created_at timestamp with time zone DEFAULT CURRENT_TIMESTAMP,
    updated_at timestamp with time zone DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT prov_agents_agent_type_check CHECK (((agent_type)::text = ANY ((ARRAY['Person'::character varying, 'Organization'::character varying, 'SoftwareAgent'::character varying])::text[])))
);


ALTER TABLE public.prov_agents OWNER TO ontextract_user;

--
-- Name: prov_entities; Type: TABLE; Schema: public; Owner: ontextract_user
--

CREATE TABLE public.prov_entities (
    entity_id uuid DEFAULT gen_random_uuid() NOT NULL,
    entity_type character varying(100) NOT NULL,
    generatedattime timestamp with time zone,
    invalidatedattime timestamp with time zone,
    wasgeneratedby uuid,
    wasattributedto uuid,
    wasderivedfrom uuid,
    entity_value jsonb DEFAULT '{}'::jsonb NOT NULL,
    entity_metadata jsonb DEFAULT '{}'::jsonb,
    character_start integer,
    character_end integer,
    created_at timestamp with time zone DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT must_have_generation_provenance CHECK ((wasgeneratedby IS NOT NULL)),
    CONSTRAINT valid_character_positions CHECK ((((character_start IS NULL) AND (character_end IS NULL)) OR ((character_start IS NOT NULL) AND (character_end IS NOT NULL) AND (character_start <= character_end))))
);


ALTER TABLE public.prov_entities OWNER TO ontextract_user;

--
-- Name: prov_relationships; Type: TABLE; Schema: public; Owner: ontextract_user
--

CREATE TABLE public.prov_relationships (
    relationship_id uuid DEFAULT gen_random_uuid() NOT NULL,
    relationship_type character varying(50) NOT NULL,
    subject_id uuid NOT NULL,
    subject_type character varying(20) NOT NULL,
    object_id uuid NOT NULL,
    object_type character varying(20) NOT NULL,
    relationship_metadata jsonb DEFAULT '{}'::jsonb,
    created_at timestamp with time zone DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT prov_relationships_object_type_check CHECK (((object_type)::text = ANY ((ARRAY['Agent'::character varying, 'Activity'::character varying, 'Entity'::character varying])::text[]))),
    CONSTRAINT prov_relationships_relationship_type_check CHECK (((relationship_type)::text = ANY (ARRAY[('wasGeneratedBy'::character varying)::text, ('wasAssociatedWith'::character varying)::text, ('wasDerivedFrom'::character varying)::text, ('wasInformedBy'::character varying)::text, ('actedOnBehalfOf'::character varying)::text, ('wasAttributedTo'::character varying)::text, ('used'::character varying)::text, ('wasStartedBy'::character varying)::text, ('wasEndedBy'::character varying)::text, ('wasQuotedFrom'::character varying)::text, ('wasRevisionOf'::character varying)::text, ('hadPrimarySource'::character varying)::text, ('alternateOf'::character varying)::text, ('specializationOf'::character varying)::text]))),
    CONSTRAINT prov_relationships_subject_type_check CHECK (((subject_type)::text = ANY ((ARRAY['Agent'::character varying, 'Activity'::character varying, 'Entity'::character varying])::text[])))
);


ALTER TABLE public.prov_relationships OWNER TO ontextract_user;

--
-- Name: provenance_activities; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.provenance_activities (
    id integer NOT NULL,
    prov_id character varying(255) NOT NULL,
    prov_type character varying(100) NOT NULL,
    prov_label character varying(500),
    started_at_time timestamp without time zone DEFAULT CURRENT_TIMESTAMP,
    ended_at_time timestamp without time zone,
    was_associated_with character varying(255),
    used_plan character varying(255),
    processing_job_id integer,
    experiment_id integer,
    activity_type character varying(50),
    activity_metadata jsonb,
    created_at timestamp without time zone DEFAULT CURRENT_TIMESTAMP NOT NULL,
    updated_at timestamp without time zone DEFAULT CURRENT_TIMESTAMP
);


ALTER TABLE public.provenance_activities OWNER TO postgres;

--
-- Name: TABLE provenance_activities; Type: COMMENT; Schema: public; Owner: postgres
--

COMMENT ON TABLE public.provenance_activities IS 'PROV-O Activity model representing processing activities';


--
-- Name: COLUMN provenance_activities.prov_id; Type: COMMENT; Schema: public; Owner: postgres
--

COMMENT ON COLUMN public.provenance_activities.prov_id IS 'PROV-O Activity identifier (e.g., activity_embeddings_456)';


--
-- Name: COLUMN provenance_activities.prov_type; Type: COMMENT; Schema: public; Owner: postgres
--

COMMENT ON COLUMN public.provenance_activities.prov_type IS 'PROV-O Activity type (e.g., ont:EmbeddingsProcessing)';


--
-- Name: COLUMN provenance_activities.was_associated_with; Type: COMMENT; Schema: public; Owner: postgres
--

COMMENT ON COLUMN public.provenance_activities.was_associated_with IS 'PROV-O wasAssociatedWith agent';


--
-- Name: COLUMN provenance_activities.used_plan; Type: COMMENT; Schema: public; Owner: postgres
--

COMMENT ON COLUMN public.provenance_activities.used_plan IS 'PROV-O used plan/protocol';


--
-- Name: provenance_activities_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.provenance_activities_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE public.provenance_activities_id_seq OWNER TO postgres;

--
-- Name: provenance_activities_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.provenance_activities_id_seq OWNED BY public.provenance_activities.id;


--
-- Name: provenance_chains; Type: TABLE; Schema: public; Owner: ontextract_user
--

CREATE TABLE public.provenance_chains (
    id uuid DEFAULT gen_random_uuid() NOT NULL,
    entity_id uuid,
    entity_type character varying(30) NOT NULL,
    was_derived_from uuid,
    derivation_activity uuid,
    derivation_metadata json,
    created_at timestamp with time zone
);


ALTER TABLE public.provenance_chains OWNER TO ontextract_user;

--
-- Name: provenance_entities; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.provenance_entities (
    id integer NOT NULL,
    prov_id character varying(255) NOT NULL,
    prov_type character varying(100) NOT NULL,
    prov_label character varying(500),
    generated_at_time timestamp without time zone DEFAULT CURRENT_TIMESTAMP,
    invalidated_at_time timestamp without time zone,
    attributed_to_agent character varying(255),
    derived_from_entity character varying(255),
    generated_by_activity character varying(255),
    document_id integer,
    experiment_id integer,
    version_number integer,
    version_type character varying(50),
    prov_metadata jsonb,
    created_at timestamp without time zone DEFAULT CURRENT_TIMESTAMP NOT NULL,
    updated_at timestamp without time zone DEFAULT CURRENT_TIMESTAMP
);


ALTER TABLE public.provenance_entities OWNER TO postgres;

--
-- Name: TABLE provenance_entities; Type: COMMENT; Schema: public; Owner: postgres
--

COMMENT ON TABLE public.provenance_entities IS 'PROV-O Entity model representing first-class provenance entities';


--
-- Name: COLUMN provenance_entities.prov_id; Type: COMMENT; Schema: public; Owner: postgres
--

COMMENT ON COLUMN public.provenance_entities.prov_id IS 'PROV-O Entity identifier (e.g., document_123_v2)';


--
-- Name: COLUMN provenance_entities.prov_type; Type: COMMENT; Schema: public; Owner: postgres
--

COMMENT ON COLUMN public.provenance_entities.prov_type IS 'PROV-O Entity type (e.g., ont:Document, ont:ProcessedDocument)';


--
-- Name: COLUMN provenance_entities.derived_from_entity; Type: COMMENT; Schema: public; Owner: postgres
--

COMMENT ON COLUMN public.provenance_entities.derived_from_entity IS 'PROV-O wasDerivedFrom relationship';


--
-- Name: COLUMN provenance_entities.generated_by_activity; Type: COMMENT; Schema: public; Owner: postgres
--

COMMENT ON COLUMN public.provenance_entities.generated_by_activity IS 'PROV-O wasGeneratedBy relationship';


--
-- Name: provenance_entities_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.provenance_entities_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE public.provenance_entities_id_seq OWNER TO postgres;

--
-- Name: provenance_entities_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.provenance_entities_id_seq OWNED BY public.provenance_entities.id;


--
-- Name: search_history; Type: TABLE; Schema: public; Owner: ontextract_user
--

CREATE TABLE public.search_history (
    id integer NOT NULL,
    query text NOT NULL,
    query_type character varying(50),
    results_count integer,
    execution_time double precision,
    user_id character varying(255),
    ip_address character varying(45),
    created_at timestamp without time zone
);


ALTER TABLE public.search_history OWNER TO ontextract_user;

--
-- Name: search_history_id_seq; Type: SEQUENCE; Schema: public; Owner: ontextract_user
--

CREATE SEQUENCE public.search_history_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE public.search_history_id_seq OWNER TO ontextract_user;

--
-- Name: search_history_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: ontextract_user
--

ALTER SEQUENCE public.search_history_id_seq OWNED BY public.search_history.id;


--
-- Name: semantic_drift_activities; Type: TABLE; Schema: public; Owner: ontextract_user
--

CREATE TABLE public.semantic_drift_activities (
    id uuid DEFAULT gen_random_uuid() NOT NULL,
    activity_type character varying(50) NOT NULL,
    start_period character varying(50) NOT NULL,
    end_period character varying(50) NOT NULL,
    temporal_scope_years integer[],
    used_entity uuid,
    generated_entity uuid,
    was_associated_with uuid,
    drift_metrics json,
    detection_algorithm character varying(100),
    algorithm_parameters json,
    started_at_time timestamp with time zone,
    ended_at_time timestamp with time zone,
    activity_status character varying(20),
    drift_detected boolean,
    drift_magnitude numeric(4,3),
    drift_type character varying(30),
    evidence_summary text,
    created_by integer,
    created_at timestamp with time zone,
    CONSTRAINT semantic_drift_activities_activity_status_check CHECK (((activity_status)::text = ANY ((ARRAY['running'::character varying, 'completed'::character varying, 'error'::character varying, 'provisional'::character varying])::text[]))),
    CONSTRAINT semantic_drift_activities_drift_magnitude_check CHECK (((drift_magnitude >= (0)::numeric) AND (drift_magnitude <= (1)::numeric)))
);


ALTER TABLE public.semantic_drift_activities OWNER TO ontextract_user;

--
-- Name: semantic_shift_analysis; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.semantic_shift_analysis (
    id uuid DEFAULT gen_random_uuid() NOT NULL,
    experiment_id integer NOT NULL,
    term_id uuid NOT NULL,
    shift_type character varying(50) NOT NULL,
    from_period character varying(100),
    to_period character varying(100),
    from_discipline character varying(100),
    to_discipline character varying(100),
    description text NOT NULL,
    evidence text,
    from_document_id integer,
    to_document_id integer,
    from_definition_id uuid,
    to_definition_id uuid,
    edge_type character varying(50),
    edge_label text,
    detected_by character varying(50),
    confidence numeric(3,2),
    created_at timestamp with time zone DEFAULT now()
);


ALTER TABLE public.semantic_shift_analysis OWNER TO postgres;

--
-- Name: TABLE semantic_shift_analysis; Type: COMMENT; Schema: public; Owner: postgres
--

COMMENT ON TABLE public.semantic_shift_analysis IS 'Identified semantic shifts and evolution patterns';


--
-- Name: term_disciplinary_definitions; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.term_disciplinary_definitions (
    id uuid DEFAULT gen_random_uuid() NOT NULL,
    term_id uuid NOT NULL,
    experiment_id integer,
    discipline character varying(100) NOT NULL,
    definition text NOT NULL,
    source_text text,
    source_type character varying(50),
    period_label character varying(100),
    start_year integer,
    end_year integer,
    key_features jsonb,
    distinguishing_features text,
    parallel_meanings jsonb,
    potential_confusion text,
    document_id integer,
    resolution_notes text,
    created_at timestamp with time zone DEFAULT now(),
    updated_at timestamp with time zone DEFAULT now()
);


ALTER TABLE public.term_disciplinary_definitions OWNER TO postgres;

--
-- Name: TABLE term_disciplinary_definitions; Type: COMMENT; Schema: public; Owner: postgres
--

COMMENT ON TABLE public.term_disciplinary_definitions IS 'Disciplinary definitions for metacognitive framework comparison tables';


--
-- Name: term_version_anchors; Type: TABLE; Schema: public; Owner: ontextract_user
--

CREATE TABLE public.term_version_anchors (
    id uuid DEFAULT gen_random_uuid() NOT NULL,
    term_version_id uuid NOT NULL,
    context_anchor_id uuid NOT NULL,
    similarity_score numeric(4,3),
    rank_in_neighborhood integer,
    created_at timestamp with time zone
);


ALTER TABLE public.term_version_anchors OWNER TO ontextract_user;

--
-- Name: term_versions; Type: TABLE; Schema: public; Owner: ontextract_user
--

CREATE TABLE public.term_versions (
    id uuid DEFAULT gen_random_uuid() NOT NULL,
    term_id uuid NOT NULL,
    temporal_period character varying(50) NOT NULL,
    temporal_start_year integer,
    temporal_end_year integer,
    meaning_description text NOT NULL,
    context_anchor json,
    original_context_anchor json,
    fuzziness_score numeric(4,3),
    confidence_level character varying(10),
    certainty_notes text,
    corpus_source character varying(100),
    source_documents json,
    extraction_method character varying(50),
    generated_at_time timestamp with time zone,
    was_derived_from uuid,
    derivation_type character varying(30),
    version_number integer,
    is_current boolean,
    created_by integer,
    created_at timestamp with time zone,
    neighborhood_overlap numeric(4,3),
    positional_change numeric(4,3),
    similarity_reduction numeric(4,3),
    source_citation text,
    CONSTRAINT term_versions_confidence_level_check CHECK (((confidence_level)::text = ANY ((ARRAY['high'::character varying, 'medium'::character varying, 'low'::character varying])::text[]))),
    CONSTRAINT term_versions_fuzziness_score_check CHECK (((fuzziness_score >= (0)::numeric) AND (fuzziness_score <= (1)::numeric))),
    CONSTRAINT term_versions_neighborhood_overlap_check CHECK (((neighborhood_overlap >= (0)::numeric) AND (neighborhood_overlap <= (1)::numeric))),
    CONSTRAINT term_versions_positional_change_check CHECK (((positional_change >= (0)::numeric) AND (positional_change <= (1)::numeric))),
    CONSTRAINT term_versions_similarity_reduction_check CHECK (((similarity_reduction >= (0)::numeric) AND (similarity_reduction <= (1)::numeric)))
);


ALTER TABLE public.term_versions OWNER TO ontextract_user;

--
-- Name: COLUMN term_versions.source_citation; Type: COMMENT; Schema: public; Owner: ontextract_user
--

COMMENT ON COLUMN public.term_versions.source_citation IS 'Academic citation for this temporal version meaning (e.g., dictionary reference, paper, etc.)';


--
-- Name: terms; Type: TABLE; Schema: public; Owner: ontextract_user
--

CREATE TABLE public.terms (
    id uuid DEFAULT gen_random_uuid() NOT NULL,
    term_text character varying(255) NOT NULL,
    entry_date timestamp with time zone,
    status character varying(20) NOT NULL,
    created_by integer,
    updated_by integer,
    created_at timestamp with time zone,
    updated_at timestamp with time zone,
    description text,
    etymology text,
    notes text,
    research_domain character varying(100),
    selection_rationale text,
    historical_significance text,
    CONSTRAINT terms_status_check CHECK (((status)::text = ANY ((ARRAY['active'::character varying, 'provisional'::character varying, 'deprecated'::character varying])::text[])))
);


ALTER TABLE public.terms OWNER TO ontextract_user;

--
-- Name: text_segments_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.text_segments_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE public.text_segments_id_seq OWNER TO postgres;

--
-- Name: text_segments_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.text_segments_id_seq OWNED BY public.text_segments.id;


--
-- Name: tool_execution_logs; Type: TABLE; Schema: public; Owner: ontextract_user
--

CREATE TABLE public.tool_execution_logs (
    id uuid DEFAULT gen_random_uuid() NOT NULL,
    orchestration_decision_id uuid NOT NULL,
    tool_name character varying(50) NOT NULL,
    tool_version character varying(50),
    execution_order integer,
    started_at timestamp with time zone DEFAULT CURRENT_TIMESTAMP,
    completed_at timestamp with time zone,
    execution_time_ms integer,
    execution_status character varying(20) DEFAULT 'running'::character varying,
    output_data jsonb,
    error_message text,
    memory_usage_mb integer,
    cpu_usage_percent numeric(5,2),
    output_quality_score numeric(4,3),
    CONSTRAINT tool_execution_logs_output_quality_score_check CHECK (((output_quality_score >= (0)::numeric) AND (output_quality_score <= (1)::numeric))),
    CONSTRAINT tool_execution_logs_status_check CHECK (((execution_status)::text = ANY ((ARRAY['running'::character varying, 'completed'::character varying, 'error'::character varying, 'timeout'::character varying, 'skipped'::character varying])::text[])))
);


ALTER TABLE public.tool_execution_logs OWNER TO ontextract_user;

--
-- Name: TABLE tool_execution_logs; Type: COMMENT; Schema: public; Owner: ontextract_user
--

COMMENT ON TABLE public.tool_execution_logs IS 'Detailed logs of individual NLP tool execution with performance metrics';


--
-- Name: COLUMN tool_execution_logs.execution_order; Type: COMMENT; Schema: public; Owner: ontextract_user
--

COMMENT ON COLUMN public.tool_execution_logs.execution_order IS 'Order in processing pipeline (0 = first, higher = later)';


--
-- Name: COLUMN tool_execution_logs.output_quality_score; Type: COMMENT; Schema: public; Owner: ontextract_user
--

COMMENT ON COLUMN public.tool_execution_logs.output_quality_score IS 'Quality assessment of tool output (0.0 = poor, 1.0 = excellent)';


--
-- Name: users; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.users (
    id integer NOT NULL,
    username character varying(80) NOT NULL,
    email character varying(120) NOT NULL,
    password_hash character varying(256) NOT NULL,
    first_name character varying(50),
    last_name character varying(50),
    organization character varying(100),
    is_active boolean NOT NULL,
    is_admin boolean NOT NULL,
    created_at timestamp without time zone NOT NULL,
    updated_at timestamp without time zone,
    last_login timestamp without time zone
);


ALTER TABLE public.users OWNER TO postgres;

--
-- Name: users_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.users_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE public.users_id_seq OWNER TO postgres;

--
-- Name: users_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.users_id_seq OWNED BY public.users.id;


--
-- Name: version_changelog_id_seq; Type: SEQUENCE; Schema: public; Owner: ontextract_user
--

CREATE SEQUENCE public.version_changelog_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE public.version_changelog_id_seq OWNER TO ontextract_user;

--
-- Name: version_changelog_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: ontextract_user
--

ALTER SEQUENCE public.version_changelog_id_seq OWNED BY public.version_changelog.id;


--
-- Name: app_settings id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.app_settings ALTER COLUMN id SET DEFAULT nextval('public.app_settings_id_seq'::regclass);


--
-- Name: document_embeddings id; Type: DEFAULT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.document_embeddings ALTER COLUMN id SET DEFAULT nextval('public.document_embeddings_id_seq'::regclass);


--
-- Name: document_processing_summary id; Type: DEFAULT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.document_processing_summary ALTER COLUMN id SET DEFAULT nextval('public.document_processing_summary_id_seq'::regclass);


--
-- Name: documents id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.documents ALTER COLUMN id SET DEFAULT nextval('public.documents_id_seq'::regclass);


--
-- Name: domains id; Type: DEFAULT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.domains ALTER COLUMN id SET DEFAULT nextval('public.domains_id_seq'::regclass);


--
-- Name: experiment_documents_v2 id; Type: DEFAULT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.experiment_documents_v2 ALTER COLUMN id SET DEFAULT nextval('public.experiment_documents_v2_id_seq'::regclass);


--
-- Name: experiments id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.experiments ALTER COLUMN id SET DEFAULT nextval('public.experiments_id_seq'::regclass);


--
-- Name: extracted_entities id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.extracted_entities ALTER COLUMN id SET DEFAULT nextval('public.extracted_entities_id_seq'::regclass);


--
-- Name: ontologies id; Type: DEFAULT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.ontologies ALTER COLUMN id SET DEFAULT nextval('public.ontologies_id_seq'::regclass);


--
-- Name: ontology_entities id; Type: DEFAULT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.ontology_entities ALTER COLUMN id SET DEFAULT nextval('public.ontology_entities_id_seq'::regclass);


--
-- Name: ontology_mappings id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.ontology_mappings ALTER COLUMN id SET DEFAULT nextval('public.ontology_mappings_id_seq'::regclass);


--
-- Name: ontology_versions id; Type: DEFAULT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.ontology_versions ALTER COLUMN id SET DEFAULT nextval('public.ontology_versions_id_seq'::regclass);


--
-- Name: processing_artifact_groups id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.processing_artifact_groups ALTER COLUMN id SET DEFAULT nextval('public.processing_artifact_groups_id_seq'::regclass);


--
-- Name: processing_jobs id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.processing_jobs ALTER COLUMN id SET DEFAULT nextval('public.processing_jobs_id_seq'::regclass);


--
-- Name: prompt_templates id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.prompt_templates ALTER COLUMN id SET DEFAULT nextval('public.prompt_templates_id_seq'::regclass);


--
-- Name: provenance_activities id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.provenance_activities ALTER COLUMN id SET DEFAULT nextval('public.provenance_activities_id_seq'::regclass);


--
-- Name: provenance_entities id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.provenance_entities ALTER COLUMN id SET DEFAULT nextval('public.provenance_entities_id_seq'::regclass);


--
-- Name: search_history id; Type: DEFAULT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.search_history ALTER COLUMN id SET DEFAULT nextval('public.search_history_id_seq'::regclass);


--
-- Name: text_segments id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.text_segments ALTER COLUMN id SET DEFAULT nextval('public.text_segments_id_seq'::regclass);


--
-- Name: users id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.users ALTER COLUMN id SET DEFAULT nextval('public.users_id_seq'::regclass);


--
-- Name: version_changelog id; Type: DEFAULT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.version_changelog ALTER COLUMN id SET DEFAULT nextval('public.version_changelog_id_seq'::regclass);


--
-- Data for Name: alembic_version; Type: TABLE DATA; Schema: public; Owner: ontextract_user
--

COPY public.alembic_version (version_num) FROM stdin;
a7b9c2d4e5f6
20241221_experiment_centric
\.


--
-- Data for Name: analysis_agents; Type: TABLE DATA; Schema: public; Owner: ontextract_user
--

COPY public.analysis_agents (id, agent_type, name, description, version, algorithm_type, model_parameters, training_data, expertise_domain, institutional_affiliation, created_at, is_active, user_id) FROM stdin;
4a32b8b0-8581-4975-8785-929ec8c4878f	Person	Manual Curation	Human curator performing manual semantic analysis	1.0	Manual_Curation	\N	\N	\N	\N	\N	\N	\N
f959c050-3cc8-4549-a2f2-d3894198ca53	SoftwareAgent	HistBERT Temporal Embedding Alignment	Historical BERT model for temporal semantic alignment	1.0	HistBERT	\N	\N	\N	\N	\N	\N	\N
fdacd5b5-4b12-41fe-83d2-172a5581e53e	SoftwareAgent	Word2Vec Diachronic Analysis	Word2Vec model trained on temporal corpora	1.0	Word2Vec	\N	\N	\N	\N	\N	\N	\N
89c10255-275f-40ac-9139-8b3fbb9c2026	SoftwareAgent	demo_orchestrator	Demo LLM orchestration agent	1.0.0	llm_orchestration	{"method": "llm_orchestration", "models": ["claude-sonnet-4", "gpt-4", "gemini-1.5"]}	\N	\N	\N	2025-09-06 16:00:24.194906-04	t	\N
\.


--
-- Data for Name: app_settings; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.app_settings (id, setting_key, setting_value, category, data_type, description, default_value, requires_llm, user_id, created_at, updated_at) FROM stdin;
1	spacy_model	"en_core_web_sm"	nlp	string	Default spaCy model	"en_core_web_sm"	f	\N	2025-11-08 14:47:25.108698	2025-11-08 14:47:25.108702
2	nltk_tokenizer	"punkt"	nlp	string	NLTK tokenizer to use	"punkt"	f	\N	2025-11-08 14:47:25.112636	2025-11-08 14:47:25.112639
3	default_language	"en"	nlp	string	Default document language	"en"	f	\N	2025-11-08 14:47:25.11335	2025-11-08 14:47:25.113351
4	enable_lemmatization	true	nlp	boolean	Enable lemmatization in NLP processing	true	f	\N	2025-11-08 14:47:25.113921	2025-11-08 14:47:25.113922
5	enable_pos_tagging	true	nlp	boolean	Enable POS tagging	true	f	\N	2025-11-08 14:47:25.114566	2025-11-08 14:47:25.114567
6	default_segmentation_method	"paragraph"	processing	string	Default text segmentation method	"paragraph"	f	\N	2025-11-08 14:47:25.115244	2025-11-08 14:47:25.115246
7	default_embedding_model	"all-MiniLM-L6-v2"	processing	string	Default embedding model	"all-MiniLM-L6-v2"	f	\N	2025-11-08 14:47:25.115792	2025-11-08 14:47:25.115793
8	embedding_dimension	384	processing	integer	Embedding vector dimension	384	f	\N	2025-11-08 14:47:25.116306	2025-11-08 14:47:25.116307
9	max_segment_length	512	processing	integer	Maximum segment length in tokens	512	f	\N	2025-11-08 14:47:25.116946	2025-11-08 14:47:25.116947
10	enable_period_aware_processing	false	processing	boolean	Enable period-aware embedding models	false	f	\N	2025-11-08 14:47:25.117516	2025-11-08 14:47:25.117517
11	default_llm_provider	"anthropic"	llm	string	Default LLM provider	"anthropic"	f	\N	2025-11-08 14:47:25.118047	2025-11-08 14:47:25.118048
12	enable_llm_enhancement	false	llm	boolean	Enable LLM enhancement features	false	f	\N	2025-11-08 14:47:25.118601	2025-11-08 14:47:25.118602
14	llm_max_tokens	500	llm	integer	Maximum tokens for LLM responses	500	f	\N	2025-11-08 14:47:25.120212	2025-11-08 14:47:25.120214
15	theme	"darkly"	ui	string	UI theme name	"darkly"	f	\N	2025-11-08 14:47:25.120977	2025-11-08 14:47:25.120978
16	default_experiment_view	"grid"	ui	string	Default experiment view mode	"grid"	f	\N	2025-11-08 14:47:25.121765	2025-11-08 14:47:25.121767
17	show_provenance_by_default	false	ui	boolean	Show provenance info by default	false	f	\N	2025-11-08 14:47:25.122529	2025-11-08 14:47:25.12253
18	items_per_page	20	ui	integer	Items per page in lists	20	f	\N	2025-11-08 14:47:25.123198	2025-11-08 14:47:25.123199
13	llm_model	"claude-sonnet-4-5-20250929"	llm	string	Default LLM model	"claude-3-5-sonnet-20241022"	f	\N	2025-11-08 14:47:25.119138	2025-11-08 14:47:25.119139
\.


--
-- Data for Name: context_anchors; Type: TABLE DATA; Schema: public; Owner: ontextract_user
--

COPY public.context_anchors (id, anchor_term, frequency, first_used_in, last_used_in, created_at) FROM stdin;
466fe463-f97c-40c8-b13f-63c539b619d8	cell	1	\N	\N	2025-08-24 13:26:19.747627-04
187a15c2-0041-4dc1-a4fe-c6d5efc265e9	granularity	1	\N	\N	2025-08-24 13:37:42.66053-04
4e4d398a-e465-4d5a-a743-903ef78bf7d5	consisting	1	\N	\N	2025-08-24 13:37:42.669698-04
bbe2884b-d4b2-44ef-a40f-5bf2bc95d6af	appearing	1	\N	\N	2025-08-24 13:37:42.67267-04
c794432d-7542-45c4-9eb3-56f383f0a232	consist	1	\N	\N	2025-08-24 13:37:42.675239-04
20bbf849-cb9f-4bd1-b242-68da857075e3	finely	1	\N	\N	2025-08-24 13:37:42.677822-04
be420c20-5278-4184-a725-3df467434af4	detailed	1	\N	\N	2025-08-24 13:37:42.680428-04
ded8a9b4-6329-4289-8481-f3d3a580821b	word	1	\N	\N	2025-09-02 14:12:42.815052-04
e75826c3-2f6a-447e-b584-70fd0278bea9	language unit	1	\N	\N	2025-09-02 14:12:42.82656-04
acc4e4ad-04fc-42b6-9f37-b70e132b26f4	charade	1	\N	\N	2025-09-02 14:12:42.829576-04
14c18a05-b195-46a3-a5b0-bc3381db0894	meronym	1	\N	\N	2025-09-02 14:12:42.832622-04
e52c0e9a-c7b1-43c1-9db8-1e2be5f7af1a	hooligan	1	\N	\N	2025-08-24 14:03:22.104634-04
99267944-387f-4c8d-a0ba-62bbc69d2f4d	usually	1	\N	\N	2025-08-24 14:03:22.110767-04
0bd2bb6f-36f3-46cf-9b60-d7ab53b77fa5	young	1	\N	\N	2025-08-24 14:03:22.113901-04
03406a50-523c-43c3-aeb4-e151c60f442a	engages	1	\N	\N	2025-08-24 14:03:22.118188-04
a50cc9e3-f468-4a60-9278-14fc783189e5	agent	2	\N	d8bee452-531c-4475-9488-1fa27d130979	2025-08-24 13:18:57.911853-04
0f6ed02a-0d5d-4325-91cf-7f8701b4f480	ontology	2	\N	561b0ffd-07f4-4152-8e8b-ec8d02d04104	2025-08-24 13:27:14.07582-04
\.


--
-- Data for Name: document_embeddings; Type: TABLE DATA; Schema: public; Owner: ontextract_user
--

COPY public.document_embeddings (id, document_id, term, period, embedding, model_name, context_window, extraction_method, metadata, created_at, updated_at) FROM stdin;
\.


--
-- Data for Name: document_processing_index; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.document_processing_index (id, document_id, experiment_id, processing_id, processing_type, processing_method, status, created_at) FROM stdin;
\.


--
-- Data for Name: document_processing_summary; Type: TABLE DATA; Schema: public; Owner: ontextract_user
--

COPY public.document_processing_summary (id, document_id, processing_type, status, source_document_id, job_id, priority, created_at, updated_at) FROM stdin;
\.


--
-- Data for Name: document_temporal_metadata; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.document_temporal_metadata (id, document_id, experiment_id, temporal_period, temporal_start_year, temporal_end_year, publication_year, discipline, subdiscipline, key_definition, semantic_features, semantic_shift_type, timeline_position, timeline_track, marker_color, extraction_method, extraction_confidence, reviewed_by, reviewed_at, created_at, updated_at) FROM stdin;
\.


--
-- Data for Name: documents; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.documents (id, title, content_type, document_type, reference_subtype, file_type, original_filename, file_path, file_size, source_metadata, content, content_preview, detected_language, language_confidence, status, word_count, character_count, created_at, updated_at, processed_at, user_id, embedding, parent_document_id, processing_metadata, version_number, version_type, experiment_id, source_document_id, processing_notes, composite_sources, composite_metadata, metadata_provenance, uuid) FROM stdin;
172	Black's Law Dictionary 2nd Edition (1910) - Agent	file	reference	legal_dictionary	pdf	AGENT 1910.pdf	uploads/63020c7b9b114b52_AGENT 1910.pdf	4371143	{"year": 1910, "discipline": "Law", "upload_order": 1}	AGE. Signifies thoee periods In the lives \nof persons of both sexes which enable them \nto do certain acts which, before they bad \narrived at those periods, they were prohibit \ned from doing. . \nThe length of time during which a per\nson has lived or a thing has existed. \nIn the old books, "age" ls commonly used \nto signify "foll age;" that le, the age of \ntwenty-one years. Litt. I 259. \n-LeJPll age. The qe at which the person \nacquires full capacity to make his own con\ntracts and deeds and transact business general ly (age of majority) or to enter into some par\nticular contract or relation, as, the "legal age \nof consent" to marriage. See Capwell v. Cap\nwell, 21 R. I. 101, 41 Atl. 1005: Montoya de \nAntonio v. Miller, 7 N. M. 289, 84 Pac. 40, 21 L.R. A.699. ' \nAGE, Awe, Alff. L. Fr. Water. Kel \nham. \nAGE PBA"l"EB. A suggestion of non \nage, made by an Infant party to a real ac\ntion, with a prayer that the proceedings \nmay be deferred until his full age. It ls \nnow abolished. St. 11 Geo. IY. ; 1 Wm. IV. \nc. 87, S 10; 1 Lil. Reg. M; S Bl Comm. 800. \nAGEXOY. A relation, created either by \nexpress or Implied contract or by law, where\nby one party (called the principal or con \nstltuent) delegates the transaction of some \nlawful buslneu or the authority to do cer \ntaln acts for him or In relation to his rights \nor property, with more or less discretionary \npower, to another person (called the agent, \nattorney, proxy, or delegate) who under\ntakes to manage the affair and render him \nan account thereof.  State v. Hubbard, 58 \nKan. 797, 61 Pac. 290, 89 L. R. A. 860; \nSternaman v. Insurance Co., 170 N. Y. 18, \n62 N. E. 768, 67 L. R. A. 318, 88 Am. St. \nRep. 625; Wynegar v. State, 167 Ind. 677, \n62 N. E. 88. \nThe contract of agency may be defined to be \na'contract by which one of the contracting par\nties confides the management of aome affair, to \nbe transacted on his account, to the other par\nty, who undertake to do the busineaa and ren,\nder an account of it. 1 Liverm. Prln. & Ag. 2. \nA contract by which one person, with greater \nor lees discretionary power, undertakes to rep\nresent an.other in certain business relations. \nWhart. Ag. 1. A relation between two or more persons, by \nwhich one party, usually called the agent or \nattomey. is authorized to do certain acts for, or \nin relation to the rights or propert,v of the \nother, who 111 denominated the principal, con \nstituent, or employer. Bouvier. \n-.Apao7, deed of. A revO<'able and volun \ntary trust for payment qf debts. ,Vharton.\nAaeao,- of aeoeaalt,r. A term sometimes ap\nplfed to the kind of implied agency which en\nables a wife to procure what 111 reasonably \nnecet1Sary for her maintenance and support on \nher husband's credit and at his expenae, when \nhe fails to make proper provision for her necee\ntrities. R01<twick v. Brower, 22 l\\Ilsc. Rep. 700, \n49 N. Y. Supp. 1046. \nAGEXESIA. In medical Jurisprudence. \nImpotenda pnerandi ; sexual impotence; \nAGENT \nIncapacity for reproduction, eating In el \nther sex, and whether artslng from struc\ntural or other cauaes. \nAGE1'1'1lmA. Sax. The true master \nor owner of a thing. Spelman. \nAG.E.NHI.N.a.. In Saxon law. A guest \nat an Inn, who, h8\\1ug stayed there for \nthree nights, was then accounted one of the \nfamily. Oowell. \nAGEXB. Lat. An agent, a conductor. \nor manager of atralrs. Distinguished from \nfactor, a workman. A plalutltr. Fleta, lib. \n4. c. 15, I s. \nAGElfT. One who represents and acts \nfor another under the contract or relation \nof agency, q. 11. \nOlautJl.oatloa. A.gents are either ge1t6f'Ol or \n1pecial. A general aient is one employed in \nhis capacity as a professional man or master \nof an art or trade, or one to whom the principal \nconfides his whole busineu or all transactions \nor functions of a desi,tnated clau ; or he fa a \nperson who is authorized b:, hia principal to \nexecute all deeds, sign all contracts, or pur\nchase all goode, required in a f)llrticular trade, busineu, or employment. See Story, Ag. I 17; \nButler v. Maples. 9 Wall. 706, 19 L. Ed. 822; \nJaques v. Todd, 8 Wend. (N. Y.) 90; Spri11.g\nfield Engine Co. v. Kenned]', 7 Ind. App. 502, 84 N . E. 81>6;_ . Cruzan v. Smith. 41 Ind. 297; \nGods .haw v. .-:Jtruck, 109 Ky. 285, 58 S. W. \n781. 61 L. R. A. 668. A special agent Is one \nemployed to conduct a -particular transaction or \npiece of business for hi principal or authorls \ned to perform a specified a.ct. Bryant v. Moore, \n26 Me. 87, 45 Am. Dec. 00: Gibson v. Snow \nHardware Co.. 94 Ala. 346, 10 South. 304: \nCoolei v. Perrine, 41 N. J. Law, 325, 32 Am. \nRep. -10. \nAgents employed for the ale of goods or mer\nchandise are called "mercantile agents," and \nare of two principal claBBeS.--brokers and fac\ntorB, (q. 11.;) a factor ls sometimes called a \n"eommiBBion agent," or "eommiasion merchant." \nRuu. Mere. Ac~ 1. \nBpoa:,ma. The term "agent" 111 to be distinguished from ita synonyms "servant," \n"representative," and "trustee." A servant acts \nIn behalf of hi, master and under the latter' direction and authority, but is regarded as a \nmere lnatrument, and not as the substitute or \nproxy of the master. Tomer v. CroBB, 83 Tex . \n218, 18 S. W. 578, 16 L. R. A. 262; People \nv. Treadwell, 69 Cal. 226, 10 Pac. 502. A \nrepresentative (such as an executor or an as \nsignee in bankruptcy) owes his power and au\nthority to the law, whkh puts him In the place \nof the person represented, although the latter \nmay have designated or chosen the represent \ntlve. A trustee acts in the interest and for the \nbenefit of one person, but by an authority de\nrived from another peraon. \nIn lateraatloaal law; A diplomatic \nagent is a person employed by a sovereign \nto manage his private alralre, or those of his \nsubjects In his name, at the court of a for\neign government. Wollf, Inst. Nat. I 1237. \nIn the practice of the hoaae of lol'da \n- p1'h7 oo-eu. In -SJJpeals, solleltors \nand other persons admitted to practlee In \nthoee courta In a Blmllar capacl to tl',tt ot \nDigitized by oogle \n\nAGENT \n10Jlcltora In ordinary courts, are technically \ncalled "agents." Macph. Prlv. Conn. 00. \n-Agent aacl patleat. A phrase Indicating \nthe state of a pel'IIOn who Is required to do a \nthinA'. and is at the same time the pel'IIOn to \nwhom it la done.-Looal ag-t. One ap\npointed to act as the rep?Hentative of a corporation and tranaact Its buslnes1 cenera.111 \n(or b1JBinetl8 of a particular character) at a giv\nen place or within a defined district. See Frick Co. v. Wright, 28 Tex. Civ. App. 340, M S. W. 608; Moore v. Freeman's Nat. Bank, 92 \nN. C. 594-; Western. etc., Organ Co. v. Ander\naon, 97 Tex. 432, 79 S. W. 517.-llaaadas aaeat. A pel'IIOn who is Invested with general \npower. involving the exercise of judgment and \ndiscretion, u distinguished from an. ordinary \nqent or employ6, who acts in an inferior ca\npacit.r., and under the direction and control of superior authority, both In regard to the extent \nof the work and the manner of executing the \name. Reddington v. Mariposa Land & Min. Co.. 19 Hun (N. Y.) 406; Taylor v. Granite \nState Prov. ABS'n, 136 N. Y. 348, 32 N. E. 992{ 32 Am. St. Rep. 749; U. S. v. American Bel \nTeL Co. (C. C.) 29 fed. 33: Up~r Miealeaippl \nTranap. Co. v. Whittaker, 16 W11. 220: Fos\nter v. Charles Betcher Lumber Co. 1 5 S. D. 57, 58 N. W. 9, 23 L. R. A. 400, 49 am. St. Rep. \n859.-Pr:l:n~te ageat. An. agent acting for an \nindividual in his _private alfall'II; as distin\nguished from a p11bl10 agent, who represent& the government in aome adminl11trative capacit:,.\nPallUe ..-1:. An acent of the public, the state, or the government; a peIBOn appointed \nto act for the nubile in some matter pertaining to the administration of rovemment or the pul>\nlic balline1J11. See Sto_ry, Ag. I 30'2; Whiteside \nv. United Statea, 98 U. S. 2G4. 23 L. Ed. 882. \n-a.al-estate a,ieat. Any person whose \n1msinetl8 it is to sell. or offer for aale, real es-\n. tate for othen, or to ren.t houees, storee, or \nother buildings, or real estate. or to collect \nftDt for othera. Act .Jul1 13. 1866, c. 49; 14 \nSt. at Larire, 118. Carstens v, McReavy, 1 \nWub. St. 359, 25 Pac. 4TI. \nApnlt.. ot oo-atleat.. pan poa\ni,leoteatur. Acting and consenting parties \nare llable to the llllJDe punishment. CS Coke, \n8). \nAGEL Lat. la tile olvil law. A. \nGeld; land generally. A portion of land in\nclcmed by definite boundaries. Municipality \nNo. 2 v. Orleans Ootton Presa, 18 La. 167, 36 \nAm. Dec. 62'. \nIa oW EwsHu law. An acre. Spelman. \nAGGEB. Lat. In the civil law. A dam, \nbank or mound. Cod. 9, 38; Town1h. Pl. \n48. \nAGGRAVATED ASBA'ln,T, An aa\nault with circumstances of aggravation, or \nof a heinous character, or with intent to \neommlt another crime. In re Burns (C. C.) \n113 Fed. 99'l; Norton v. State, 14 Tex. 303. \nSee AllaAULT. \nDefined In Penn91lvanla u follow: "If an1 pel'IIOn shall unlawfully and maliciously inflict \nupon another person, either with or without \nany weapon or instrument, any rrievo1111 bodily \nllarm., or tllllawfall1 cut. stab, or wound any \nother IJ4lrBOD. be shall be ,rullt.r. of a misde\nmeanor,'' etc. Briptl.J. Purd. Dir, p. 434, I \n1t7. \nAG ILLA RIUS \nAGGRAVATIOl'f, Any circumt1tance at\ntending the commission of a crime or tort \nwhich Increases its guilt or enormity or \nadds to its Injurious consequences, but which \niB above and beyond the essential constitu\nents of the crime or tort itself. \nMatter of &gJtravatlon, -correctly understood, \ndoes not consist in acts of the same kind and \nde1eription as those constituting the gist of the action, but In something done b:, the defendant, \non the occasion of committing the tresp8.1'8, \nwhich is. to some extent, of a different legal character from the principal act complained of. \nHatbawa1 v. Rice, 19 Vt. 107. \nIa pleadbag. The Introduction of mat\nter into the declaration which tends to In\ncrease the amount of damages. but does not \naffect the right of action Itself. Steph. Pl. \n257; 12 Mod. 597. \nAGGREGATE. Composed of several : \nconsisting of mnny penons united together. \n1 Bl. Comm. 469. \n-Assresate eorporatlo-. See CORPORA \nTION. \nAGGREGATIO .aE.NflUJIIL The meet \nIng of minds. The moment when a contract \n18. complete. A suppoeed derivation of the \nword "agreement." \nAGGRESO:a. The party who first of\nfers violence or offense. He who begins a \nquarrel or dispute, either 'by threatening or \natrlldng another. \nAGGRIEVED. Having suffered !OBS or \ninjury ; . damnlfied : Injured. \nAGGRIEVED PARTY. Under statutes \ngranting the right of appeal to the party \naggrieved by an order or Judgment, the par\nty aggrieved ls one whose pecuniary inter\nest la directly affected by the adjudication; \none whose right of property may be estab\nllahed or divested thereby. Ruff v. Mont\ngomery, 83 MIBB. 185, 36 South. 67; McFar\nland v. Pierce, 161 Ind. r546, 45 N. E. 706; \nLamar v. Lamar, 118 Ga. 684, 45 8. E. 498; \nSmith 1'. Bradstreet, 16 Pick. (Mass.) 264; \nBryant v. Allen, 6 N. H. 116; Wiggin v. \nSwett, 6 Mete. (Maas.) 194, 39 Am. Dec. 716 : \nTillinghast v. Brown University, 24 R. I. 179, \n62 Atl. 891 : Lowery v. Lowery, 64 N. C. \n110; Raleigh v. Rogers, 25 N . .J. Eq. 506. Or \none against whom error has been commftte1l. \nKinealy v. Macklin, 67 Mo. 93. \nA.GILD. In Saxon law. Free from pen\nalty, not subject, to the payment of ,Ud, or \ntcereQild ,  that is, the customary fine or pe\ncuniary compensation for an offense. Spel\nman; O>well \nAGILER. In Saxon law. An observer \nor Informer. \nAGILLARl'U8. L. Lnt. In old Engllsb \nlaw. A hayward, herdwnrd, or keeper of \nthe berd of cattle in a common G field. Cow.ell. \nDigitized by 008 e 	AGE. Signifies thoee periods In the lives \nof persons of both sexes which enable them \nto do certain acts which, before they bad \narrived at those periods, they were prohibit \ned from doing. . \nThe length of time during which a per\nson has lived or a thing has existed. \nIn the old books, "age" ls commonly used \nto signify "foll age;" that le, the age of \ntwenty-one years. Litt. I 259. \n-LeJPll age. The qe at which the person \nacquires full capacity to make his own con\ntracts and deeds and tra	en	0.9	uploaded	2053	11599	2025-11-15 16:08:53.248754	2025-11-15 16:08:53.24876	\N	1	\N	\N	\N	1	original	\N	\N	\N	\N	\N	{}	46c22311-1730-45fb-a4e8-a85dc80f7348
173	Intention - G.E.M. Anscombe (1957)	file	reference	academic	pdf	Anscombe-Intention-1956.pdf	uploads/ba62f46727b54594_Anscombe-Intention-1956.pdf	521606	{"year": 1957, "discipline": "Philosophy", "upload_order": 2}	 \n \nIntention\nAuthor(s): G. E. M. Anscombe\nSource: Proceedings of the Aristotelian Society, New Series, Vol. 57 (1956 - 1957), pp. 321-\n332\nPublished by: Oxford University Press on behalf of The Aristotelian Society\nStable URL: https://www.jstor.org/stable/4544583\nAccessed: 05-09-2025 17:48 UTC\n \nJSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide\nrange of content in a trusted digital archive. We use information technology and tools to increase productivity and\nfacilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.\n \nYour use of the JSTOR archive indicates your acceptance of the Terms & Conditions of Use, available at\nhttps://about.jstor.org/terms\n \nThe Aristotelian Society, Oxford University Press are collaborating with JSTOR to\ndigitize, preserve and extend access to Proceedings of the Aristotelian Society\nThis content downloaded from 132.174.234.36 on Fri, 05 Sep 2025 17:48:40 UTC \nAll use subject to https://about.jstor.org/terms \n\n Meeting of the Aristotelian Society at 21, Bedford Square, London,\n W.C.I, on 3rd June, 1957, at 7.30 p.m.\n XIV.-INTENTION \n By G. E. M. ANSCOMBE \n What distinguishes actions which are intentional from those \n which are not? The answer that suggests itself is that they \n are the actions to which a certain sense of the question \n 'Why? ' is given application; the sense is defined as that in \n which the answer, if positive, gives a reason for acting. But \n this hardly gets us any further, because the questions \n 'What is the relevant sense of the question " Why? " ' and \n 'What is meant by " reason for acting"?' are one and the \n same.\n To see the difficulties here, consider the question ' Why \n did you knock the cup off the table ? ' answered by' I thought \n I saw a face at the window and it made me jump.' Now \n we cannot say that since the answer mentions something \n previous to the action, this will be a cause as opposed to a \n reason; for if you ask 'Why did you kill him?' the answer \n ' he killed my father' is surely a reason rather than a cause, \n but what it mentions is previous to the action. It is true \n that we don't ordinarily think of a case like giving a sudden \n start when we speak of a reason for acting. ' Giving a sudden \n start ', someone might say, ' is not acting in the sense suggested \n by the expression "reason for acting".' Hence, though \n indeed we readily say e.g. ' What was the reason for your \n starting so violently? ' this is totally unlike ' What is your \n reason for excluding so-and-so from your will? ' or 'What \n is your reason for sending for a taxi? ' But what is the \n difference? Why is giving a start or gasp not an ' action', \n while sending for a taxi or crossing the road is one? The \n answer cannot be ' Because an answer to the question \n " why? " may give a reason in the latter cases ', for the \n answer may 'give a reason' in the former cases too; and \nThis content downloaded from 132.174.234.36 on Fri, 05 Sep 2025 17:48:40 UTC \nAll use subject to https://about.jstor.org/terms \n\n 322 G. E. M. ANSCOMBE \n we cannot say ' Ah, but not a reason for acting; ' we should \n be going round in circles. We need to find the difference \n between the two kinds of ' reason ' without talking about \n ' acting'; and if we do, perhaps we shall discover what \n is meant by 'acting' when it is said with this special \n emphasis. \n It will hardly be enlightening to say ' in the case of the \n sudden start the "reason" is a cause'; the topic of causality \n is in a state of too great confusion; all we know is that this \n is one of the places where we do use the word ' cause '. \n But we also know that this is rather a strange case of \n causality; the subject is able to give a cause of a thought \n or feeling or bodily movement in the same kind of \n way as he is able to state the place of his pain or the \n position of his limbs. Such statements are not based on \n observation. \n Nor can we say: 'Well, the " reason " for a movement \n is a cause, and not a reason in the sense of "reason for \n acting ", when the movement is involuntary; it is a reason \n as opposed to a cause, when the movement is voluntary and \n intentional.' This is partly because in any case the object \n of the whole enquiry is really to delineate such concepts \n as the voluntary and the intentional, and partly because \n one can also give a ' reason' which is only a ' cause' for \n what is voluntary and intentional. E.g. ' Why are you \n walking up and down like that? '-' It's that military band; \n it excites me.' Or ' What made you sign the document \n at last? '-' The thought: " It is my duty " kept hammering \n away in my mind until I said to myself" I can do no other", \n and so signed.' \n Now we can see that the cases where this difficulty \n arises are just those where the cause itself, qua cause, (or \n perhaps one should rather say the causation itself) is in the \n class of things known without observation. \n I will call the type of cause in question a ' mental cause'. \n Mental causes are possible, not only for actions (' The martial \n music excites me, that is why I walk up and down ') but \nThis content downloaded from 132.174.234.36 on Fri, 05 Sep 2025 17:48:40 UTC \nAll use subject to https://about.jstor.org/terms \n\n INTENTION 323 \n also for feelings and even thoughts. In considering actions, \n it is important to distinguish between mental causes and \n motives; in considering feelings, such as fear or anger, it \n is important to distinguish between mental causes and \n objects of feeling. To see this, consider the following \n cases: \n A child saw a bit of red stuff on a turn in a stairway and \n asked what it was. He thought his nurse told him it was a \n bit of Satan and felt dreadful fear of it. (No doubt she said \n it was a bit of satin.) What he was frightened of was the \n bit of stuff; the cause of his fright was his nurse's remark. \n The object of fear may be the cause of fear, but, as \n Wittgenstein' remarks, is not as such the cause of fear. (A \n hideous face appearing at the window would of course be \n both cause and object, and hence the two are easily confused.) \n Or again, you may be angry at someone's action, when \n what makes you angry is some reminder of it, or someone's \n telling you of it. \n This sort of cause of a feeling or reaction may be reported \n by the person himself, as well as recognised by someone \n else, even when it is not the same as the object. Note that \n this sort of causality or sense of ' causality ' is so far from \n accommodating itself to Hume's explanations that people \n who believe that Hume pretty well dealt with the topic of \n causality would entirely leave it out of their calculations; \n if their attention were drawn to it they might insist that the \n word ' cause' was inappropriate or was quite equivocal. \n Or conceivably they might try to give a Humeian \n account of the matter as far as concerned the outside \n observer's recognition of the cause; but hardly for the \n patient's. \n Now one might think that when the question 'Why?' \n is answered by giving the intention with which a person \n acts-a case of which I will here simply characterise by \n ' Philosophical Investigations, ? 476. \nThis content downloaded from 132.174.234.36 on Fri, 05 Sep 2025 17:48:40 UTC \nAll use subject to https://about.jstor.org/terms \n\n 324 G. E. M. ANSCOMBE \n saying that it mentions something future-this is also a case \n of a mental cause. For couldn't it be recast in the form: \n 'Because I wanted ... or ' Out of a desire that...' ? \n If a feeling of desire for an apple affects me and I get up and \n go to a cupboard where I think there are some, I might \n answer the question what led to this action by mentioning \n the desire as having made me . . . etc. But it is not in all \n cases that ' I did so and so in order to . . .' can be backed \n up by ' I felt a desire that . . . ' I may e.g. simply hear \n a knock on the door and go downstairs to open it without \n experiencing any such desire. Or suppose I feel an upsurge \n of spite against someone and destroy a message he has \n received so that he shall miss an appointment. If I describe \n this by saying' I wanted to make him miss that appointment', \n this does not necessarily mean that I had the thought ' If I \n do this, he will . . . ' and that it affected me with a desire \n of bringing that about which led up to my action. This may \n have happened, but need not. It could be that all that \n happened was this: I read the message, had the thought \n ' That unspeakable man ! ' with feelings of hatred, tore the \n message up, and laughed. Then if the question 'Why did \n you do that? ' is put by someone who makes it clear that \n he wants me to mention the mental causes-i.e., what went \n on in my mind and issued in the action-I should perhaps \n give this account; but normally the reply would be no such \n thing. That particular enquiry is not very often made. \n Nor do I wish to say that it always has an answer in cases \n where it can be made. One might shrug or say ' I don't \n know that there was any definite history of the kind you \n mean', or ' It merely occurred to me . . \n A 'mental cause', of course, need not be a mental \n event, i.e., a thought or feeling or image; it might be a \n knock on the door. But if it is not a mental event, it must \n be something perceived by the person affected-e.g. the \n knock on the door must be heard-so if in this sense anyone \n wishes to say it is always a mental event, I have no objection. \n A mental cause is what someone would describe if he were \n asked the specific question: what produced this action or \nThis content downloaded from 132.174.234.36 on Fri, 05 Sep 2025 17:48:40 UTC \nAll use subject to https://about.jstor.org/terms \n\n INTENTION 325 \n thought or feeling in you? i.e., what did you see or hear or \n feel, or what ideas or images cropped up in your mind, and \n led up to it? I have isolated this notion of a mental cause \n because there is such a thing as this question with this sort \n of answer, and because I want to distinguish it from the \n ordinary senses of ' motive ' and ' intention', rather than \n because it is in itself of very great importance; for I believe \n that it is of very little. But it is important to have a clear \n idea of it, partly because a very natural conception of \n motive ' is that it is what moves (the very word suggests \n that)-glossed as ' what causes' a man's actions etc. And \n ' what causes ' them is perhaps then thought of as an event \n that brings the effect about-though how-i.e. whether it \n should be thought of as a kind of pushing in another \n medium, or in some other way-is of course completely \n obscure. \n In philosophy a distinction has sometimes been drawn \n between ' motives' and 'intentions in acting' as referring \n to quite different things. A man's intention is what he \n aims at or chooses; his motive is what determines the aim \n or choice; and I suppose that ' determines ' must here be \n another word for ' causes '. \n Popularly, ' motive' and ' intention ' are not treated as \n so distinct in meaning. E.g. we hear of ' the motive of \n gain '; some philosophers have wanted to say that such an \n expression must be elliptical; gain must be the intention, and \n desire of gain the motive. Asked for a motive, a man might \n say' I wanted to . . . 'which would please such philosophers; \n or 'I did it in order to . . . ' which would not; and yet \n the meaning of the two phrases is here identical. When a \n man's motives are called good, this may be in no way distinct \n from calling his intentions good-e.g. ' he only wanted to \n make peace among his relations'. \n Nevertheless there is even popularly a distinction \n between the meaning of ' motive ' and the meaning of \n 'intention'. E.g. if a man kills someone, he may be said \n to have done it out of love and pity, or to have done it out \n of-hatred; these might indeed be cast in the forms "to release \nThis content downloaded from 132.174.234.36 on Fri, 05 Sep 2025 17:48:40 UTC \nAll use subject to https://about.jstor.org/terms \n\n 326 G. E. M. ANSCOMBE \n him from this awful suffering', or 'to get rid of the swine' \n but though these are forms of expression suggesting objectives, \n they are perhaps expressive of the spirit in which the man \n killed rather than descriptive of the end to which the killing \n was a means-a future state of affairs to be produced by the \n killing. And this shows us part of the distinction that there \n is between the popular senses of motive and intention. We \n should say: popularly, ' motive for an action' has a rather \n wider and more diverse application than ' intention with \n which the action was done '. \n When a man says what his motive was, speaking popu- \n larly, and in a sense in which ' motive 'is not interchangeable \n with 'intention', he is not giving a 'mental cause' in the \n sense that I have given to that phrase. The fact that the \n mental causes were such-and-such may indeed help to make \n his claim intelligible. And further, though he may say \n that his motive was this or that one straight off and without \n lying-i.e. without saying what he knows or even half knows \n to be untrue-yet a consideration of various things, which \n may include the mental causes, might possibly lead both \n him and other people to judge that his declaration of his \n own motive was false. But it appears to me that the mental \n causes are seldom more than a very trivial item among the \n things that it would be reasonable to consider. As for the \n importance of considering the motives of an action, as \n opposed to considering the intention, I am very glad not to \n be writing either ethics or literary criticism, to which this \n question belongs. \n Motives may explain actions to us; but that is not to say \n that they ' determine ', in the sense of causing, actions. We \n do say: ' His love of truth caused him to . . . ' and similar \n things, and no doubt such expressions help us to think that \n a motive must be what produces or brings about a choice. \n But this means rather ' He did this in that he loved the \n truth '; it interprets his action. \n Someone who sees the confusions involved in radically \n distinguishing between motives and intentions and in \n defining motives, so distinct, as the determinants of choice, \nThis content downloaded from 132.174.234.36 on Fri, 05 Sep 2025 17:48:40 UTC \nAll use subject to https://about.jstor.org/terms \n\n INTENTION 327 \n may easily be inclined to deny both that there is any such \n thing as mental causality, and that ' motive ' means anything \n but intention. But both of these inclinations are mistaken. \n We shall create confusion if we do not notice (a) that \n phenomena deserving the name of mental causality exist, \n for we can make the question 'Why?' into a request for \n the sort of answer that I considered under that head; \n (b) that mental causality is not restricted to choices or \n voluntary or intentional actions but is of wider application; \n it is restricted to the wider field of things the agent knows \n about not as an observer, so that it includes some involuntary \n actions; (c) that motives are not mental causes; and (d) that \n there is application for ' motive ' other than the applications \n of ' the intention with which a man acts '. \n Revenge and gratitude are motives; if I kill a man as an \n act of revenge I may say I do it in order to be revenged, \n or that revenge is my object; but revenge is not some further \n thing obtained by killing him, it is rather that killing him is \n revenge. Asked why I killed him, I reply 'Because he \n killed my brother.' We might compare this answer, which \n describes a concrete past event, to the answer describing a \n concrete future state of affairs which we sometimes get in \n statements of objectives. It is the same with gratitude, \n and remorse, and pity for something specific. These motives \n differ from, say, love or curiosity or despair in just this way: \n something that has happened (or is at present happening) is \n given as the ground of an action or abstention that is good \n or bad for the person (it may be oneself, as with remorse) at \n whom it is aimed. And if we wanted to explain e.g. revenge, \n we should say it was harming someone because he had done \n one some harm; we should not need to add some description \n of the feelings prompting the action or of the thoughts that \n had gone with it. Whereas saying that someone does \n something out of, say, friendship cannot be explained in any \n such way. I will call revenge and gratitude and remorse \n and pity backward-looking motives, and contrast them with \n motive-in-general. \n Motive-in-general is a very difficult topic which I do \nThis content downloaded from 132.174.234.36 on Fri, 05 Sep 2025 17:48:40 UTC \nAll use subject to https://about.jstor.org/terms \n\n 328 G. E. M. ANSCOMBE \n not want to discuss at any length. Consider the statement \n that one motive for my signing a petition was admiration \n for its promoter, X. Asked ' Why did you sign it? ' I \n might well say ' Well, for one thing, X, who is promoting it, \n did . . . ' and describe what he did in an admiring way. \n I might add ' Of course, I know that is not a ground for \n signing it, but I am sure it was one of the things that most \n influenced me '-which need not mean: ' I thought explicitly \n of this before signing.' I say ' Consider this' really with a \n view to saying ' let us not consider it here.' It is too \n complicated. The account of motive popularised by \n Professor Ryle does not appear satisfactory. He recommends \n construing ' he boasted from vanity' as saying ' he boasted \n * . . and his doing so satisfies the law-like proposition that \n whenever he finds a chance of securing the admiration and \n envy of others, he does whatever he thinks will produce this \n admiration and envy.'2 This passage is rather curious and \n roundabout in its way of putting what it seems to say, but \n I can't understand it unless it implies that a man could not \n be said to have boasted from vanity unless he always behaved \n vainly, or at least very often did so. But this does not seem \n to be true. \n To give a motive (of the sort I have labelled ' motive-in- \n general', as opposed to backward-looking motives and \n intentions) is to say something like ' See the action in this \n light.' To explain one's own actions by an account indica- \n ting a motive is to put them in a certain light. This sort of \n explanation is often elicited by the question 'Why? ' The \n question whether the light in which one so puts one's action \n is a true light is a notoriously difficult one. \n The motives admiration, curiosity, spite, friendship, fear, \n love of truth, despair and a host of others are either of this \n extremely complicated kind, or are forward-looking or \n mixed. I call a motive forward-looking if it is an intention. \n For example, to say that someone did something for fear \n 2 The Concept of Mind, p. 89. \nThis content downloaded from 132.174.234.36 on Fri, 05 Sep 2025 17:48:40 UTC \nAll use subject to https://about.jstor.org/terms \n\n INTENTION 329 \n of . . . often comes to the same as saying he did so lest \n or in order that . . . should not happen. \n Leaving then, the topic of motive-in-general or ' inter- \n pretative' motive, let us return to backward-looking \n motives. Why is it that in revenge and gratitude, pity and \n remorse, the past event (or present situation) is a reason \n for acting, not just a mental cause? \n Now the most striking thing about these four is the way \n in which good and evil are involved in them. E.g. if I am \n grateful to someone, it is because he has done me some \n good, or at least I think he has, and I cannot show gratitude \n by something that I intend to harm him. In remorse, I \n hate some good things for myself; I could not express remorse \n by getting myself plenty of enjoyments, or for something that \n I did not find bad. If I do something out of revenge which \n is in fact advantageous rather than harmful to my enemy, \n my action, in its description of being advantageous to him, \n is involuntary. \n These facts are the clue to our present problem. If an \n action has to be thought of by the agent as doing good or \n harm of some sort, and the thing in the past as good or bad, \n in order for the thing in the past to be the reason for the \n action, then this reason shows not a mental cause but a \n motive. This will come out in the agent's elaborations on \n his answer to the question ' Why?' \n It might seem that this is not the most important point, \n but that the important point is that a proposed action can be \n questioned and the answer be a mention of something past. \n I I am going to kill him.'-' Why? '-' He killed my father.' \n But do we yet know what a proposal to act is; other than a \n prediction which the predictor justifies, if he does justify it, \n by mentioning a reason for acting? and the meaning of the \n expression ' reason for acting ' is precisely what we are at \n present trying to elucidate. Might one not predict mental \n causes and their effects? Or even their effects after the \n causes have occurred? E.g. 'This is going to make me \n angry.' Here it may be worth while to remark that it is a \n mistake to think one cannot choose whether to act from a \n 2L \nThis content downloaded from 132.174.234.36 on Fri, 05 Sep 2025 17:48:40 UTC \nAll use subject to https://about.jstor.org/terms \n\n 330 G. E. M. ANSCOMBE \n motive. Plato saying to a slave ' I should beat you if I were \n not angry' would be a case. Or a man might have a \n policy of never making remarks about a certain person \n because he could not speak about that man unenviously \n or unadmiringly. \n We have now distinguished between a backward-looking \n motive and a mental cause, and found that here at any rate \n what the agent reports in answer to the question ' Why? ' is \n a reason-for-acting if, in treating it as a reason, he conceives \n it as something good or bad, and his own action as doing \n good or harm. If you could e.g. show that either the action \n for which he has revenged himself, or that in which he.has \n revenged himself, was quite harmless or beneficial, he ceases \n to offer a reason, except prefaced by ' I thought '. If \n it is a proposed revenge he either gives it up or changes his \n reasons. No such discovery would affect an assertion of \n mental causality. Whether in general good and harm \n play an essential part in the concept of intention is something \n it still remains to find out. So far good and harm have only \n been introduced as making a clear difference between a \n backward-looking motive and a mental cause. When the \n question ' Why? ' about a present action is answered by \n description of a future state of affairs, this is already \n distinguished from a mental cause just by being future. \n Here there does not so far seem to be any need to characterise \n intention as being essentially of good or of harm. \n Now, however, let us consider this case: \n Why did you do it? \n Because he told me to. \n Is this a cause or a reason.? It appears to depend very much \n on what the action was or what the circumstances were. \n And we should often refuse to make any distinction at all \n between something's being a reason and its being a cause \n of the kind in question; for that was explained as what one \n is after if one asks the agent what led up to and issued in an \n action, but being given a, reason and accepting it might be \nThis content downloaded from 132.174.234.36 on Fri, 05 Sep 2025 17:48:40 UTC \nAll use subject to https://about.jstor.org/terms \n\n INTENTION 331 \n such a thing. And how would one distinguish between cause \n and reason in such a case as having hung one's hat on a peg \n because one's host said ' Hang up your hat on that peg ' ? \n Nor, I think, would it be correct to say that this is a reason \n and not a mental cause because of the understanding of the \n words that went into obeying the suggestion. Here one \n would be attempting a contrast between this case and, say, \n turning round at hearing someone say Boo But this case \n would not in fact be decisively on one side or the other; \n forced to say whether the noise was a reason or a cause, \n one would probably decide by how sudden one's reaction \n was. Further, there is no question of understanding a \n sentence in the following case: 'Why did you waggle your \n two fore-fingers by your temples? '-' Becasue he was doing \n it; ' but this is not particularly different from hanging one's \n hat up because one's host said ' Hang your hat up.' \n Roughly speaking, if one were forced to go on with the \n distinction, the more the action is described as a mere \n response, the more inclined one would be to the word \n ' cause '; while the more it is described as a response to \n something as having a significance that is dwelt on by the \n agent, or as a response surrounded with thoughts and \n questions, the more inclined one would be to use the word \n reason'. But in very many cases the distinction would have \n no point. \n This, however, does not mean that it never has a point. \n The cases on which we first grounded the distinction might \n be called ' full-blown ': that is to say, the case of e.g. revenge \n on the one hand, and of the thing that made me jump and \n knock a cup off a table on the other. Roughly speaking, \n it establishes something as a reason to object to it, not as \n when one says 'Noises should not make you jump like that: \n hadn't you better see a doctor? ' but in such a way as to \n link it up with motives and intentions. 'You did it because \n he told you to? But why do what he says? ' Answers like \n ' he has done a lot for me '; ' he is my father '; ' it would \n have been the worse for me if I hadn't ' give the original \n answer a place among reasons. Thus the full-blown \nThis content downloaded from 132.174.234.36 on Fri, 05 Sep 2025 17:48:40 UTC \nAll use subject to https://about.jstor.org/terms \n\n 332 G. E. M. ANSCOMBE \n cases are the right ones to consider in order to see the distinc- \n tion between reason and cause. But it is worth noticing that \n what is so commonly said, that reason and cause are every- \n where sharply distinct notions, is not true. \nThis content downloaded from 132.174.234.36 on Fri, 05 Sep 2025 17:48:40 UTC \nAll use subject to https://about.jstor.org/terms 	 \n \nIntention\nAuthor(s): G. E. M. Anscombe\nSource: Proceedings of the Aristotelian Society, New Series, Vol. 57 (1956 - 1957), pp. 321-\n332\nPublished by: Oxford University Press on behalf of The Aristotelian Society\nStable URL: https://www.jstor.org/stable/4544583\nAccessed: 05-09-2025 17:48 UTC\n \nJSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide\nrange of content in a trusted digital archive. We use information technology and too	en	0.9	uploaded	4747	26467	2025-11-15 16:08:53.305488	2025-11-15 16:08:53.305493	\N	1	\N	\N	\N	1	original	\N	\N	\N	\N	\N	{}	6d474a1a-d532-4a7f-b910-e54b09d35f98
174	Intelligent Agents: Theory and Practice - Wooldridge & Jennings (1995)	file	reference	academic	pdf	Wooldridge and Jennings - 1995 - Intelligent agents theory and practice.pdf	uploads/eefeaa914cfd4b35_Wooldridge and Jennings - 1995 - Intelligent agents theory and practice.pdf	1012945	{"year": 1995, "discipline": "Artificial Intelligence", "upload_order": 3}	The Knowledge Engineering Review, Vol. 10:2, 1995, 115-152 \nIntelligent agents: theory and practice \nMICHAEL WOOLDRIDGE 1 and NICHOLAS R. JENNINGS 2 \n1 Deparrmenr of Compwing. Manche.,ter Metropolitan Univeni1y, Chester Street, Manches1er MI 5GD, UK \n(M. Wooldridge(ri)doc.mmu.ac.uk) \n2 nepartmmt of Electronic F.ngineering, Queen Mary & Westfield College, Mile End Road, London El 4NS, UK \n( N. R .JennmgI(0.1qm w. ac. uk) \nAbstract \nThe concept of an agent has become important in both artificial intelligence (AI) and mainstream \ncomputer science. Our aim in this paper is to point the reader at what we perceive to be the most \nimportant theoretical and practical issues associated with the design and construction of intelligent \nagents. For convenience, we divide these issues into three areas (though as the reader will see, the \ndivisions arc at times somewhat arbitrary). Agent theory is concerned with the question of what an \nagent is, and the use of mathematical formalisms for representing and reasoning about the \nproperties of agents. Agent architectures can he thought of as software engineering models of \nagents; researchers in this area are primarily concerned with the problem of designing software or \nhardware systems that will satisfy the properties specified by agent theorists. Finally, agent \nlanguages are software systems for programming and experimenting with agents; these languages \nmay embody principles proposed by theorists. The paper is not intended to serve as a tutorial \nintroduction to all the issues mentioned; we hope instead simply to identify the most important \nissues, and point to work that elaborates on them. The article includes a short review of current and \npotential applications of agent technology. \n1 Introduction \nWe begin our article with descriptions of three events that occur sometime in the future: \nI. The key air-traffic control system~ in the country of Ruritania suddenly fail, due to freak \nweather conditions. Fortunately, computerised air-traffic control systems in neighbouring \ncountries negotiate between themselves to track and deal with all affected flights, and the \npotentially disastrous situation passes without major incident. \n2. Upon logging in to your computer, you are presented with a list of email messages, sorted into \norder of importance by your personal digital assistant (PDA). You are then presented with a \nsimilar list of news articles; the assistant draws your attention to one particular article, which \ndescribes hitherto unknown work that is very close to your own. After an electronic discussion \nwith a number of other PD As, your PDA has already obtained a relevant technical report for \nyou from an FfP site, in the anticipation that it will be of interest. \n3. You are editing a file, when your PDA requests your attention: an email message has arrived, \nthat contaim notification about a paper you sent to an important conference, and the PDA \ncorrectly predicted that you would want to sec it as soon as possible. The paper has been \naccepted, and without prompting, the PDA begins to look into travel arrangements, by \nconsulting a number of databases and other networked information sources. A short time later, \nyou are pre~ented with a summary of the cheapest and most convenient travel options. \nWe shall not claim that computer systems of the sophistication indicated in these scenarios are just \naround the corner, hut serious academic research is underway into similar applications: air-traffic \n\nM. WOOLDRIDGE AND NICHOLAS JENNINGS 116 \ncontrol ha~ long been a research domain in distributed artificial intelligence (DAI) (Steeb et al., \n1988); various types of information manager, that filter and obtain information on behalf of their \nusers, have been prototyped (Maes, 1994a); and systems such as those that appear in the third \nscenario are discussed in (McGregor, 1992; Levy ct al., 1994). The key computer-based com\nponents that appear in each of the above scenarios are known as agents. It is interesting to note that \none way of defining Al is by saying that it is the subfield of computer science which aims to construct \nagents that exhibit aspects of intelligent behaviour. The notion of an "agent" is thus central to AI. It \nis perhaps surprising, therefore, that until the mid to late 1980s, researchers from mainstream AI \ngave relatively little consideration to the issues surrounding agent synthesis. Since then, however, \nthere has been an intense flowering of interest in the subject: agents are now widely discussed by \nresearchers in mainstream computer science, as well as those working in data communications and \nconcurrent systems research, robotics, and user interface design. A British national daily paper \nrecently predicted that: \n"Agent-hased computing (ABC) is likely to be the next significant breakthrough in software development. \n(Sargent, 1992) \nMoreover, the UK-based consultancy firm Ovum has predicted that the agent technology industry \nwould be worth some US$3.5 billion worldwide by the year 2000 (Houlder, 1994). Researchers \nfrom both industry and academia arc thus taking agent technology seriously: our aim in this paper is \nto survey what we perceive to be the most important issues in the design and construction of \nintelligent agents, of the type that might ultimate appear in applications such as those suggested by \nthe fictional scenarios ahove. We begin our article, in the following sub-section, with a discussion \non the subject of exactly what an agent is. \nI. I What is an agent? \nCarl Hewitt recently remarked 1 that the question what is an agent? is embarrassing for the agent\nbased computing community in just the same way that the question what is intelligence? is \nembarrassing for the mainstream AI community. The problem is that although the term is widely \nused, by many people working in closely related areas, it defies attempts to produce a single \nuniversally accepted definition. This need not necessarily be a problem: after all, if many people \nare successfully developing interesting and useful applications, then it hardly matters that they do \nnot agree on potentially trivial terminological details. However, there is also the danger that unless \nthe issue is discussed, "agent" might become a "noise'" term, subject to both abuse and misuse, to \nthe potential confusion of the research community. It is for this reason that we briefly consider the \nquestion. \nWe distinguish two general usages of the term "agent": the first is weak, and relatively \nuncontentious; the second is stronger, and potentially more contentious. \nI. I. I A Weak Notion of Agency \nPerhaps the most general way in which the term agent is used is to denote a hardware or (more \nusually) software-based computer system that enjoys the following properties: \n autonomy: agents operate without the direct intervention of humans or others, and have some \nkind of control over their actions and internal state (Castelfranchi, 1995); \n social ability: agents interact with other agents (and possibly humans) via some kind of agent\ncommunication language (Genesereth & Ketchpel, 1994); \n reactivity: agents perceive their environment (which may be the physical world, a user via a \ngraphical user interface, a collection of other agents, the Internet, or perhaps all of these \ncombined), and respond in a timely fashion to changes that occur in it; \n pro-activeness: agents do not simply act in response to their environment, they are able to exhibit \ngoal-directed behaviour by taking the initiative. \n1 At the Thirteenth International Workshop on Distributed Al. \n\nIntelligent agents: theory and practice 117 \nA simple way of conceptualising an agent is thus as a kind of UNIX*like software process, that \nexhibits the properties listed above. This weak notion of agency has found currency with a \nsurprisingly wide range of researchers. For example, in mainstream computer science, the notion \nof an agent as a self-contained, concurrently executing software process, that encapsulates some \nstate and is able to communicate with other agents via message passing, is seen as a natural \ndevelopment of the objcct*based concurrent programming paradigm (Agha, 1986; Agha ct al., \n1993). \nThis weak notion of agency is also that used in the emerging discipline of agent-based software \nengineering: \n"[Agents} communicate with their peers by exchanging messages in an expressive agent communication \nLanguage. While agents can be as simple as subroutines, typically they are larger entities with some sort of \npersistent control." (Gcncscrcth & Kctchpcl, 1994, p.48) \nA softbot (software robot) is a kind of agent: \n"A softbot i~ an agent that interacts with a software environment by issuing commands and interpreting the \nenvironments feedback. A softbot's effectors arc commands (e.g. Unix shell commands such as mv or \ncompress) meant 10 change the external environments state. A softbot's sensors are commands (e.g. pwd \nor ls in Unix) meant to provide . . information." (Etzioni et al., 1994, p.10) \n1.1.2 A stronger notion of agency \nFor some researchers-particularly those working in AI- the term "agent" has a stronger and \nmore specific meaning than that sketched out above. These researchers generally mean an agent to \nbe a computer system that, in addition to having the properties identified above, is either \nconceptualised or implemented using concepts that arc more usually applied to humans. For \nexample, it is quite common in AI to characterise an agent using mentalistic notions, such as \nknowledge, belief, intention, and obligation (Shoham, 1993). Some AI researchers have gone \nfurther, and considered emotional agents (Bates et al., 1992a; Bates, 1994). (Lest the reader \nsuppose that this is just pointless anthropomorphism, it should be noted that there are good \narguments in favour of designing and building agents in terms of human-like mental states-sec \nsection 2.) Another way of giving agents human-like attributes is to represent them visually, \nperhaps by using a cartoon-like graphical icon or an animated face (Maes, 1994a, p. 36)-for \nobvious reasons, such agents are of particular importance to those interested in human-computer \ninterfaces. \n1.1.3 Other attributes of agency \nVarious other attributes arc sometimes discussed in the context of agency. For example: \n mobility is the ability of an agent to move around an electronic network (White, 1994); \n veracity is the assumption that an agent will not knowingly communicate false information \n(Galliers, 1988b, pp. 159-164); \n benevolence is the assumption that agents do not have conflicting goals, and that every agent will \ntherefore always try to do what is asked of it (Rosenschein and Genesereth, 1985, p. 91); and \n rationality is (crudely) the assumption that an agent will act in order to achieve its goals, and will \nnot act in such a way as to prevent its goals being achieved-at least insofar as its beliefs permit \n(Galliers, 1988b, pp. 49-54). \n(A discussion of some of these notions is given below; various other attributes of agency are \nformally defined in (Goodwin, 1993).) \n1.2 The structure of this article \nNow that we have at least a preliminary understanding of what an agent is, we can embark on a \nmore detailed look at their properties, and how we might go about constructing them. For \n\nM. WOOLDRIDGE AND NICHOLAS JENNINGS 118 \nconvenience, we identify three key issues, and structure our survey around these (cf. Seel, 1989, \np.1 ), \n Agent theories are essentially specifications. Agent theorists address such questions as: How are \nwe to conceptualise agents? What properties should agents have, and how are we to formally \nrepresent and reason about these properties? \n Agent architectures represent the move from specification to implementation. Those working in \nthe area of agent architectures address such questions as: How are we to construct computer \nsystems that satisfy the properties specified by agent theorists? What software and/or hardware \nstructures are appropriate? What is an appropriated separation of concerns? \n Agent languages are programming languages that may embody the various principles proposed \nby theorists. Those working in the area of agent languages address such questions as: How are \nwe to program agents? What are the right primitives for this task? How are we to effectively \ncompile or execute agent programs? \nAs we pointed out above, the distinctions between these three areas are occasionally unclear. The \nissue of agent theories is discussed in the section 2. In section 3, we discuss architectures, and in \nsection 4, we discuss agent languages. A brief discussion of applications appears in section 5, and \nsome concluding remarks appear in section 6. Each of the three major sections closes with a \ndiscussion, in which we give a brief critical review of current work and open problems, and a section \npointing the reader to further relevant reading. \nFinally, some notes on the scope and aims of the article. First, it is important to realise that we \nare writing very much from the point of view of AI, and the material we have chosen to review \nclearly reflects this bias. Secondly, the article is not a intended as a review of Distributed AI, \nalthough the material we discuss arguably falls under this banner. We have deliberately avoided \ndiscussing what might be called the macro aspects of agent technology (i.e., those issues relating to \nthe agent society, rather than the individual (Gasser, 1991), as these issues are reviewed more \nthoroughly elsewhere (see Bond and Gasser, 1988, pp. 1-56, and Chaibdraa et al., 1992). Thirdly, \nwe wish to reiterate that agent technology is, at the time of writing, one of the most active areas of \nresearch in AI and computer science generally. Thus, work on agent theories, architectures, and \nlanguages is very much ongoing. In particular, many ofthe fundamental problems associated with \nagent technology can by no means be regarded as solved. This article therefore represents only a \nsnapshot of past and current work in the field, along with some tentative comments on open \nproblems and suggestions for future work areas. Our hope is that the article will introduce the \nreader to some of the different ways that agency is treated in D(AI), and in particular to current \nthinking on the theory and practice of such agents. \n2 Agent theories \nIn the preceding section, we gave an informal overview of the notion of agency. In this section, we \nturn our attention to the theory of such agents, and in particular, to formal theories. We regard an \nagent theory as a specification for an agent; agent theorists develop formalisms for representing the \nproperties of agents, and using these formalisms, try to develop theories that capture desirable \nproperties of agents. Our starting point is the notion of an agent as an entity 'which appears to be \nthe subject of beliefs, desires, etc.' (Seel, 1989, p. 1). The philosopher Dennett has coined the term \nintentional system to denote such systems. \n2. 1 Agents as intentional system~ \nWhen explaining human activity, it is often useful to make statements such as the following: \nJanine took her umbrella because she believed it was going to rain. \nMichael worked hard because he wanted to possess a PhD. \n\nIntelligent agents: theory and practice 119 \nThese statements make use of a folk psychology, by which human behaviour is predicted and \nexplained through the attribution of attitudes, such as believing and wanting (as in the above \nexamples), hoping, fearing and so on. This folk psychology is well established: most people reading \nthe above statements would say they found their meaning entirely clear, and would not give them a \nsecond glance. \nThe attitudes employed in such folk psychological descriptions are called the intentional notions. \nThe philosopher Daniel Dennett has coined the term intentional system to describe entities "whose \nbehaviour can be predicted by the method of attributing belief, desires and rational acumen" \n(Dennett, 1987, p. 49). Dennett identifies different "grades" of intentional system: \n"A first-order intentional system has beliefs and desires (etc.) but no beliefs and desires (and no doubt other \nintentional states) about beliefs and desires .... A second-order intentional system is more sophisticated; it \nhas beliefs and desires (and no doubt other intentional states) about beliefs and desires (and other \nintentional state~)-both those of others and its own" (Dennett, 1987, p. 243) \nOne can carry on this hierarchy of intentionality as far as required. \nAn obvious question is whether it is legitimate or useful to attribute beliefs, desires, and so on, to \nartificial agents. Isn't this just anthropomorphism? McCarthy, among others, has argued that there \nare occasions when the intentional stance is appropriate: \nTo ascribe beliefs, free will, intentions, consciousness, abilities, or wants to a machine is legitimate when \nsuch an ascription expresses the same information about the machine that it expresses about a person. It is \nuseful when the ascription helps us understand the structure of the machine, its past or future behaviour, or \nhow to repair or improve it. It is perhaps never logically required even for humans, but expressing \nreasonably briefly what is actually known about the state of the machine in a particular situation may \nrequire mental qualities or qualities isomorphic to them. Theories of belief, knowledge and wanting can be \nconstructed for machines in a simpler setting than for humans, and later applied to humans. Ascription of \nmental qualities is most straightforward for machines of known structure such as thermostats and computer \noperating systems, but is most useful when applied to entities whose structure is incompletely known." \n(McCarthy, 1978) (quoted in (Shoham, 1990)) \nWhat objects can be described by the intentional stance? As it turns out, more or less anything can. \nIn his doctoral thesis, Seel showed that even very simple, automata-like objects can be consistently \nascribed intentional descriptions (Seel 1989); similar work by Rosenschein and Kaelbling (albeit \nwith a different motivation), arrived at a similar conclusion (Rosenschein & Kaelbling, 1986). For \nexample, consider a light switch: \n"It is perfectly coherent to treat a light switch as a (very cooperative) agent with the capability of \ntran~mitting current at will, who invariably transmits current when it believes that we want it transmitted \nand not otherwise; flicking the switch is simply our way of communicating our desires.'' (Shoham, 1990, p. \n6) \nAnd yet most adults would find such a description absurd-perhaps even infantile. Why is this? \nThe answer seems to be that while the intentional stance description is perfectly consistent with the \nobserved behaviour of a light switch, and is internally consistent, \n.. it does not buy 11.S anything, since we essentially understand the mechanism sufficiently to have a \nsimpler, mechanistic description of its behaviour." (Shoham, 1990, p. 6) \nPut crudely, the more we know about a system, the less we need to rely on animistic, intentional \nexplanations of its behaviour. However, with very complex systems, even if a complete, accurate \npicture of the system's architecture and working is available, a mechanistic, design stance \nexplanation of its behaviour may not be practicable. Consider a computer. Although we might \nhave a complete technical description of a computer available, it is hardly practicable to appeal to \nsuch a description when explaining why a menu appears when we click a mouse on an icon. In such \nsituations, it may be more appropriate to adopt an intentional stance description, if that description \nis consistent, and simpler than the alternatives. The intentional notions are thus abstraction tools, \nwhich provide us with a convenient and familiar way of describing, explaining, and predicting the \nbehaviour of complex systems. \n\nM. WOOLDRIDGE AND NICHOLAS JE~NINGS 120 \nBeing an intentional system seems to be a necessary condition for agenthood. but is it a sufficient \ncondition? In his Master's thesis, Shardlow trawled through the literature of cognitive science and \nits component disciplines in an attempt to find a unifying concept that underlies the notion of \nagenthood. He was forced to the following conclusion: \n"Perhaps there is something more to an agent than its capacity for beliefs and desires, hut whatever that \nthing is, it admits no unified account within cognitive science." (Shardlow, 1990) \nSo, an agent is a system that is most conveniently described by the intentional stance; one whose \nsimplest consistent description requires the intentional stance. Before proceeding, it is worth \nconsidering exactly which attitudes are appropriate for representing agents. For the purposes of \nthis survey, the two most important categories are information attitudes and pro-attitudes: \ninformation attitudes {\nbelief \nknowledge \npro-attitudes I\ndes;ce \nintention \nobligation \nl\ncommitment \nchoice \nThus information attitudes are related to the information that an agent has about the world it \noccupies, whereas pro-attitudes are those that in some way guide the agent's actions. Precisely \nwhich combination of attitudes is most appropriate to characterise an agent is, as we shall sec later, \nan issue of some debate. However, it seems reasonable to suggest that an agent must be \nrepresented in terms of at least one information attitude, and at least one pro-attitude. Note that \npro- and information attitudes are closely !inked, as a rational agent will make choices and form \nintentions, etc., on the basis of the information it has about the world. Much work in agent theory is \nconcerned with sorting out exactly what the relationship between the different attitudes is. \nThe next step is to investigate methods for representing and reasoning about intentional \nnotions. \n2.2 Representing intentional notions \nSuppose one wishes to reason about intentional notions in a logical framework. Consider the \nfollowing statement (after Genesereth & Nilsson, 1987, pp. 210-211): \nJanine believes Cronos is the father of Zeus. (1) \nA naive attempt to translate (1) into first-order logic might result in the following: \nBel(Janine, Father(Zeus, Cronos)) (2) \nUnfortunately, this naive translation does not work, for two reasons. The first is syntactic: the \nsecond argument to the Bel predicate is a formula of first-order logic, and is not, therefore, a term. \nSo (2) is not a well-formed formula of classical first-order logic. The second problem is semantic, \nand is potentially more serious. The constants Zeus and Jupiter, by any reasonable interpretation, \ndenote the same individual: the supreme deity of the classical world. It is therefore acceptable to \nwrite, in first-order logic: \n(Zeus= Jupiter). (3) \nGiven (2) and (3), the standard rules of first-order logic would allow the derivation of the following: \nBel(Janine, Father(Jupiter, Cronos)) (4) \nBut intuition rejects this derivation as invalid: believing that the father of Zeus is Cronos is not the \nsame as believing that the father of Jupiter is Cronos. So what is the problem? Why does first-order \n\nIntelligent agents: theory and practice 121 \nlogic fail here? The problem is that the intentional notions-such as belief and desire-are \nreferentially opaque, in that they set up opaque contexts, in which the standard substitution rules of \nfirst-order logic do not apply. In classical (propositional or first-order) logic, the denotation, or \nsemantic value, of an expression is dependent solely on the denotations of its sub-expressions. For \nexample, the denotation of the propositional logic formulap /\\ q is a function of the truth-values of \np and q. The operators of classical logic are thus said to be truth functional. In contrast, intentional \nnotions such as belief are not truth functional. It is surely not the case that the truth value of the \nsentence: \nJanine believes p (5) \nis dependent solely on the truth value of p 2 So substituting equivalents into opaque contexts is not \ngoing to preserve meaning. This is what is meant by referential opacity. Clearly, classical logics are \nnot suitable in their standard form for reasoning about intentional notions: alternative formalisms \nare required. \nThe number of basic techniques used for alternative formalisms is quite small. Recall, from the \ndiscussion above, that there arc two problems to be addressed in developing a logical formalism for \nintentional notions: a syntatic one, and a semantic one. It follows that any formalism can be \ncharacterised in terms of two independent attributes: its language of formulation, and semantic \nmodel (Konolige, 1986a, p. 83). \nThere are two fundamental approaches to the syntactic problem. The first is to use a modal \nlanguage, which contains non-truth-functional modal operators, which arc applied to formulae. An \nalternative approach involves the use of a meta-language: a many-sorted first-order language \ncontaining terms that denote formulae of some other object-language. Intentional notions can be \nrepresented using a meta-language predicate, and given whatever axiomatisation is deemed \nappropriate. Both of these approaches have their advantages and disadavantagcs, and will be \ndiscussed in the sequel. \nAs with the syntactic problem, there arc two basic approaches to the semantic problem. The \nfirst, best-known, and probably most widely used approach is to adopt a possible worlds semantics, \nwhere an agent's beliefs, knowledge, goals, and so on, arc characterised as a set of so-called \npossible worlds, with and accessibility relation holding between them. Possible worlds semantics \nhave an associated correspondence theory which makes them an attractive mathematical tool to \nwork with (Chellas, 1980). However, they also have many associated difficulties, notably the well\nknown logical omniscience problem, which implies that agents are perfect reasoners (we discuss \nthis problem in more detail below). A number of variations on the possible-worlds theme have \nbeen proposed, in an attempt to retain the correspondence theory, but without logical omnis\ncience. The commonest alternative to the possible worlds model for belief is to use a sentential, or \ninterpreted symbolic structures approach. In this scheme, beliefs are viewed as symbolic formulae \nexplicitly represented in a data structure associated with an agent. An agent then believes q) if i:p is \npresent in its belief data structure. Despite its simplicity, the sentential model works well under \ncertain circumstances (Konolige, 1986a). \nIn the subsections that follow, we discuss various approaches in some more detail. We begin \nwith a close look at the basic possible world:-, model for logics of knowledge (episremic logics) and \nlogics of belief (doxastic logics). \n2.3 Possible worlds semantics \nThe possible worlds model for logics of knowledge and belief was originally proposed by Hintikka \n(1962), and is now most commonly formulated in a normal modal logic using the techniques \n2 Note, however, that the sentence (5) is itself a proposition, in that its denotation is the value true or false. \n\nM. WOOLDRIDGE AND NICHOLAS JENNINGS 122 \ndeveloped by Kripke (1963). 3 Hintikka's insight was to see that an agent's beliefs could be \ncharacterised as a set of possible worlds, in the following way. Consider an agent playing a card \ngame such as poker. 4 In this game, the more one knows about the cards possessed by one's \nopponents, the better one is able to play. And yet complete knowledge of an opponent's cards is \ngenerally impossible (if one excludes cheating). The ability to play poker well thus depends, at least \nin part, on the ability to deduce what cards are held by an opponent, given the limited information \navailable. Now suppose our agent possessed the ace of spades. Assuming the agent's sensory \nequipment was functioning normally, it would be rational of her to believe that she possessed this \ncard. Now suppose she were to try to deduce what cards were held by her opponents. This could be \ndone by first calculating all the various different ways that the cards in the pack could possibly have \nbeen distributed among the various players. (This is not being proposed as an actual card playing \nstrategy, but for illustration!) For argument's sake, suppose that each possible configuration is \ndescribed on a separate piece of paper. Once the process is complete, our agent can then begin to \nsystematically eliminate from this large pile of paper all those configurations which are not possible, \ngiven what she knows. For example, any configuration in which she did not possess the ace of spades \ncould be rejected immediately as impossible. Call each piece of paper remaining after this process a \nworld. Each world represents one state of affairs considered possible, given what she knows. \nHintikka coined the term epistemic alternatives to describe the worlds possible given one's beliefs. \nSomething true in all our agent"s epistemic alternatives could be said to be believed by the agent. \nFor example, it will be true in all our agent's epistemic alternatives that she has the ace of spades. \nOn a first reading, this seems a peculiarly roundabout way of characterising belief. but it has two \nadvantages. First, it remains neutral on the subject of the cognitive structure of agents. It certainly \ndoesn't posit any internalised collection of possible worlds. It is just a convenient way of \ncharacterising belief. Second, the mathematical theory associated with the formalisation of \npossible worlds is extremely appealing (see below). \nThe next step is to show how possible worlds may be incorporated into the semantic framework \nof a logic. Epistemic logics arc usually formulated as normal modal logics using the semantics \ndeveloped by Kripke (1963). Before moving on to explicitly epistemic logics, we consider a simple \nnormal modal logic. This logic is essentially classical propositional logic, extended by the addition \nof two operators:' " (necessarily), and"" (possibly). Let Prop= {p, q, .. . } be a countable set \nof atomic propositions. Then the syntax of the logic is defined by the following rules: (i) if p e Prop \nthen pis a formula; (ii) if rp, 1.jJ are formulae, then so are -,rp and i:p V 1.jJ; and (iii) if rp i~ a formula \nthen so arc qi and ff. The operators "-," (not) and "V" ( or) have their standard meanings. The \nremaining connectives of classical propositional logic can be defined as abbreviations in the usual \nway. The formula q, is read: "necessarily cp" and the formula ff is read: "possibly cp". The \nsemantics of the modal connectives arc given by introducing an accessibility relation into models for \nthe language. This relation defines what worlds are considered accessible from every other world. \nThe formula qi is then true if q; is true in every world accessible from the current world; rp is true \nif rp is true in at least one world accessible from the current world. The two modal operators are \nduals of each other, in the sense that the universal and existential quantifiers of first-order logic arc \nduals: \nIt would thus have been possible to take either one as primitive, and introduce the other as a \nderived operator. The two basic properties of this logic arc as follows. First, the following axiom \nschema is valid: (q-, = 1.jJ) = (Oq, -=O1.JJ). This axiom is called K, in honour of Kripkc. The second \nproperty is as follows: if <pis valid, then q; is valid. Now, since K is valid, it will be a theorem of any \n3 In Hintikka's original work. he used a technique based on "model sets. which is equivalent to Kripke"s \nformalism, though less elegant. See Hughes and Cresswell (1968, pp. 351-352) for a compari~on and \ndiscussion of the two techniques. \n4 This example was adapted from Halpern (1987). \n\nIntelligent agents: theory and practice 123 \ncomplete axiomatisation of normal modal logic. Similarly, the second property will appear as a rule \nof inference in any axiomisation of normal modal logic; it is generally called the necessitation rule. \nThese two properties turn out to be the most problematic features of normal modal logics when \nthey are used as logics of knowledge/belief (this point will be examined later). \nThe most intriguing properties of normal modal logics follow from the properties of the \naccessibility relation, R, in models. To illustrate these properties, consider the following axiom \nschema: <p =- <p. It turns out that this axiom is characteristic of the class of models with a reflexive \naccessibility relation. (By characteristic, we mean that it is true in all and only those models in the \nclass.) There are a host of axioms which correspond to certain properties of R: the study of the way \nthat properties of R correspond to axioms is called correspondence theory. For our present \npurposes, we identify just four axioms: the axiom called T (which corresponds to a reflexive \naccessibility relation); D (serial accessibility relation); 4 (transitive accessibility relation); and 5 \n(euclidean accessibility relation): \nT q; => q; D ,p ~ 'P \n4 q; => q; 5 'P ~ 'P-\nThe results of correspondence theory make it straightforward to derive completeness results for a \nrange of simple normal modal logics. These results provide a useful point of comparison for normal \nmodal logics, and account in a large part for the popularity of this style of semantics. \nTo use the logic developed above as an epistemic logic, the formula q: is read as: "it is known \nthat rp". The worlds in the model are interpreted as epistemic alternatives, the accessibility relation \ndefines what the alternatives arc from any given world. \nThe logic defined above deals with the knowledge of a single agent. To deal with multi-agent \nknowledge, one adds to a model structure an indexed set of accessibility rehitions, one for each \nagent. The language is then extended by replacing the single modal operator "O" by an indexed set \nof unary modal operators { K 1 }, where i E { 1, ... , n }. The formula K;r:r is read: 'i knows that cp". \nEach operator K, is given exactly the same properties as' ". \nThe next step is to consider how well normal modal logic serves as a logic of knowledge/belief. \nConsider first the necessitation rule and axiom K, since any normal modal system is committed to \nthese. The necessitation rule tells us that an agent knows all valid formulae. Amongst other things, \nthis means an agent knows all propositional tautologies. Since there is an infinite number of these, \nan agent will have an infinite number of items of knowledge: immediately, one is faced with a \ncounter-intuitive property of the knowledge operator. Now consider the axiom K, which says that \nan agent's knowledge is closed under implication. Together with the necessitation rule, this axiom \nimplies that an agent's knowledge is closed under logical consequence: ~n agent believes all the \nlogical consequences of its beliefs. This also seems counter intuitive. For example, suppose, like \nevery good logician, our agent knows Pcano's axioms. Now Fermat's last theorem follows from \nPean o's axioms-but it took the combined efforts of some of the best minds over the past century to \nprove it. Yet if our agent's beliefs are closed under logical consequence, then our agent must know \nit. So consequential closure, implied by necessitation and the K axiom, seems an overstrong \nproperty for resource bounded reasoners. \nThese two problems-that of knowing all valid formulae, and that of knowledge/belief being \nclosed under logical consequence-together constitute the famous logical omniscience problem. It \nhas been widely argued that this problem makes the possible worlds model unsuitable for \nrepresenting resource bounded believers-and any real system is resource bounded. \n2.3.1 Axioms for knowledge and belief \nWe now tonsider the appropriateness of the axioms D. T, 4, and 5 for logics of knowledge/ \nbelief. The axiom D says that an agent's beliefs are non-contradictory; it can be re-written as: \nK,cp => -.K, ..,<p, which is read: 'if i knows rp, then i doesn't know -irp'". This axiom seems a \nreasonable property of knowledge/belief. The axiom Tis often called the knowledge axiom, since it \nsays that what is known is true. It is usually accepted as the axiom that distinguishes knowledge \n\nM. WOOLDRIDGE AND NICHOLAS JENNINGS 124 \nfrom belief: it seems reasonable that one could believe something that is false, but one would \nhesitate to say that one could know something false. Knowledge is thus often defined as true belief; \ni knows cp if i believes <p and <pis true. So defined, knowledge satisfies T. Axiom 4 is called the \npositive introspection axiom. Introspection is the process of examining one's own beliefs, and is \ndiscussed in detail in (Konolige, 1986a, Chapter 5). The positive introspection axiom says that an \nagent is aware of what it knows. Similarly, axiom 5 is the negative introspective axiom, which says \nthat an agent is aware of what it doesn't know. Positive and negative introspection together imply \nan agent has perfect knowledge about what it does and doesn't know (cf. (Konolige, 1986a, \nEquation (5.11), p. 79)). Whether or not the two types of introspection are appropriate properties \nfor knowledge/belief is-the subject of sonie debate. However, it"is g~nerally accepted that positive \nintrospection is a less demanding property than negative introspection, and is thus a more \nreasonable property for resource bounded reasoners. \nGiven the comments above, the axioms KTD45 are often chosen as a logic of (idealised) \nknowledge, and KD45 as a logic of (idealised) belief. \n2.4 Alternatives to the possible worlds model \nAs a result of the difficulties with logical omniscience, many researchers have attempted to develop \nalternative formalisms for representing belief. Some of these are attempts to adapt the basic \npossible worlds model; others represent significant departures from it. In the subsections that \nfollow, we examine some of these attempts. \n2.4.1 Levesque-belief and awareness \nIn a 1984 paper, Levesque proposed a solution to the logical omniscience problem that involves \nmaking a distinction between explicit and implicit belief (Levesque, 1984). Crudely, the idea is that \nan agent has a relatively small set of explicit beliefs, and a very much larger (infinite) set of implicit \nbeliefs, which includes the logical consequences of the explicit beliefs. To fonnalise this idea, \nLevesque developed a logic with two operators; one each for implicit and explicit belief. The \nsemantics of the explicit belief operator were given in terms of a weakened possible worlds \nsemantics, by borrowing some ideas from situation semantics (Barwise & Perry, 1983; Devlin, \n1991). The semantics of the implicit belief operator were given in terms of a standard possible \nworlds approach. A number of objections have been raised to Levesque's model (Reichgelt, 1989b. \np. 135): first, it does not allow quantification-this drawback has been rectified by Lakemeycr \n(1991); second, it docs not seem to allow for nested beliefs; third, the notion of a situation, which \nunderlies Levesque's logic is, if anything, more mysterious than the notion of a world in possible \nworlds; and fourth, under certain circumstances, Levesque's proposal still makes unrealistic \npredictions about agent's reasoning capabilities. \nIn an effort to recover from this last negative result, Fagin and Halpern have developed a "logic \nof general awareness" based on a similar idea to Levesque's but with a very much simpler semantics \n(Fagin & Hapern, 1985). However, this proposal has itself been criticised by some (Konolige, \n1986b). \n2.4.2 Konolige-the deduction model \nA more radical approach to modelling resource bounded believers was proposed by Konolige \n(Konolige, 1986a). His deduction model of belief is, in essence, a direct attempt to model the \n"beliefs" of :.ymbolic Al systems. Konolige observed that a typical knowledge-based system has \ntwo key components: a database of symbolically represented "beliefs" (which may take the form of \nrules. frames, semantic nets, or, more generally, formulae in some logical language), and some \nlogically incomplete inference mechanism. Konolige modelled such systems in terms of deduction \nstructures. A deduction structure is a pair d = (ii, p), where ~ is a base set of formulae in some \nlogical language, and pis a set of inference rules (which may be logically incomplete), representing \nthe agent's reasoning mechanism. To simplify the formalism, Konolige assumed that an agent \n\nIntelligent agents: theory and practice 125 \nwould apply its inference rules wherever possible, in order to generate the deductive closure of its \nbase beliefs under its deduction rules. We model deductive closure in a function close: \nwhere 6.1--,, rp means that rpcan be proved from 6. using only the rules in p. A belief logic can then be \ndefined, with the semantics to a modal belief connective [i], where i is an agent, given in terms of the \ndeduction structured; modelling i's belief system: [i]qi iff rp e c!ose(d;). \nKonolige went on to examine the properties of the deduction model at some length, and \ndeveloped a variety of proof methods for his logics, including resolution and tableau systems \n(Geissler & Konolige, 1986). The deduction model is undoubtedly simple; however, as a direct \nmodel of the belief systems of AI agents, it has much to commend it. \n2.4.3 Meta-languages and syntactic modalities \nA meta-language is one in which it is possible to represent the properties of another language. A \nfirst-order meta-language is a first-order logic, with the standard predicates, quantifier, terms, and \nso on, whose domain contains formulae of some other language, called the object language. Using a \nmeta-language, it is possible to represent a relationship between a meta-language term denoting an \nagent, and an object language term denoting some formula. For example, the meta-language \nformula Bel(Janine,[Father(Zeus, Cronos)]) might be used to represent the example (1) that we \nsaw earlier. The quote marks, [ ... ], are used to indicate that their contents are a meta-language \nterm denoting the corresponding object-language formula. \nUnfortunately, meta-language formalisms have their own package of problems, not the least of \nwhich is that they tend to fall prey to inconsistency (Montague, 1963; Thomason, 1980). However, \nthere have been some fairly successful meta-language formalisms, including those by Konolige \n(1982), Haas (1986), Morgenstern (1987), and Davies (1993). Some results on retrieving consist\nency appeared in the late 1980s (Pcrlis, 1985, 1988; des Rivieres & Levesque, 1986; Turner, 1990). \n2.5 Pro-attitudes: goals and desires \nAn obvious approach to developing a logic of goals or desires is to adapt possible worlds \nsemantics-sec, e.g .. Cohen and Levesque (1990a), Wooldridge (1994). In this view, each goal\naccessible world represents one way the world might be if the agent's goals were realised. However, \nthis approach falls prey to the side effect problem, in that it predicts that agents have a goal of the \nlogical consequences of their goals (cf. the logical omniscience problem, discussed above). This is \nnot a desirable property: one might have a goal of going to the dentist, with the necessary \nconsequence of suffering pain, without having a goal of suffering pain. The problem is discussed (in \nthe context of intentions), in Bratman ( 1990). The basic possible worlds model has been adapted by \nsome researchers in an attempt to overcome this problem (Wainer, 1994). Other, related semantics \nfor goals have been proposed (Doyle et al., 1991; Kiss & Reichgelt, 1992; Rao& Georgeff, 1991b). \n2.6 Theories of agency \nAll of the formalisms considered so far have focused on just one aspect of agency. However, it is to \nbe expected that a realistic agent theory will be represented in a logical framework that combines \nthese various components. Additionally, we expect an agent logic to be capable of representing the \ndynamic aspects of agency. A complete agent theory, expressed in a logic with these properties, \nmust define how the attributes of agency are related. For example, it will need to show how an \nagent's information and pro-attitudes are related; how an agent's cognitive state changes over time; \nhow the environment affects an agent's cognitive state; and how an agent's information and pro\nattitudes lead it to perform actions. Giving a good account of these relationships is the most \nsignificant problem faced by agent theorists. \n\nM. WOOLDRIDGE AND NICHOLAS JENNINGS 126 \nAn all-embracing agent theory is some time off, and yet signficant steps have been taken towards \nit. In the following subsections, we briefly review some of this work. \n2.6. I Moore-knowledge and action \nMoore was in many ways a pioneer of the use of logics for capturing aspects of agency (Moore, \n1990). His main concern was the study of knowledge pre-conditions for actions-the question of \nwhat an agent needs to know in order to be able to perform some action. He formalised a model of \nability in a logic containing a modality for knowledge, and a dynamic logic-like apparatus for \nmodelling action (cf. Hare!, 1984). This formalism allowed for the possibility of an agent having \nincomplete information about how to achieve some goal, and performing actions in order to find \nout how to achieve it. Critiques of the formalism (and attempts to improve on it) may be found in \nMorgenstern (1987) and Lesperance (1989). \n2.6.2 Cohen and Levesque-intention \nOne of the best-known and most influential contributions to the area of agent theory is due to \nCohen and Levesque (1990a). Their formalism was originally used to develop a theory of intention \n(as in "I intend to ... "), which the authors required as a pre-requisite for a theory of speech acts \n(Cohen & Levesque, 1990b). However, the logic has subsequently proved to be so useful for \nreasoning about agents that it has been used in an analysis of conflict and cooperation in multi\nagent dialogue (Galliers, 1988a,b), as well as several studies in the theoretical foundations of \ncooperative problem solving (Levesque ct al., 1990; Jennings, 1992; Castelfranchi, 1990; Castel\nfranchi et al., 1992). Here, we shall review its use in developing a theory of intention. \nFollowing Bratman (1990), Cohen and Levesque identify seven properties that must be satisfied \nby a reasonable theory of intention: \n1. Intentions pose problems for agents, who need to determine ways of achieving them. \n2. Intentions provide a "filter" for adopting other intentions, which must not conflict. \n3. Agents track the success of their intentions, and arc inclined to try again if their attempts fail. \n4. Agents believe their intentions are possible. \n5. Agents do not believe they will not bring about their intentions. \n6. Under certain circumstances, agents believe they will bring about their intentions. \n7. Agents need not intend ail the expected side effects of their intentions. \nGiven these criteria, Cohen and Levesque adopt a two-tiered approach to the problem of \nformalising intention. First, they construct a logic of rational agency, "being careful to sort out the \nrelationships among the basic modal operators" (Cohen & Levesque, 1990a, p. 221). Over this \nframework, they introduce a number of derived constructs, which constitute a "partial theory of \nrational action" (Cohen & Levesque, 1990a, p. 221); intention is one of these constructs. \nThe first major derived construct is the persistent goal. An agent has a persistent goal of rp iff: \nl. It has a goal that q; eventually becomes true, and believes that rp is not currently true. \n2. Before it drops the goal cp, one of the following conditions must hold: i the agent believes cp has \nbeen satisfied; or ii the agent believes cp will never be satisfied. \nIt is a small step from persistent goals to a first definition of intention, as in 'intending to act'': an \nagent intends to do action a iff it has a persistent goal to have brought about a state wherein it \nbelieved it was about to do (1, and then did a. Cohen and Levesque go on to show how such a \ndefinition meets manyofBratman'scritcria for a theory of intention (outlined above). A critique of \nCohen and Levesque's theory of intention may be found in Singh (1992). \n2.6.3 Rao and Georgeff-belief, desire, intention architectures \nAs we observed earlier, there is no clear consensus in either the Al or philosophy communities \nabout precisely which combination of information and pro-attitudes are best suited to characteris\ning rational agents. In the work of Cohen and Levesque, described above, just two basic attitudes \n\nIntelligent agents: theory and practice 127 \nwere used: beliefs and goals. Further attitudes, such as intention, were defined in terms of these. In \nrelated work, Rao and Georgeff have developed a logical framework for agent theory based on \nthree primitive modalities: beliefs, desires and intentions (Rao & Georgeff, 1991a,b, 1993). Their \nformalism is based on a branching model of time (cf. Emerson & Halpern, 1986), in which belief-, \ndesire- and intention-accessible worlds are themselves branching time structures. \nThey are particularly concerned with the notion of realism-the question of how an agent's \nbeliefs about the future affect its desires and intentions. In other work, they also consider the \npotential for adding (social) plans to their formalism (Rao & Georgcff, 1992b; Kinny et al., 1992). \n2.6.4 Singh \nA quite different approach to modelling agents was taken by Singh, who has developed an \ninteresting family of logics for representing intentions, beliefs, knowledge, know-how, and \ncommunication in a branching-time framework (Singh, 1990, 199la,b; Singh & Asher, 1991); these \narticles are collected and expanded in Singh (1994). Singh's formalism is extremely rich, and \nconsiderable effort has been devoted to establishing its properties. However, its complexity \nprevents a detailed discussion here. \n2.6.5 Werner \nIn an extensive sequence of papers, Werner has laid the foundations of a general model of agency, \nwhich draws upon work in economics, game theory, situated automata theory, situation semantics, \nand philosophy (Werner, 1988, 1989, 1990, 1991). At the time of writing, however, the properties \nof this model have not been investigated in depth. \n2.6.6 Wooldridge-modelling multi-agent systems \nFor his 1992 doctoral thesis, Wooldridge developed a family of logics for representing the \nproperties of multi-agent systems (Wooldridge, 1992; Wooldridge & Fisher, 1992). Unlike the \napproaches cited above, Wooldridge's aim was not to develop a general framework for agent \ntheory. Rather, he hoped to construct formalisms that might be used in the specification and \nverification of realistic multi-agent systems. To this end, he developed a simple, and in some sense \ngeneral, model of multi-agent systems, and showed how the histories traced out in the execution of \nsuch a system could be used as the semantic foundation for a family of both linear and branching \ntime temporal belief logics. He then gave examples of how these logics could be used in the \nspecification and verification of protocols for cooperative action. \n2. 7 Communication \nFormalisms for representing communication in agent theory have tended to be based on speech act \ntheory, as originated by Austin (1962), and further developed by Searle (1969) and others (Cohen \n& Perrault, 1979; Cohen & Levesque, 1990a). Briefly, the key axiom of speech act theory is that \ncommunicative utterances arc actions, in just the sense that physical actions arc. They are \nperformed by a speaker with the intention of bringing about a desired change in the world: \ntypically, the speaker intends to bring about some particular mental state in a listener. Speech acts \nmay fail in the same way that physical actions may fail: a listener generally has control over her \nmental state, and cannot be guaranteed to react in the way that the speaker intends, Much work in \nspeech act theory has been devoted to classifying the various different types of speech acts. Perhaps \nthe two most widely recognised categories of speech acts are representatives ( of which informing is \nthe paradigm example), and directives (of which requesting is the paradigm example). \nAlthough not directly based on work in speech acts (and arguably more to do with architectures \nthan theories), we shall here mention work on agent communication languages (Genesereth & \nKetchpel, 1994). The best known work on agent communication languages is that by the ARPA \nknowledge sharing effort (Patil et al,, 1992). This work has been largely devoted to developing two \nrelated languages: the knowledge query and manipulation language (KQML) and the knowledge \n\nM. WOOLDRIDGE AND NICHOLAS JENNINGS 128 \ninterchange format (KIF). KOML provides the agent designer with a standard syntax for \nmessages, and a number of performatives that define the force of a message. Example performa* \ntives include tell, perform, and reply; the inspiration for these message types comes largely from \nspeech act theory. KIF provides a syntax for message content-KIF is essentially the first-order \npredicate calculus, recast in a LISP-like syntax. \n2.8 Discussion \nFormalisms for reasoning about agents have come a long way since Hintikka's pioneering work on \nlogics of knowledge and belief (Hintikka, 1962). Within AI, perhaps the main emphasis of \nsubsequent work has been on attempting to develop formalisms that capture the relationship \nbetween the various elements that comprise an agent's cognitive state; the paradigm example of \nthis work is the well-known theory of intention developed by Cohen and Levesque (1990a). \nDespite the very real progress that has been made, there still remain many fairly fundamental \nproblems and issues still outstanding. \nOn a technical level, we can identify a number of issues that remain open. First, the problems \nassociated with possible worlds semantics (notably, logical omniscience) cannot be regarded as \nsolved. As we observed above, possible worlds remain the semantics of choice for many \nresearchers, and yet they do not in general represent a realistic model of agents with limited \nresources-and of course all real agents are resource-bounded. One solution is to ground possible \nworlds semantics, giving them a precise interpretation in terms of the world. This was the approach \ntaken in Rosenschein and Kaelbling's situated automata paradigm, and can be very successful. \nHowever, it is not clear how such a grounding could be given to proattitudes such as desires or \nintentions (although some attempts have been made (Singh, 1990a; Wooldridge, 1992; Werner, \n1990)). There is obviously much work remaining to be done on formalisms for knowledge and \nbelief, in particular in the area of modelling resource bounded reasoners. \nWith respect to logics that combine different attitudes, perhaps the most important problems \nstill outstanding relate to intention. In particular, the relationship between intention and action has \nnot been formally represented in a satisfactory way The problem seems to be that having an \nintention to act makes it more likely that an agent will act, but does not generally guarantee it. \nWhile it seems straightforward to build systems that appear to have intentions (Wooldridge, 1995), \nit seems much harder to capture this relationship formally. Other problems that have not yet really \nbeen addressed in the literature include the management of multiple, possibly conflicting \nintentions, and the formation, scheduling, and reconsideration of intentions. \nThe question of exactly which combination of attitudes is required to characterise an agent is \nalso the subject of some debate. As we observed above, a currently popular approach is to use a \ncombination of beliefs, desires, and intentions (hence BDI architectures (Rao and Georgeff, \n199lb)). However, there are alternatives: Shoham, for example, suggests that the notion of choice \nis more fundamental (Shoham, 1990). Comparatively little work has yet been done on formally \ncomparing the suitability of these various combinations. One might draw a parallel with the use of \ntemporal logics in mainstream computer science, where the expressiveness of specification \nlanguages is by now a well-understood research area (Emerson & Halpern, 1986). Perhaps the \nobvious requirement for the short term is experimentation with real agent specifications, in order \nto gain a better understanding of the relative merits of different formalisms. \nMore general!y, the kinds of logics used in agent theory tend to be rather elaborate, typically \ncontaining many modalities which interact with each other in subtle ways. Very little work has yet \nbeen carried out on the theory underlying such logics (perhaps the only notable exception is \nCatach, 1988). Until the general principles and limitations of such multi-modal logics become \nunderstood, we might expect that progress with using such logics will be slow. One area in which \nwork is likely to be done in the near future is theorem proving techniques for multi-modal logics. \nFinally, there is often some confusion about the role played by a theory of agency. The view we \ntake is that such theories represent specifications for agents. The advantage of treating agent \n\nIntelligent agents: theory and practice 129 \ntheories as specifications, and agent logics as specification languages, is that the problems and \nissues we then face are familiar from the discipline of software engineering: How useful or \nexpressive is the specification language? How concise are agent specifications? How does one \nrefine or otherwise transform a specification into an implementation? However, the view of agent \ntheories as specifications is not shared by all researchers. Some intend their agent theories to be \nused as knowledge representation formalisms, which raises the difficult problem of algorithms to \nreason with such theories. Still others intend their work to formalise a concept of interest in \ncognitive science or philosophy (this is, of course, what Hintikka intended in his early work on \nlogics of knowledge of belief). What is clear is that it is important to be precise about the role one \nexpects an agent theory to play. \n2. 9 Further reading \nFor a recent discussion on the role of logic and agency, which lays out in more detail some \ncontrasting views on the subject, see Israel (1993, pp. 17-24). For a detailed discussion of \nintentionality and the intentional stance, see Dennett (1978, 1987). A number of papers on AI \ntreatments of agency may be found in Allen et al. ( 1990). For an introduction to modal logic, sec \nChellas (1980); a slightly older, though more wide ranging introduction, may be found in Hughes \nand Cresswell (1968). As for the use of modal logics to model knowledge and belief, see Halpern \nand Moses (1992), which includes complexity results and proof procedures. Related work on \nmodelling knowledge has been done by the distributed systems community, who give the worlds in \npossible worlds semantics a precise interpretation; for an introduction and further references, see \nHalpern (1987) and Fagin et al. (1992). Overviews of formalisms for modelling belief and \nknowledge may be found in Halpern (1986), Konolige (1986a), Reichgelt (1989a) and Wooldridge \n(1992). A variant of the possible worlds framework, called the recursive modelling method, is \ndescribed in Gmytrasiewicz and Durfee (1993); a deep theory of belief may be found in Mack \n(1994). Situation semantics, developed in the early 1980s and recently the subject of renewed \ninterest, represent a fundamentally new approach to modelling the world and cognitive systems \n(Barwise & Perry, 1983; Devlin, 1991). However, situation semantics are not (yet) in the \nmainstream of (D)AJ, and it is not obvious what impact the paradigm will ultimately have. \nLogics which integrate time with mental states are discussed in Kraus and Lehmann (1988), \nHalpern and Vardi (1989) and Wooldridge and Fisher (1994); the last of these presents a tableau\nbased proof method for a temporal belief logic. Two other important references for temporal \naspects are Shoham (1988. 1989). Thomas has developed some logics. for representing agent \ntheories as part of her framework for agent programming languages; see Thomas et al. (1991) and \nThomas (1993) and section 4. For an introduction to temporal logics and related topics, see \nGoldblatt (1987) and Emerson (1990). A non-formal discussion of intention may be found in \nBratman (1987), or more briefly (Bratman, 1990). Further work on modelling intention may be \nfound in Grosz and Sidner (1990), Sadek (1992), Goldman and Lang (1991), Konolige and Pollack \n(1993), Bell (1995) and Dongha (1995). Related work, focusing less on single-agent attitudes, and \nmore on social aspects, is Levesque et al. (1990), Jennings (1993a), Wooldridge (1994) and \nWooldridge and Jennings (1994). \nFinally, although we have not discussed formalisms for reasoning about action here, we \nsuggested above that an agent logic would need to incorporate some mechanism for representing \nagent's actions. Our reason for avoiding the topic is simply that the field is so big, it deserves a \nwhole review in its own right. Good starting points for AI treatments of action arc Allen (1984), \nand Allen et al. (1990, 1991). Other treatments of action in agent logics arc based on formalisms \nborrowed from mainstream computer science, notably dynamic logic (originally developed to \nreason about computer programs) (Hare!, 1984). The logic of seeing to it that has been discussed in \nthe formal philosophy literature, but has yet to impact on (D)AI (Belnap & Perloff, 1988; Perloff, \n1991; Belnap, 1991; Segerberg, 1989). \n\nM. WOOLDRIDGE A:-!D NICHOi.AS JENNINGS 130 \n3 Agent architectures \nUntil now, this article has been concerned with agent theory-the construction of formalisms for \nreasoning about agents, and the properties of agents expressed in such formalisms. Our aim in this \nsection is to shift the emphasis from theory to practice. We consider the issues surrounding the \nconstruction of computer systems that satisfy the properties specified by agent theorists. This is the \narea of agent architectures. Maes defines an agent architecture as: \n"(A] particular methodology for building [agents]. It specifies how ... the agent can be decomposed into \nthe construction of a set of component modules and how these modules should be made to interact. The \ntotal set of modules and their interactions has to provide an answer to the question of how the sensor data \nand the current internal state of the agent determine the actions ... and future internal state of the agent. \nAn architecture cncompa~ses techniques and algorithms that support this methodology' (Maes, 1991, \np.115) \nKaelbling considers an agent architecture to be: \n"(A] specific collection of software (or hardware) modules, typically designated by boxes with arrows \nindicating the data and control flow among the modules. A more abstract view of an architecture is as a \ngeneral methodology for designing particular modular decompositions for particular tasks.,. (Kaelbling, \n1991, p.86) \nThe classical approach to building agents is to view them as a particular type of knowledge-based \nsystem. This paradigm is known as symbolic Al: we begin our review of architectures with a look at \nthis paradigm, and the assumptions that underpin it. \n3.1 Classical approaches: deliberative architectures \nThe foundation upon which the symbolic AI paradigm rests is the physical-symbol system \nhypothesis, formulated by Newell and Simon (1976). A physical symbol system is defined to be a \nphysically realisable set of physical entities (symbols) that can be combined to form structures, and \nwhich is capable of running processes that operate on those symbols according to symbolically \ncoded sets of instructions. The physical-symbol system hypothesis then says that such a system is \ncapable of general intelligent action. \nft is a short step from the notion of a physical symbol system to McCarthy's dream of a sentential \nprocessing automaton, or deliberative agent. (The term "deliberative agent" seems to have derived \nfrom Genesercth"s use of the the term "deliberate agent'' to mean a specific type of symbolic \narchitecture (Genesereth and Ntlsson, 1987, pp. 325-327).) We define a deliberative agent or agent \narchitecture to be one that contains an explicitly represented, symbolic model of the world, and in \nwhich decisions (for example about what actions to perform) arc made via logical (or at least \npseudo-logical) reasoning, based on pattern matching and symbolic manipulation. The idea of \ndeliberative agents based on purely logical reasoning is highly seductive: to get an agent to realise \nsome theory of agency one might naively suppose that it is enough to simply give it logical \nrepresentation of this theory and "get it to do a bit of theorem proving" (Shardlow. 1990, section \n3.2). If one aims to build an agent in this way, then there are at least two important problems to be \nsolved: \n1. The transduction problem: that of translating the real world into an accurate, adequate \nsymbolic description, in time for that description to be useful. \n2. The representation/reasoning problem: that of how to symbolically represent information \nabout complex real-world entities and processes, and how to get agents to reason with this \ninformation in time for the results to be useful. \nThe former problem has led to work on vision, speech understanding, learning. etc. The latter has \nled to work on knowledge representation, automated reasoning, automatic planning, etc. Despite \nthe immense volume of work that these problems have generated, most researchers would accept \nthat neither is anywhere near solved. Even seemingly trivial problems, such as commonsense \n\nIntelligent agents: theory and practice 131 \nreasoning, have turned out to be extremely difficult (cf. the CYC project (Guba & Lenat, 1994)). \nThe underlying problem seems to be the difficulty of theorem proving in even very simple logics, \nand the complexity of symbol manipulation algorithms in general: recall that first-order logic is not \neven decidable, and modal extensions to it (including representations of belief, desire, time, and so \non) tend to be highly undecidable. Thus, the idea of building "agents as theorem provers"-what \nmight be called an extreme logicist view of agency-although it is very attractive in theory, seems, \nfor the time being at least, to be unworkable in practice. Perhaps more troubling for symbolic AI is \nthat many symbol manuipulation algorithms of interest are intractable. lt seems hard to build \nuseful symbol manipulation algorithms that will he guaranteed to terminate with useful results in an \nacceptable fixed time bound. And yet such algorithms seem essential if agents are to operate in any \nreal-world. time-constrained domain. Good discussions of this point appear in Kaelbling (1986) \nand Russell and Wefald (1991). \nIt is because of these problems that some researchers have looked to alternative techniques for \nbuilding agents; such alternatives are discussed in section 3.2. First, however, we consider efforts \nmade within the symbolic Al community to construct agents. \n3.1.1 Planning agents \nSince the early 1970s, the AI planning community has been closely concerned with the design of \nartificial agents; in fact, it seems reasonable to claim that most innovations in agent design have \ncome from this community. Planning is essentially automaticprngamming: the design of aeourse of \naction that, when executed, will result in the achievement of some desired goal. Within the \nsymbolic AI community, it has long been assumed that some form of Al planning system will be a \ncentral component of any artificial agent. Perhaps the best-known early planning system was \nSTRIPS (Fikes & Nilsson, 1971). This system takes a symbolic description of both the world and a \ndesired goal state, and a set of action descriptions, which characterise the pre- and post-conditions \nassociated with various actions. It then attempts to find a sequence of actions that will achieve the \ngoal, by using a simple means-ends analysis. which essentially involves matching the post\nconditions of actions against the desired goal. The STRIPS planning algorithm was very simple, \nand proved to be ineffective on problems of even moderate complexity. Much effort was \nsubsequently devoted to developing more effective techniques. Two major innovations were \nhierarchical and non-linear planning (Sacerdoti, 1974, 1975). However, in the mid 1980s, Chapman \nestablished some theoretical results which indicate that even such refined techniques will ultimately \nturn out to be unusable in any time-constrained system (Chapman, 1987). These results have had a \nprofound influence on subsequent AI planning research; perhaps more than any other, they have \ncaused some researchers to question the whole symbolic AI paradigm, and have thus led to the \nwork on alternative approaches that we discuss in section 3.2. \nIn spite of these difficulties, various attempts have been made to construct agents whose primary \ncomponent is a planner. For example: the Integrated Planning, Execution and Monitoring (IPEM) \nsystem is based on a sophisticated non-linear planner (Ambros-Ingerson and Steel, 1988); Wood"s \nAUTODRIVE system has planning agents operating in a highly dynamic environment (a traffic \nsimulation) (Wood, 1993); Etzioni has built "soft bots" that can plan and act in a Unix environment \n(Etzioni et al., 1994); and finally, Cohen's PHOENIX system includes planner-based agents that \noperate in the domain of simulated forest fire management (Cohen et al., 1989). \n3.1.2 Rratman, Israel and Pollack-IRMA \nIn section 2, we saw that some researchers have considered frameworks for agent theory based on \nbeliefs, desires, and intentions (Rao & Georgeff, 1991b). Some researchers have also developed \nagent architectures based on these attitudes. One example is the Intelligent Resource-hounded \nMachine Architecture (IRMA) (Bratman et a!., 1988). This architecture has four key symbolic data \nstructures: a plan library, and explicit representations of beliefs, desires, and intentions. Addition\nally, the architecture has: a reasoner, for reasoning about the world; a means-end analyser, for \ndetermining which plans might be used to achieve the agent's intentions; an opportunity analyser, \n\nM. WOOLDRIDGE AND NICHOLAS JENNINGS 132 \nwhich monitors the environment in order to determine further options for the agent; a filtering \nprocess; and a deliberation process. The filtering process is responsible for determining the subset \nof the agent's potential courses of action that have the property of being consistent with the agent's \ncurrent intentions. The choice between competing options is made by the deliberation process. The \nIRMA architecture has been evaluated in an experimental scenario known as the Tileworld \n(Pollack & Ringuette, 1990). \n3.1.3 Vere and Bickmore-HOMER \nAn interesting experiment in the design of intelligent agents was conducted by Vere and Bickmore \n(1990). They argued that the enabling technologies for intelligent agents are sufficiently developed \nto be able to construct a prototype autonomous agent, with linguistic ability, planning and acting \ncapabilities, and so on. They developed such an agent, and christened it HOMER. This agent is a \nsimulated robot submarine, which exists in a two-dimensional "Seaworld'', about which it has only \npartial knowledge. HOMER takes instructions from a user in a limited subset of English with about \nan 800 word vocubulary; instructions can contain moderately sophisticated temporal references. \nHOMER can plan how to achieve its instructions (which typically relate to collecting and moving \nitems around the Seaworld), and can then execute its plans, modifying them as required during \nexecution. The agent has a limited episodic memory, and using this, is able to answer questions \nabout its past experiences. \n3.2.4 Jennings-GRATE* \nGRATE* is a layered architecture in which the behaviour of an agent is guided by the mental \nattitudes of beliefs, desires, intentions and joint intentions (Jennings, 1993b). Agents are divided \ninto two distinct parts: a domain level system and a cooperation and control layer. The former \nsolves problems for the organisation; be it in the domain of industrial control, finance or \ntransportation. The latter is a meta-level controller which operates on the domain level system with \nthe aim of ensuring that the agent's domain level activities are coordinated with those of others \nwithin the community. The cooperation layer is composed of three generic modules: a control \nmodule which interfaces to the domain level system, a situation assessment module and a \ncooperation module. The assessment and cooperation modules provide an implementation of a \nmodel of joint responsibility (Jennings, 1992), which specifics how agents should act both locally \nand towards other agents whilst engaged in cooperative problem solving. The performance of a \nGRATE* community has been evaluated against agents which only have individual intentions, and \nagents which behave in a selfish manner, in the domain of electricity transportation management. \nA significant improvement was noted when the situation became complex and dynamic (Jennings, \n1995). \n3.2 Alternative approaches: reactive architectures \nAs we observed above, there arc many unsolved (some would say insoluble) problems associated \nwith symbolic Al. These problems have led some researchers to question the viability of the whole \nparadigm, and to the development of what are generally known as reactive architectures. For our \npurposes, we shall define a reactive architecture to be one that does not include any kind of central \nsymbolic world model, and does not use complex symbolic reasoning. \n3.2. l Brooks-behaviour languages \nPossibly the most vocal critic of the symbolic AI notion of agency has been Rodney Brooks, a \nresearcher at MIT who apparently became frustrated by AI approaches to building control \nmechanisms for autonomous mobile robots. In a 1985 paper, he outlined an alternative architec\nture for building agents, the so called subsumption architecture (Brooks, 1986). The review of \nalternative approaches begins with Brooks' work. \nIn recent papers, Brooks (1990, 1991a,b) has propounded three key theses: \n\nIntelligent agents: theory and practice 133 \n1. Intelligent behaviour can be generated without explicit representations of the kind that symbolic \nAI proposes. \n2. Intelligent behaviour can be generated without explicit abstract reasoning of the kind that \nsymbolic AI proposes. \n3. Intelligence is an emergent property of certain complex systems. \nBrooks identifies two key ideas that have informed his research: \n1. Situatedness and embodiment: "Real" intelligence is situated in the world, not in disembodied \nsystems such as theorem provers or expert systems. \n2. Intelligence and emergence: "Intelligent" behaviour arises as a result of an agent's interaction \nwith its environment. Also, intelligence is "in the eye of the beholder"; it is not an innate, \nisolated property. \nIf Brooks was just a Dreyfus-style critic of AI, his ideas might not have gained much currency. \nHowever, to demonstrate his claims, he has built a number of robots, based on the suhsumption \narchitecture. A subsumption architecture is a hierarchy of task-accomplishing behaviours. Each \nbehaviour "competes" with others to exercise control over the robot. Lower layers represent more \nprimitive kinds of behaviour (such as avoiding obstacles), and have precedence over layers further \nup the hierarchy. It should be stressed that the resulting systems are, in terms of the amount of \ncomputation they need to do, extremely simple, with no explicit reasoning of the kind found in \nsymbolic AI systems. But despite this simplicity, Brooks has demonstrated the robots doing tasks \nthat would be impressive if they were accomplished by symbolic AI systems. Similar work has been \nreported by Steels, who described simulations of "Mars explorer" systems, containing a large \nnumber of subsumption-architecture agents, that can achieve near-optimal performance in certain \ntasks (Steels, 1990). \n3.2.2 Agre and Chapman-PENG! \nAt about the same time as Brooks was describing his first results with the subsumption architecture, \nChapman was completing his Master's thesis, in which he reported the theoretical difficulties with \nplanning described above, and was coming to similar conclusions about the inadequacies of the \nsymbolic AI model himself. Together with his co-worker Agre, he began to explore alternatives to \nthe AI planning paradigm (Chapman & Agre, 1986). \nAgre observed that most everyday activity is routine" in the sense that it requires little-if \nany-new abstract reasoning. Most tasks, once learned, can be accomplished in a routine way, with \nlittle variation. Agre proposed that an efficient agent architecture could be based on the idea of \n''running arguments". Crudely, the idea is that as most decisions are routine, they can be encoded \ninto a low-level structure (such as a digital circuit), which only needs periodic updating, perhaps to \nhandle new kinds of problems. His approach was illustrated with the celebrated PENGI system \n(Agre & Chapman, 1987). PENGI is a simulated computer game, with the central character \ncontrolled using a scheme such as that outlined above. \n3.2.3 Rosenschein and Kaelhling-situated automata \nAnother sophisticated approach is that of Rosenschein and Kaclbling (Rosenschein, 1985; \nRosenschein & Kaelbling, 1986; Kaelbling & Rosenschcin, 1990; Kaelbling, 1991). In their situated \nautomata paradigm, an agent is specified in declarative terms. This specification is then compiled \ndown to a digital machine, which satisfies the declarative specification. This digital machine can \noperate in a provably time-bounded fashion; it does not do any symbol manipulation, and in fact no \nsymbolic expressions arc represented in the machine at all. The logic used to specify an agent is \nessentially a modal logic of knowledge (see above). The technique depends upon the possibility of \ngiving the worlds in possible worlds semantics a concrete interpretation in terms of the states of an \nautomaton: \n\nM. WOOLDRIDGE AND NICHOLAS JENNINGS 134 \n"[An agent} ... xis said to carry the information that pin world states, written s I= K(x,p), if for all world \nstates in which x has the same value as it does ins, the proposition pis true." (Kae!bling & Rosenschcin, \n1990, p. 36) \nAn agent is specified in terms of two components: perception and action. Two programs are then \nused to synthesise agents: RULER is used to specify the perception component of an agent; \nGAPPS is used to specify the action component. \nRULER takes as its input three components: \n"f A j specification of the semantics of the [ agent's} inputs ("whenever bit 1 is on, it is raining"); a set of static \nfacts ("whenever it is raining, the ground is wet"); and a specification of the state transitions of the world ("if \nthe ground is wet, it stays wet until the sun comes out"). The programmer then specifies the desired \nsemantics for the output ("if this bit is on, the ground is wet"), and the compiler ... [synthesises] a circuit \nwhose output will have the correct semantics .... All that declarative '"knowledge" has been reduced to a \nvery simple circuit." (Kaelb!ing, 1991, p. 86) \nThe GAPPS program takes as its input a set of goal reduction rules (essentially rules that encode \ninformation about how goals can be achieved), and a top level goal, and generates a program that \ncan be translated into a digital circuit to realise the goal. Once again, the generated circuit does not \nrepresent or manipulate symbolic expressions; all symbolic manipulation is done at compile time. \nThe situated automata paradigm has attracted much interest, as it appears to combine the best \nelements of both reactive and symbolic, declarative systems. However, at the time of writing, the \ntheoretical limitations of the approach are not well understood; there are similarities with the \nautomatic synthesis of programs from temporal logic specifications, a complex area of much \nongoing work in mainstream computer science (see the comments in Emerson, 1990). \n3.2.4 Maes-Agent network architecture \nPattie Maes has developed an agent architecture in which an agent is defined as a set of competence \nmodules (Macs, 1989, 1990b, 1991 ). These modules loosely resemble the behaviours of Brooks' \nsubsumption architecture (above). Each module is specified by the designer in terms of pre- and \npost-conditions (rather like STRIPS operators), and an activation level, which gives a real-valued \nindication of the relevance of the module in a particular situation. The higher the activation level of \na module, the more likely it is that this module will influence the behaviour of the agent. Once \nspecified, a set of competence modules is compiled into a spreading activation network, in which the \nmodules are linked to one-another in ways defined by their pre- and post-conditions. For example, \nif module a has post-condition rp, and module b has pre-condition cp, then a and bare connected by \na successor link. Other types of link include predecessor links and conflicter links. When an agent is \nexecuting, various modules may become more active in given situations, and may be executed. The \nresult of execution may be a command to an effector unit, or perhaps the increase in activation !eve! \nof a successor module. \nThere are obvious similarities between the agent network architecture and neural network \narchitectures. Perhaps the key difference is that it is difficult to say what the meaning of a node in a \nneural net is; it only has a meaning in the context of the net itself. Since competence modules are \ndefined in declarative terms, however, it is very much easier to say what their meaning is. \n3.3 Hybrid architectures \nMany researchers have suggested that neither a completely deliberative nor completely reactive \napproach is suitable for building agents. They have argued the case for hybrid systems, which \nattempt to marry classical and alternative approaches. \nAn obvious approach is to build an agent out of two (or more) subsystems: a deliberative one, \ncontaining a symbolic world model, which develops plans and makes decisions in the way proposed \nby mainstream symbolic AI; and a reactive one, which is capable of reacting to events that occur in \nthe environment without engaging in complex reasoning. Often, the reactive component is given \n\nIntelligent agents: theory and practice 135 \nsome kind of precedence over the deliberative one, so that it can provide a rapid response to \nimportant environmental events. This kind of structuring leads naturally to the idea of a layered \narchitecture, of which TouringMachincs (Ferguson, 1992) and InteRRaP (Muller & Pischel, 1994) \nare good examples. (These architectures are described below.) In such an architecture, an agent's \ncontrol subsystems are arranged into a hierarchy, with higher layers dealing with information at \nincreasing levels of abstraction. Thus, for example, the very lowest layer might map raw sensor \ndata directly onto effector outputs, while the uppermost layer deals with long-term goals. A key \nproblem in such architectures is what kind of control framework to embed the agent's subsystems \nin, to manage the interactions between the various layers. \n3.3.1 Georgeff and Lansky-PRS \nOne of the best-known agent architectures is the Procedural Reasoning System (PRS), developed \nby Georgeff and Lansky (1987). Like IRMA (see above), the PRS is a belief-desire-intention \narchitecture, which includes a plan library, as well as explicit symbolic representations of beliefs, \ndesires, and intentions. Beliefs are facts, either about the external world or the system's internal \nstate. These facts are expressed in classical first-order logic. Desires are represented as system \nbehaviours (rather than as static representations of goal states). A PRSplan library contains a set of \npartially-elaborated plans, called knowledge areas (KA), each of which is associated with an \nini,ocation condition. This condition determines when the KA is to be actil'ated. KAs may be \nactivated in a goal-driven or data-driven fashion; KAs may also be reactive, allowing the PRS to \nrespond rapidly to changes in its environment. The set of currently active KAs in a system represent \nits intentions. These various data structures are manipulated by a system interpreter, which is \nresponsible for updating beliefs, invoking KAs, and executing actions. The PRS has been \nevaluated in a simulation of maintenance procedures for the space shuttle, as well as other domains \n(Georgcff & lngrand, 1989). \n3.3.2 Ferguson-Touring Machines \nFor his 1992 Doctoral thesis, Ferguson developed the Touring Machines hybrid agent architecture \n(Ferguson, 1992a,b). 5 The architecture consists of perception and action subsystems, which \ninterface directly with the agent's environment, and three control layers, embedded in a control \nframework, which mediates between the layers. Each layer is an independent, activity-producing, \nconcurrently executing process. \nThe reactive layer generates potential courses of action in response to events that happen too \nquickly for other layers to deal with. It is implemented as a set of situation-action rules, in the style \nof Brooks' subsumption architecture (see above). \nThe planning layer constructs plans and selects actions to execute in order to achieve the agent's \ngoals. This layer consists of two components: a planner, and a focus of attention mechanism. The \nplanner integrates plan generation and execution. and uses a library of partially elaborated plans, \ntogether with a topological world map, in order to construct plans that will accomplish the agent's \nmain goal. The purpose of the focus of attention mechanism is to limit the amount of information \nthat the planner must deal with, and so improve its efficiency. It does this by filtering out irrelevant \ninformation from the environment. \nThe modelling layer contains symbolic representations of the cognitive state of other entities in \nthe agent's environment. These models are manipulated in order to identify and resolve goal \nconflicts-situations where an agent can no longer achieve its goals, as a result of unexpected \ninterference. \nThe three layers are able to communicate with each other (via message passing), and are \nembedded in a control framework. The purpose of this framework is to mediate between the \n5 1t i~ worth noting that Fcrgu~on's thesis gives a good overview of the problems and issues associated with \nbuilding rational, resource-bounded agents. Moreover. the description given of the TouringMachines \narchitecture is itself extremely clear. We recommend it as a point of departure for further reading. \n\nM. WOOLDRIDGE AND '.'JICHOLAS JENNINGS 136 \nlayers, and in particular, to deal with conflicting action proposals from the different layers. The \ncontrol framework does this by using control rules. \n3.3.3 BurmeLHer et al.-COSY \nThe COSY architecture is a hybrid BDI-architecture that includes elements of both the PRS and \nIRMA, and was developed specifically for a multi-agent testbed called DASEDIS (Burmeister & \nSundermeyer; Haddadi, 1994). The architecture has five main components: (i) sensors; (ii) \nactuators; (iii) communications (iv) cognition; and (v) intention. The first three components are \nstraightforward: the sensors receive non-communicative perceptual input, the actuators allow the \nagent to perform non-communicative actions, and the communications component allows the \nagent to send messages. Of the remaining two components, the intention component contains \n"long-term goals, attitudes, responsibilities and the like ... the control elements taking part in the \nreasoning and decision-making of the cognition component" (Haddadi, 1994, p. 15), and the \ncognition component is responsible for mediating between the intentions of the agent and its beliefs \nabout the world, and choosing an appropriate action to perform. Within the cognition component \nis the knowledge base containing the agent's beliefs, and three procedural components: a script \nexecution component, a protocol execution component, and a reasoning, deciding and reacting \ncomponent. A script is very much like a script in Schan k's original sense: it is a stereotypical recipe \nor plan for achieving a goal. Protocols are stereotypical dialogues representing cooperation \nframeworks such as the contract net (Smith, 1980). The reasoning, deciding and reacting \ncomponent is perhaps the key component in COSY. It is made up of a number of other subsystems, \nand is structured rather like the PRS and IRMA (see above). An agenda is maintained, that \ncontains a number of active scripts. These scripts may be invoked in a goal-driven fashion (to satisfy \none of the agent's intentions), or a data-driven fashion (in response to the agent's current \nsituation). A filter component chooses between competing scripts for exerntion. \n3.3.4 MUiler et al.~lnteRRaP \nIntcRRaP, like Ferguson's TouringMachines, is a layered architecture, with each successive layer \nrepresenting a higher level of abstraction than the one below it (Millier & Pischel, 1994; Millier et \nal., 1995; Millier, 1994). In InteRRaP, these layers are further subdivided into two vertical layers: \none containing layers of knowledge bases, the other containing various control components, that \ninteract with the knowledge bases at their level. At the lowest level is the world interface control \ncomponent, and the corresponding world level knowledge base. The world interface component, \nas its name suggests, manages the interface between the agent and its environment, and thus deals \nwith acting, communicating, and perception. \nAbove the world interface component is the behaviour-based component. The purpose of this \ncomponent is to implement and control the basic reactive capability of the agent. This component \nmanipulates a set of patterns of behaviour (PoB). A PoB is a structure containing a pre-condition \nthat defines when the PoB is to be activated, various conditions that define the circumstances under \nwhich the PoB is considered to have succeeded or failed, a post-condition (Ula STRIPS (Fikes & \nNilsson, 1971)), and an executable body, that defines what action should be performed if the PoB is \nexecuted. (The action may be a primitive, resulting in a call on the agent's world interface. or may \ninvolve calling on a higher-level layer to generate a plan.) \nAbove the behaviour-based component in TnteRRaP is the plan-based component. This \ncomponent contains a planner that is able to generate single-agent plans in response to requests \nfrom the behaviour-based component. The knowledge-base at this layer contains a set of plans, \nincluding a plan library. The highest layer of InteRRaP is the cooperation component. This \ncomponent is able to generate joint plans, that satisfy the goals of a number of agents, by \nelaborating plans selected from a plan library. These plans arc generated in response to requests \nfrom the plan-based component. \nControl in lnteRRaP is both data- and goal-driven. Perceptual input is managed by the world\ninterface, and typically results in a change to the world model. As a result of changes to the world \n\nIntelligent agents: theory and practice 137 \nmodel, various patterns of behaviour may be activated, dropped, or executed. As a result of PoB \nexecution, the plan-based <;omponent and cooperation component may be asked to generate plans \nand joint plans respectively, in order to achieve the goals of the agent. This ultimately results in \nprimitive actions and messages being generated by the world interface. \n3.4 Discussion \nThe deliberative, symbolic paradigm is, at the time of writing, the dominant approach in (D)AL \nThis state of affairs is likely to continue, at least for the near future. There seem to be several \nreasons for this. Perhaps most importantly, many symbolic AI techniques (such as rule-based \nsystems) carry with them an associated technology and methodology that is becoming familiar to \nmainstream computer scientists and software engineers. Despite the well-documented problems \nwith symbolic AI systems, this makes symbolic AI agents (such as GRATE*, Jennings, 1993b) an \nattractive proposition when compared to reactive systems, which have as yet no associated \nmethodology. The need for a development methodology seems to be one of the most pressing \nrequirements for reactive systems. Anecdotal descriptions of current reactive systems implemen\ntations indicate that each such system must be individually hand-crafted through a potentially \nlengthy period of experimentation (Wavish and Graham, 1995). This kind of approach seems \nunlikely to be usable for large systems. Some researchers have suggested that techniques from the \ndomain of genetic algorithms or machine learning might be used to get around these development \nproblems, though this work is at a very early stage. \nThere is a pressing need for research into the capabilities of reactive systems, and perhaps in \nparticular to the types of application for which these types of system are best suited; some \npreliminary work has been done in this area, using a problem domain known as the Tile World \n(Pollack & Ringuette, 1990) With respect to reactive systems, Ferguson suggests that: \n"jT]he strength of purely non-deliberative architectures lies in their ability to exploit local patterns of \nactivity in their current surroundings in order to generate more or less hardwired action responses .. for a \ngiven set of stimuli Successful operation using this method pre-supposes: i that the complete set of \nenvironmental stimuli required for unambiguously determining action sequences is always present and \nreadily identifiable-in other words, that the agent's activity can be sttuationally determined; ii that the \nagent has no global task constraints ... which need to be reasoned about at run time; and iii that the agent's \ngoal or desire system is capable of being represented implicitly in the agent's structure according to a fixed, \npre-compiled ranking scheme." (Ferguson. 1992a, pp. 29-30} \nHybrid architectures, such as the PRS, TouringMachines, InteRRaP, and COSY, are currently a \nvery active area of work, and arguably have some advantages over both purely deliberative and \npurely reactive architectures. However, an outstanding problem with such architectures is that of \ncombining multiple interacting subsystems (deliberative and reactive) cleanly, in a well-motivated \ncontrol framework. Humans seem to manage different levels of abstract behaviour with compari\ntive ease; it is not clear that current hybrid architectures can do so. \nAnother area where as yet very little work has been done is the generation of goals and \nintentions. Most work in AT assumes that an agent has a single, well-defined goal that it must \nachieve. But if agents are ever to be really autonomous, and act pro-actively, then they must be \nable to generate their own goals when either the situation demands, or the opportunity arises. \nSome preliminary work in this area is Norman and Long (1995). Similarly, little work has yet been \ndone into the management and scheduling of multiple, possibly conflicting goals; some preliminary \nwork is reported in Dongha (1995). \nFinally, we turn to the relationship between agent theories and agent architectures. To what \nextent do the agent architectures reviewed above correspond to the theories discussed in section 2? \nWhat, if any, is the theory that underpins an architecture? With respect to purely deliberative \narchitectures, there is a wealth of underlying theory. The close relationship between symbolic \nprocessing systems and mathematical logic means that the semantics of such architectures can often \nbe represented as a logical system of some kind. There is a wealth of work establishing such \n\nM. WOOLORrDGE A.ND NICHOLAS JENNINGS 138 \nrelationships in Al, of which a particularly relevant example is Rao and Georgeff (1992a). This \narticle discusses the relationship between the abstract BDI logics developed by Rao et al. for \nreasoning about agents, and an abstract "agent interpreter", based on the PRS. However, the \nrelationship between the logic and the architecture is not formalised; the BDI logic is not used to \ngive a formal semantics to the architecture, and in fact it is difficult to see how such a logic could he \nused for this purpose. A serious attempt to define the semantics of a (somewhat simple) agent \narchitecture is presented in Wooldridge (1995), where a formal model of the system MyWorld, in \nwhich agents are directly programmed in terms of beliefs and intentions, is used as the basis upon \nwhich to develop a logic for reasoning about MyWorld systems. Although the logic contains \nmodalities for representing beliefs and intentions, the semantics of these modalities are given in \nterms of the agent architecture itself, and the problems associated with possible worlds do not, \ntherefore, arise; this work builds closely on Konolige's models of the beliefs of symbolic AI systems \n(Konoligc, 1986a). However, more work needs to be done using this technique to model more \ncomplex architectures, before the limitations and advantages of the approach are well-understood. \nLike purely deliberative architectures, some reactive systems are also underpinned by a \nrelatively transparent theory. Perhaps the best example is the situated automata paradigm, where \nan agent is specified in terms of a logic of knowledge, and this specification is compiled down to a \nsimple digital machine that can he realistically said to realise its corresponding specification. \nHowever, for other purely reactive architectures, based on more ad hoc principles, it is not clear \nthat there is any transparent underlying theory. It could be argued that hybrid systems also tend to \nbead hoc, in that while their structures are well-motivated from a design point of view, it is not clear \nhow one might reason about them, or what their underlying theory is. In particular, architectures \nthat contain a number of independent activity producing subsystems, which compete with each \nother in real time to control the agent's activities, seem to defy attempts at formalisation. It is a \nmatter of debate whether this needs he considered a serious disadvantage, but one argument is that \nunle~s we have a good theoretical model of a particular agent or agent architecture, then we shall \nnever really understand why it works. This is likely to make it difficult to generalise and reproduce \nresults in varying domains. \n3.5 Further reading \nMost introductory textbooks on Al discuss the physical symbol system hypothesis; a good recent \nexample of such a text is Ginsberg (1993). A detailed discussion of the way that this hypothesis has \naffected thinking in symbolic AI is provided in Shardlow (1990). There are many objections to the \nsymbolic AI paradigm, in addition to those we have outlined above. Again, introductory textbooks \nprovide the stock criticisms and replies. \nThere is a wealth of material on planning and planning agents. See Georgeff (1987) for an \noverview of the state of the art in planning (as it was in 1987), Allen et al. (1990) for a thorough \ncollection of papers on planning (many of the papers cited above are included), and Wilkins (1988) \nfor a detailed description of SIPE, a sophisticated planning system used in a real-world application \n(the control of a brewery!) Another important collection of planning papers is Georgeff and \nLansky (1986). The book by Dean and Wellman and the book by Allen et al. contain much useful \nrelated material (Dean and Wellman, 1991; Allen et al., 1991 ). There is now a regular international \nconference on planning; the proceedings of the first were published as Hendler (1992). \nThe collection of papers edited by Maes (1990a) contains many interesting papers on alterna\ntives to the symbolic AI paradigm. Kaelbling (1986) presents a clear discussion of the issues \nassociated with developing resource-bounded rational agents, and proposes an agent architecture \nsomewhat similar to that developed by Brooks. A proposal by Nilsson for teleo reactive programs\ngoal directed programs that nevertheless respond to their environment-is described in Nilsson \n(1992). The proposal draws heavily on the situated automata paradigm; other work based on this \nparadigm is described in Shoham (1990) and Kiss and Reichgelt (1992). Schoppcrs has proposed \ncompiling plans in advance, using traditional planning techniques, in order to develop universal \n\nIntelligent agents: theory and practice 139 \nplans, which are essentially decision trees that can be used to efficiently determine an appropriate \naction in any situation (Schoppers, 1987). Another proposal for building "reactive planners" \ninvolves the use of reactive action packages (Firby, 1987). \nOther hybrid architectures are described in Hayes-Roth (1990), Downs and Reichgelt (1991), \nAylett and Eustace (1994) and Bussmann and Demazeau (1994). \n4 Agent languages \nAs agent technology becomes more established, we might expect to see a variety of software tools \nbecome available for the design and construction of agent-based systems; the need for software \nsupport tools in this area was identified as long ago as the.mid-1980s (Gasser et al., 1987). The \nemergence of a number of prototypical agent languages is one sign that agent technol0gy is \nbecoming more widely used, and that many more agent-based applications are likely to be \ndeveloped in the near future. By an agent language, we mean a system that allows one to program \nhardware or software computer systems in terms of some of the concepts developed hy agent \ntheorists. At the very least, we expect such a language to include some structure corresponding to \nan agent. However, we might also expect to sec some other attributes of agency (beliefs, goals, or \nother mentalistic notion~) used to program agents. Some of the languages we consider below \nembody this strong notion of agency; others do not. However, all have properties that make them \ninteresting from the point of view of this review. \n4.0. I Concurrent object languages \nConcurrent object languages are in many respects the ancestors of agent languages. The notion of a \nself-contained concurrently executing object, with some internal state that is not directly accessible \nto the outside world, responding to messages from other such objects, is very close to the concept of \nan agent as we have defined it. The earliest concurrent object framework was Hewitt's Actor model \n(Hewitt, 1977; Agha, 1986); another well-known example is the ABCL system (Yonezawa, 1990). \nFor a discussion on the relationship between agents and concurrent object programming, see \nGasser and Briot (1992). \n4.0.2 Shoham-agent-oriented programming \nYoav Shoham has proposed a "new programming paradigm, based on a societal view of \ncomputation" (Shoham, 1990, p. 4; 1993). The key idea that informs this agent-oriented program\nming (AOP) paradigm is that of directly programming agents in terms of the mentalistic, \nintentional notions that agent theorists have developed to represent the properties of agents. The \nmotivation behind such a proposal is that, as we observed in section 2, humans use the intentional \nstance as an abstraction mechanism for representing the properties of complex systems. ln the same \nway that we use the intentional stance to describe humans, it might be useful to use the intentional \nstance to program machines. \nShoham proposes that a fully developed AOP system will have three components: \n a logical system for defining the mental state of agents; \n an interpreted programming language for programming agents; \n an "agentification" process, for compiling agent programs into low-level executable systems. \nAt the time of writing, Shoham has only published results on the first two components. (In Shoham \n(1990, p. 12), he wrote that "the third is still somewhat mysterious to me", though later in the paper \nhe indicated that he was thinking along the lines of Rosenschcin and Kaelblings situated automata \nparadigm (Rosenschcin & and Kaelbling, 1986).) Shohams first attempt at an AOP language was \nthe AGENT0 system. The logical component of this system is a quantified multi-modal logic, \nallowing direct reference to time. No semantics are given, but the logic appears to be based on \nThomas et al. (1991). The logic contains three modalities: belief, commitment and ability. The \nfollowing is an acceptable formula of the logic, illustrating its key properties: \n\nM. WOOLDRIDGE AND NICHOLAS JENNINGS 140 \nCA~ open(door) 8 = Bl CA~ open (door)8. \nThis formula is read: "if at time 5 agent a can ensure that the door is open at time 8, then at time 5 \nagent b believes that at time 5 agent a can ensure that the door is open at time 8". \nCorresponding to the logic is the AGENT0 programming language. In this language, an agent is \nspecified in terms of a set of capabilities (things the agent can do), a set of initial beliefs and \ncommitments, and a set of commitment rules. The key component, which determines how the agent \nacts, is the commitment rule set. Each commitment rule contains a message condition, a mental \ncondition, and an action. To determine whether such a rule fires, the message condition is matched \nagainst the messages the agent has received; the mental condiiion is matched against the beliefs of \nthe agent. If the rule fires, then the agent becomes committed to the action. Actions may be private, \ncorresponding to an internally executed subroutine, or communicative, i.e., sending messages. \nMessages are constrained to be one of three types: "requests" or "unrequests" to perform or refrain \nfrom actions, and "inform" messages, which pass on information-Shoham indicates that he took \nhis inspiration for these message types from speech act theory (Searle, 1969; Cohen & Perrault, \n1979). Request and unrequest messages typically result in the agent's commitments being \nmodified; inform messages result in a change to the agent's beliefs. \n4.0.3 Tltomas-PLACA \nAGENT0 was only ever intended as a prototype, to illustrate the principles of AOP. A more \nrefined implementation was developed by Thomas, for her 1993 doctoral thesis (Thomas, 1993). \nHer Planning Communicating Agents (PLACA) language was intended to address one severe \ndrawback to AGENT0: the inability of agents to plan, and communicate requests for action via \nhigh-level goals. Agents in PLACA are programmed in much the same way as in AGENT0, in \nterms of mental change rules. The logical component of PLACA is similar to AGENTO's, but \nincludes operators for planning to do actions and achieve goals. The semantics of the logic and its \nproperties are examined in detail. However, PLACA is not at the "production" stage; it is an \nexperimental language. \n4.0.4 Fisher-Concurrent MetateM \nOne drawback with both AGENT0 and PLACA is that the relationship between the logic and \ninterpreted programming language is only loosely defined: in neither case can the programming \nlanguage be said to truly execute the associated logic. The Concurrent MetateM language \ndeveloped by Fisher can make a stronger claim in this respect (Fisher, 1994). A Concurrent \nMetateM system contains a number of concurrently executing agents. each of which is able to \ncommunicate with its peers via asynchronous broadcast message passing. Each agent is pro\ngrammed by giving it a temporal logic specification of the behaviour that it is intended the agent \nshould exhibit. An agent's specification is executed directly to generate its behaviour. Execution of \nthe agent program corresponds to iteratively building a logical model for the temporal agent \nspecification. It is possible to prove that the procedure used to execute an agent specification is \ncorrect, in that if it is possible to satisfy the specification, then the agent will do so (Barringer et al., \n1989). \nThe logical semantics of Concurrent MetateM are closely related to the semantics of temporal \nlogic itself. This means that, amongst other things, the specification and verification of Concurrent \nMetateM systems is a realistic proposition (Fisher & Wooldridge, 1993). At the time of writing, \nonly prototype implementations of the language are available; full implementations are expected \nsoon. \n4.0.5 The IMAGINE Project-APRIL and MAIL \n,APRIL (McCabe & Clark, 1995) and MAIL (Haugeneder ct al., 1994) are two languages for \ndeveloping multi-agent applications that were developed as part of the ESPRIT project IMAGINE \n(Haugenedcr, 1994). The two languages arc intended to fulfil quite different roles. APRIL was \n\nIntelligent agents: theory and practice 141 \ndesigned to provide the core features required to realise most agent architectures and systems. \nThus APRIL provides facilities for multi-tasking (via processes, which are treated as first-class \nobjects, and a Unix-like fork facility), communication (with powerful message-passing facilities \nsupporting network-transparent agent-to-agent links); and pattern matching and symbolic process\ning capabilities. The generality of APRIL comes at the expense of powerful abstractions-an \nAPRIL system builder must implement an agent or system architecture from scratch using \nAPRIL's primitives. In contrast, the MAIL language provides a rich collection of pre-defined \nabstractions, including plans and multi-agent plans. APRIL was originally envisaged as the \nimplementation language for MAIL. The MAIL system has been used to implement several \nprototype multi-agent systems, including an urban traffic management scenario (Haugeneder and \nSteiner, 1994). \n4.0.6 General Magic, lnc.-TELESCRIPT \nTELESCRIPT is a language-based environment for constructing agent societies that has been \ndeveloped by General Magic, Inc.: it is perhaps the first commercial agent language. \nTELESCRIPT technology is the name given by General Magic to a family of concepts and \ntechniques they have developed to underpin their products. There are two key concepts in \nTELESCRIPT technology: places and agents. Places are virtual locations that are occupied by \nagents. Agents are the providers and consumers of goods in the electronic marketplace applications \nthatTELESCRIPTwas developed to support. Agents are software processes, and are mobile: they \nare able to move from one place to another, in which case their program and state are encoded and \ntransmitted across a network to another place, where execution recommences. Agents are able to \ncommunicate with one-another: if they occupy different places, then they can connect across a \nnetwork, in much the standard way; if they occupy the same location, then they can meet one \nanother. \nFour components have been developed by General Magic to support TELESCRIPT tech\nnology. The first is the TELESCRIPT language. This language "is designed for carrying out \ncomplex communication tasks: navigation, transportation, authentication, access control, and so \non" (White, 1994, p.17). The second component is the TELESCRIPT engine. An engine acts as an \ninterpreter for the TELESCRIPT language, maintains places, schedules agents for execution, \nmanages communication and agent transport, and finally, provides an interface with other \napplications. The third component is the TELESCRIPT protocol set. These protocols deal \nprimarily with the encoding and decoding of agents, to support transport between places. The final \ncomponent is a set of software tools to support the development of TELESCRIPT applications. \n4.0.7 Connah and Wavish-ABLE \nA group at Philips research labs in the UK have developed an Agent Behaviour Language (ABLE), \nin which agents are programmed in terms of simple, rule-like licences (Connah & Wavish, 1990; \nWavish, 1992). Licences may include some representation of time (though the language is not \nbased on any kind of temporal logic): they loosely resemble behaviours in the subsumption \narchitecture (see above). ABLE can be compiled down to a simple digital machine, realised in the \n"C" programming language. The idea is similar to situated automata, though there appears to \nbe no equivalent theoretical foundation. The result of the compilation process is a very fast \nimplementation, which has been used to control a Compact Disk-Interactive (CD-I) application. \nABLE has recently been extended to a version called Real-Time ABLE (RTA) (Wavish & \nGraham, 1995). \n4.1 Discussion \nThe emergence of various language-based software tools for building agent applications is clearly \nan important development for the wider acceptance and use of agent technology. The release of \nTELESCRIPT, a commercial agent language (albeit one that does not embody the strong notion of \n\nM. WOOLDRIDGE A:-1D NICHOLAS JENNINGS 142 \nagency discussed in this paper) is particularly important, as it potentially makes agent technology \navailable to a user base that is industrially (rather than academically) oriented. \nWhile the development of various languages for agent-based applications is of undoubted \nimportance, it is worth noting that all of the academically produced languages mentioned above are \nin some sense prototypes. Each was designed either to illustrate or examine some set of principles, \nand these languages were not, therefore, intended as production tools. Work is thus needed, both \nto make the languages more robust and usable, and to investigate the usefulness of the concepts \nthat underpin them. As with architectures, work is needed to investigate the kinds of domain for \nwhich the different languages are appropriate. \nFinally, we turn to the relationship between an agent language and the corresponding theories \nthat we discussed in section 2. As with architectures, it is possible to divide agent languages into \nvarious different categories. Thus AGENT0, PLACA, Concurrent MetateM, APRIL, and MAIL \narc deliberative languages, as they arc all based on traditional symbolic AI techniques. ABLE, on \nthe other hand, is a purely reactive language. With AGENT0 and PLACA, there is a clear (if \ninformal) relationship between the programming language and the logical theory the language is \nintended to realise. In both cases, the programming language represents a subset of the \ncorresponding logic, which can be interpreted directly. However, the relationship between logic \nand language is not formally defined. Like these two languages, Concurrent MetateM is intended \nto correspond to a logical theory. But the relationship hetween Concurrent MetateM and the \ncorresponding logic is much more closely defined, as this language is intended to be a directly \nexecutable version of the logic. Agents in Concurrent MetateM, however, are not defined in terms \nof mentalistic constructs. For a discussion on the relationship between Concurrent MetateM and \nAGENT0-like languages, see Fisher (1995). \n4.2 Further reading \nA recent collection of papers on concurrent object systems is Agha ct al. (1993). Various languages \nhave been proposed that marry aspects of object-based systems with aspects of Shohams agent\noriented proposal. Two examples are AGENTSPEAK and DAISY. AGENTSPEAK is loosely \nbased on the PRS agent architecture, and incorporates aspects of concurrent-object technology \n(Weerasooriya et al., 1995). In contrast, DAISY is based on the concurrent-object language CUBL \n(Adorni & Poggi, 1993), and incorporates aspects of the agent-oriented proposal (Poggi, 1995). \nOther languages of interest include OZ (Henz et al., 1993) and IC PRO LOG Il (Chu, 1993). \nThe latter, as its name suggests, is an extension of PROLOG, which includes multiple-threads, \nhigh-level communication primitives, and some object-oriented features. \n5 Applications \nAlthough this article is not intended primarily as an applications review. it is nevertheless worth \npausing to examine some of the current and potential applications of agent technology. \n5.1 Cooperative problem solving and distributed Al \nAs we observed in section 1, there has been a marked flowering of interest in agent technology \nsince the mid-1980s. This interest is in part due to the upsurge of interest in Distributed Al \nAlthough DAI encompasses most ofthc issues we have discussed in this paper, it should be stressed \nthat the dassical emphasis in DAI has been on macro phenomena (the social level), rather than the \nmicro phenomena (the agent level) that we have been concerned with in this paper. DAI thus looks \nat such issues as how a group of agents can be made to cooperate in order to efficiently solve \nproblems, and how the activities of such a group can be efficiently coordinated. DAI researchers \nhave applied agent technology in a variety of areas. Example applications include power systems \nmanagement (Wittig, 1992; Varga et al., 1994), air-traffic control (Steeb et al., 1988), particle \n\nIntelligent agents: theory and practice 143 \naccelerator control (Jennings et al., 1993), intelligent document retrieval (Mukhopadhyay et al., \n1986), patient care (Huang et al., 1995), telecommunications network management (Weihmayer & \nVelthuijsen, 1994), spacecraft control (Schwuttke & Quan, 1993), computer integrated manufac\nturing (Parunak, 1995), concurrent engineering (Cutkosky et al., 1993), transportation manage\nment (Fischer et al., 1993), job shop scheduling (Morley & Schelberg, 1993), and steel coil \nprocessing control (Mori et al., 1988). The classic reference to DAI is Bond and Gasser (1988), \nwhich includes both a comprehensive review article and a collection of significant papers from the \nfield; a more recent review article is Chaib-draa et al. (1992). \n5.2 Interface agents \nMacs defines interface agents as: \n"[C]omputer programs that employ artificial Intelligence techniques in order to provide a~sistancc to a user \ndealing with a particular application .... The metaphor is that of a personal assistant who is collaborating \nwith the user in the same work environment." (Macs, 1994h, p. 71) \nThere are many interface agent prototype applications: for example, the NewT system is a \nUSENET news tilter (along the lines mentioned in the second scenario that introduced this article) \n(Maes, 1994a, pp. 38-39). A NcwT agent is trained by giving it series of examples, illustrating \narticles that the user would and would not choose to read. The agent then begins to make \nsuggestions to the user, and is given feedback on its suggestions. NewT agents are not intended to \nremove human choice, but to represent an extension of the human's wishes: the aim is for the agent \nto be able to bring to the attention of the user articles of the type that the user has shown a \nconsistent interest in. Similar ideas have been proposed by McGregor, who imagines prescient \nagents-intelligent administrative assistants that predict our actions, and carry out routine or \nrepetitive administrative procedures on our behalf (McGregor, 1992). \nThere is much related work being done by the computer supported cooperative work (CSCW) \ncommunity. CSCW is informally defined by Haecker to be "computer assisted coordinated activity \nsuch as problem solving and communication carried out by a group of collaborating individuals" \n(Haecker, 1993, p. l). The primary emphasis of CSCW is on the development of (hardware and) \nsoftware tools to support collaborative human work-the term gruupware has been coined to \ndescribe such tools. Various authors have proposed the use of agent technology in groupware. For \nexample, in his participant systems proposal, Chang suggests systems in which humans collaborate \nwith not only other humans, but also with artificial agents (Chang, 1987). We refer the interested \nreader to the collection of papers edited by Haecker (1993) and the article by Greif (1994) for more \ndetails on CSCW, \n5.3 Information agents and cooperative information systems \nAn information agent is an agent that has access to at least one, and potentially many information \nsources, and is able to collate and manipulate information obtained from these sources to answer \nqueries posed by users and other information agents (the network of interoperating information \nsources are often referred to as intelligent and cooperative information systems (Papazoglou et al., \n1992)). The information sources may be of many types, including, for example, traditional \ndatabases as well as other information agents. Finding a solution to a query might involve an agent \naccessing information sources over a network. A typical scenario is that of a user who has heard \nabout somebody at Stanford who has proposed something called agent-oriented programming, \nThe agent is asked to investigate, and, after a careful search of various FTP sites, returns with an \nappropriate tychnical report, as well as the name and contact details of the researcher involved. A \nnumber of studies have been made of information agents, including a theoretical study of how \nagents are able to incorporate information from different sources (Levy et al., 1994; Gruber, 1991 ), \nas well a prototype system called IRA (information retrieval agent) that is able to search for loosely \n\nM. WOOLDRIDGE AND NICHOLAS JENNINGS 144 \nspecified articles from a range of document repositories (Voorhees, 1994). Another important \nsystem in this area is called Carnot (Huhns et al., 1992), which allows pre-existing and hetero\ngeneous database systems to work together to answer queries that are outside the scope of any of \nthe individual databases. \n5.4 Believable agents \nThere is ohvious potential for marrying agent technology with that of the cinema, computer games, \nand virtual reality. The Oz project 6 was initiated to develop: \n.. artistically interesting, highly interactive, simulated worlds . . to give users the experience of living in \n(not merely watching) dramatically rich worlds that include moderately competent, emotional agents" \n(Batesetal., 1992b,p. 1) \nTo construct such simulated worlds, one must first develop believable agents: agents that "provide \nthe illusion oflife, thus permitting the audience's suspension of disbelief" (Bates, 1994, p. 122). A \nkey component of such agents is emotion: agents should not be represented in a computer game or \nanimated film as the flat, featureless characters that appear in current computer games. They need \nto show emotions; to act and react in a way that resonates in tune with our empathy and \nunderstanding of human behaviour. The Oz group have investigated various architectures for \nemotion (Bates et al., 1992a), and have developed at least one prototype implementation of their \nideas (Bates, 1994). \n6 Concluding remarks \nThis paper has reviewed the main concepts and issues associated with the theory and practice of \nintelligent agents. It has drawn together a very wide range of material, and has hopefully provided \nan insight into what an agent is, how the notion of an agent can be formalised, how appropriate \nagent architectures can be designed and implemented, how agents can be programmed, and the \ntypes of applications for which agent-based solutions have been proposed. The subject matter of \nthis review is important because it is increasingly felt, both within academia and industry, that \nintel!igent agents wilt be a key technology as computing systems become ever more distributed, \ninterconnected, and open. In such environments, the ability of agents to autonomously plan and \npursue their actions and goals, to cooperate, coordinate, and negotiate with others, and to respond \nflexibly and intelligently to dynamic and unpredictable situations will lead to significant improve\nments in the quality and sophistication of the software systems that can be conceived and \nimplemented, and the application areas and problems which can be addressed. \nAcknowledgements \nMuch of this paper was adapted from the first author's 1992 PhD thesis (Wooldridge, 1992), and as \nsuch this work was supported by the UK Science and Engineering Research Council (now the \nEPSRC). We arc grateful to those people who read and commented on earlier drafts of this article, \nand in particular to the participants of the 1994 workshop on agent theories, architectures, and \nlanguages for their encouragement, enthusiasm, and helpful feedback. Finally, we would like to \nthank the referees of this paper for their perceptive and helpful comments. \nReferences \nAdorni, G and Poggi, A, 1993. "An object-oriented language for distributed artificial intelligence" Inter\nnational Journal of Man-Machine Studies 38 435-453. \nAgha, G, 1986. ACTORS: A Model of Concurrent Computation in Di:,tributed Systems. MIT Press. \nAgha, G, Wegner, P and Yonezawa, A (eds.), 1993. Research Directions in Concurrent Objec1-0riented \nProgramming. MIT Pre:.s. \n6 Not to be confused with the Oz programming language (Henz ct al., 1993). \n\nIntelligent agents: theory and practice 145 \nAgre, P and Chapman, D, 1987. "PENGI: An implementation of a theory of activity" In: Proceedings of the \nSixth National Conference on Artificial Intelligence (AAAI-87), pp 268-272, Seattle, WA. \nAllen, JF, 1984. "Towards a general theory of action and time" Artificial Intelligence 23 (2) 123--154. \nAllen, JF, Hendler, J and Tate, A (eds.), 1990. Readings in Planning. Morgan Kaufmann. \nAllen, JF, Kautz, H, Pelavin, Rand Tenenberg, J, 1991. Reasoning About Plans. Morgan Kaufmann. \nAmbros-Ingerson, J and Steel, S, 1988. "Integrating planning, execution and monitoring" In: Proceedings of \nthe Seventh National Conference on Artificial Intelligence (AAAI-88), pp 83-88, St. Paul, MN. \nAustin, JL, 1962. How to Do Things With Words. Oxford University Press. \nAy!ett, Rand Eustace, D, 1994. "Multiple cooperating robots-combining planning and behaviours'' In: SM \nDeen (ed) Proceedings of the 1993 Workshop on Cooperating Knowledge Based Systemv (CKBS-93), pp 3--\n11. DAKE Centre, University of Keele, UK. \nHaecker, RM (ed.) 1993. Readings in Groupware and Computer-Supported Cooperative Work. Morgan \nKaufmann. \nBarringer, H, Fisher, M, Gabbay, D, Gough, G and Owens, R, 1989. "MetateM: A framework for \nprogramming in temporal logic" In: REX Workshop on Stepwise Refinement of Di~trihwed System~: \nModels, Formalisms, Correctness (LNCS Volume 430) pp 94-129. Springer-Verlag. \nBarwise, J and Perry, J, 1983. Situations and Attitudes, MIT Press. \nBates, J, 1994. "The role of emotion in believable agents" Communications of the ACM 37 (7) 122-125. \nBates, J, Bryan Loyall, A and Scott Reilly, W, 1992a. "An architecture for action, emotion, and social \nbehaviour". Technical Report CMU-CS-92-144, School of Computer Science, Carnegie-Mellon Univer\nsity, Pittsburgh, PA. \nBates, J, Bryan Loyall, A and Scott Reilly, W, 19926. "Integrating reactivity, goals, and emotion in a broad \nagent". Technical Report CMU-CS-92-142, School of Computer Science, Carnegie-Mellon University, \nPittsburgh, PA. \nBell, J, 1995. "Changing attitudes". In: M Wooldridge and NR Jennings (eds.) Intelligent Agents: Theories, \nArchitectures, and Languages (LNAI Volume 890), pp 40--55, Springer-Verlag. \nBelnap, N, 1991. ''Backwards and forwards in the modal logic of agency'' Philosophy and Phenomenological \nResearch LI (4) 777-807. \nBelnap, N and Perloff, M, 1988. "Seeing to it that: a canonical form for agentives" Theoria 54175-199. \nBond, AH and Gasser, L (eds.) 1988. Readings in Distribwed Artificial Intelligence, Morgan Kaufmann. \nBratman, ME, 1987. Intentions, Plans, and Practical Reason, Harvard University Press. \nBratman, ME, 1990. "What is intention?" In: PR Cohen, JL Morgan and ME Pollack (eds.) Intentions in \nCommunication, pp 15-32, MIT Press. \nBratman, ME, Israel, DJ and Pollack, ME, 1988. "Plans and resource-bounded practical reasoning'' \nComputational Intelligence 4 349-355. \nBrooks, RA, 1986. "A robust layered control system for a mobile robot" IEEE Journal of Robotics and \nAulomation 2 (1) 14-23. \nBrooks, RA, 1990. "Elephants don't play chess" In: P Maes (ed.) Designing Aulonomous Agenl.~, pp 3-15, \nMIT Press. \nBrooks, RA, 1991a. "Intelligence without reason" In: Proceedings of the Twelflh International Joint \nConference on Artificial Intelligence (JJCAl-91), pp 569-595, Sydney, Australia. \nBrooks, RA, 19916. "Intelligence without representation" Artificial Intelligence 47 139-159. \nBurmeister, Band Sundermeyer, K. 1992. ''Cooperative problem solving guided by intentions and percep\ntion" In: E Werner and Y Demazeau (e<l~.) Decentralized Al 3-Proceedings of the Third European \nWorkshop on Modelling Auronomous Agents and Multi-Agent Worlds (MAAMAW-91), pp 77-92, \nElsevier. \nBussman, Sand Demazeau, Y, 1994. "An agent model combining reactive and cognitive capabilities" In: \nProceedings of the IEEE International Conference on Intelligent Robots and Systeml (IROS-94), Munich, \nGermany. \nCaste!franchi, C, 1990. "Social power" In: Y Demazeau and J-P MU!ler(eds.) Decentralized Al-Proceedings \nof the First European Workshop on Modelling Autonomous Agents in Multi-Agent Worlds (MAAMA W-\n8Y), pp 49--62, Elsevier. \nCastelfranchi, C, 1995. "Guarantees for autonomy in cognitive agent architecture" In: M Wooldridge and NR \nJennings (eds.) Intelligent Agents. Theories, Architectures, and Languages (LNAI Volume 890), pp 56--70, \nSpringer-Verlag. \nCastelfranchi, C, Miceli, Mand Cesta, A, 1992. 'Dcpcndcncc relations among autonomous agents" In: E \nWerner and 'Y Dema:ceau (eds.) Decentralized Al 3-Proceedings of the Third European Workshop on \nModeUing Awonomous Agents and Multi-Agent Worlds (MAAMA W-Yl), pp 215-231, Elsevier. \nCatach, L, 1988. "Norma! multimodal logics"' In: Proceedings of the Seventh National Conference on Anificial \nIntelligence (AAAJ-88), pp 491-495. St. Paul, MN. \n\nM. WOOLDRIDGE AND NICHOLAS JENNINGS 146 \nChaib-draa, B, Moulin, B, Mandiau, Rand Millot, P, 1992. "Trends in distributed artificial intelligence \nArtificial Intelligence Review 6 35-66. \nChang, E, 1987. "Participant systems" In: M Huhns (ed.) Distributed Artificial Intelligence, pp 311-340, \nPitman. \nChapman, D, 1987. "Planning for conjunctive goals" Artificial Intelligence 32 333--378. \nChapman, D and Agre, P, 1986. "Abstract reasoning as emergent from concrete activity" In: MP Georgeff \nand AL Lansky (eds.) Reasoning About Actions & Plans~Proceedings of the 1986 Workshop pp 411-424, \nMorgan Kaufmann. \nChel\\as, B, 1980. Modal Logic: An Introduction, Cambridge University Press. \nChu, D, 1993. "l.C. PROLOG JI: A language for implementing multi-agent systems" In: SM Deen (ed.) \nProceedings of the 1992 Workshop on Cooperating Knowledge Based Systems (CKBS-92), pp 61-74, \nDAKE Centre, University of Keele, UK. \nCohen, PR, Greenberg ML, Hart OM and Howe AE. 1989. "Trial by fire: Understanding the design \nrequirements for agents in complex environments" Al Magazine 10 (3) 32-48. \nCohen, PR and Levesque, HJ, 1990a. "'Intention is choice with commitment" Artificial Intelligence 42 213-\n261. \nCohen, PR and Levesque, HJ, 1990h. 'Rational interaction as the basis for communication" In: PR Cohen, J \nMorgan and ME Pollack (eds.) Intentions in Communication, pp 221-256. MIT Press. \nCohen, PR and Perrault, CR, 1979. 'Elements of a plan based theory of speech acts" Cognitive Science 3177-\n212. \nConnah, D and Wavish. P, 1990. 'An experiment in cooperation" In: Y Demazeau and J-P Miiller (eds.) \nDecentralized AI-Proceedings of the First European Workshop on Modelling Autonomous Agents in \nMulti-Agent Worlds (MAAMA W-89), pp 197-214, Elsevier. \nCutkosky, MR, Engelmorc, RS, Fikes. RE, Gcnesereth. MR, Gruber, T, Mark, WS, Tenenbaum, JM and \nWeber, JC, 1993. "PACT: An experiment in integrating concurrent engineering systems'' IEEE Computer \n26 (l) 28-37. \nDavies, NJ, 1993. Truth, Modality, and Action, PhD thesis, Department of Computer Science, University of \nEssex, Colchester, UK. \nDean, TL and Wellman, MP. 1991. Planning and Control, Morgan Kaufmann. \nDennett, DC, 1978. Brainstorm~, MIT Press. \nDennett, DC. 1987. The Intentional Stance, MIT Press. \ndes Rivieres. J and Levesque. HJ, 1986. ''The consistency of ~yntactical treatments of knowledge" In: JY \nHalpern ( ed,) Proceedings of the 1986 Conference on Theoretical Aspects of Reasoning About Knowledge, \npp 115-130, Morgan Kaufmann. \nDevlin, K, 1991. Logic and Information, Cambridge University Press. \nDongha, P, 1995 "Toward a formal model of commitment for rc~ource-bounded agent~" In: M Wooldridge \nand NR Jennings (eds.) Intelligent Agents: Theories, Architectures, and Languages ( LNAJ Volume 890), pp \n86--101. Springer-Verlag. \nDown~, J and Rcichgelt, II, 1991. "Integrating classical and rcm.:tive planning within an architecture for \nautonomous agents In: J Hertzberg (ed,) European Worhhop on Planning (LNAI Volume 522), pp 13--\n26. \nDoyle. J, Shoham, Y and Wellman, MP, 1991. ''A logic of relative desire" In: ZW Ras and M Zemankova \n(cd~.) Methodologies for Intelligent Systems-Sixth International Symposium, ISMIS-91 (LNAI Volume \n542). Springer-Verlag. \nEmerson EA, 1990. "Temporal and modal logic" In: J van Leeuwen (ed.) HandhookofThroretical Computer \nScience. pp 996--1072, Elsevier. \nEmerson, EA and Halpern, JY, 1986. "'Sometimes' and 'not never' revisited: on branching time versus linear \ntime temporal logic" Journal of the ACM 33 (1) 151-178. \nEtzioni, 0. Lesh, N and Segal, R, 1994. 'Building soft bots for UNIX'' In: 0 Etzioni (ed.) Software Agents\nPaper~ from the 1994 Spring Symposium (Technical Report SS-94-03), pp 9-16, AAAI Press. \nFagin, Rand Halpern, JY, 1985. "Belief, awarenes~. and limited reasoning" In: Proceedings of the Ninth \nInternational Joint Conference on Artificial Intelligence (IJCAl-85), pp 480--490. Los Angeles, CA. \nFagin. R, Halpern, JY and Yard!, MY, 1992. "What can machines know? on the properties of knowledge in \ndistributed system~ Journal of rhe A CM 39 (2) 328--376. \nFerguson IA. 1992a. Touring Machines: An Architecture for Dynamic, Rational, Mobile Agents, PhD thesis, \nClare Hall, University of Cambridge, UK. (Also available as Technical Report No. 273, University of \nCambridge Computer Laboratory.) \nFerguson. IA, 1992h. Towar<ls an architecture for adaptive, rational, mobile agents In: E Werner and Y \nDemazeau (eds.) Decentralized Al 3-Proceedings of the Third European Workshop on Modelling \nAutonomous Agents and Multi-Agent Worlds (MAAMA W-91), pp 249-262, Elsevier. \n\nIntelligent agents: theory and practice 147 \nFikes, RE and Nilsson. N, 1971. "STRIPS: A new approach to the application of theorem proving to problem \nsolving' Artificial Intelligence 5 (2) 189-208. \nFirby, JA. 1987. "An investigation into reactive planning in complex domains"' In: Proceedings of the Tenth \nInternational Joint Conference on Artificial Intelligence (IJCA/-87), pp 202-206, Milan, Italy. \nFischer, K, Kuhn. N, MUiler, HJ, MUiler, JP and Pischel, M, 1993. 'Sophisticated and distributed: The \ntransportation domain" In: Proceedings of the Fifth European Workshop on Modelling Autonomous \nAgents and Multi-Agent Worlds (MAAMA W-93), Neuchatel, Switzerland. \nFisher, M, 1994. "A survey ofConcurrcnt MetateM-the language and its applications"" In: DM Gabbay and \nHJ Ohlbach (eds.) Temporal Logic-Proceedings of the First International Conference (LNAJ Volume \n827). pp 480-505, Springer-Verlag. \nFisher, M. 1995. "Representing and executing agent-based systems In: M Wooldridge and NR Jennings \n(eds.) Intelligent Agents: Theories, Architectures, and Languages (LNAJ Volume 890), pp 307-323, \nSpringer-Verlag. \nFisher, M and Wooldridge, M, 1993. "Specifying and verifying distributed intelligent systems" In: M \nFilgueiras and L Damas (eds.) Progress in Artificial Intelligence-Sixth Portuguese Conference on Anificial \nIntelligence (LNAI Volume 727), pp 13-28. Springer-Verlag. \nGalliers. JR, 1988a. "A strategic framework for multi-agent cooperative dialogue" In: Proceedings of the \nEighth European Conference on Artificial Intelligence (ECAl-88), pp 415-420, Munich, Germany. \nGalliers, JR, 1988b. A Theoretical Framework for Computer Models of Cooperative Dialogue, Acknowledg\ning Multi-Agent Conflict. PhD thesis, Open University, UK. \nGasser, L, 1991. "Social conceptions of knowledge and action: DAI foundations and open systems semantics" \nArtificial Intelligence 47 107-138. \nGasser, L, Braganza, C and Hermann, N, 1987. "MACE: A flexible testbed for distributed AI research" In: \nM Huhns (ed.) Distributed Artificial Intelligence, pp 119-152, Pitman. \nGasser, Land Briot, JP, 1992. "Object-based concurrent programming and DAI" In: Distributed Artificial \nIntelligence: Theory and Praxis. pp 81-108, Kluwer Academic. \nGeissler, C and Konolige. K, 1986. "A resolution method for quantified modal logics of knowledge and \nhelief' In: JY Halpern (ed.) Proceedings of the 1986 Conference on Theoretical Aspects of Reasoning About \nKnowledge, pp 309-324, Morgan Kaufmann. \nGenescreth, MR and KetchpeL SP, 1994. "Software agents" Communications of the ACM 37 (7) 48-53. \nGenesereth. MR and Nils~on, N, 1987. Logical Foundations of Artificial Intelligence, Morgan Kaufmann. \nGcorgeff, MP, 1987. "Planning" Annual Review of Computer Science 2 359-400. \nGeorgeff, MP and lngrand, FF, 1989. "Decision-making in an embedded reasoning system" In: Proceedings \nof the Eleventh International Joint Conference on Artificial Intelligence ( IJCAl-89), pp 972-978, Detroit, \nML \nGcorgcff. MP and Lansky, AL (eds.) 1986. Reasoning About Actions & Plans-Proceedings of the 1986 \nWorkshop, Morgan Kaufmann. \nGeorgeff, MP and Lansky, AL, 1987. "Reactive reasoning and planning In: Proceedings of the Sixth National \nConference on Artificial Intelligence (AAAJ-87), pp 677-682, Seattle, WA. \nGinsberg, M. 1993. Essentials of Artificial Intelligence, Morgan Kaufmann. \nGmytrasiewicz, P and Durfee. EH, 1993. "Elements of a utilitarian theory of knowledge and action" In: \nProceedings of the Thirteenth International Joint Conference on Artificial Intelligence (IJCA/-93), pp 396--\n402, ChambCry. France. \nGoldblatt, R. 1987. Logics of Time and Computation, Centre for the Study of Language and lnformation\nLccturc Notes Series. (Distributed by Chicago University Press.) \nGoldman, RP and Lang. RR, 1991 "Intentions in time"', Technical Report TUTR 93-101, Tulane University. \nGoodwin, R. 1993. "Formalizing properties of agents", Technical Report CMU-CS-93-159, School of \nComputer Science. Carnegie-Mellon University, Pittsburgh, PA. \nGreif. I, 1994. "Desktop agent~ in group-enabled products" Communications of the ACM 37 (7) 100-105. \nGrosz, BJ and Sidner, CL, 1990. ''Plans for discourse" In: PR Cohen, J Morgan and ME Pollack (eds.) \nIntentions in Communication. pp 417-444, MIT Press. \nGruber. TR, 1991. "The role of common ontology in achieving sharable, reusable knowledge bases' In: R \nFikes and E Sandcwall (eds.) Proceedings of Knowledge Representation and Reasoning (KR&R-91), \nMorgan Kaufmann. \nGuha, RV and Lcnat, DB, 1994. "Enabling agents to work together" Communications of the ACM 37 (7) 127-\n142. \nHaas, A, 1986. "A syntactic theory of belief and knowledge" Artificial Intelligence 28 (3) 245-292. \nHaddadi, A. 1994. "A hybrid architecture for multi-agent systems" In: SM Deen (ed.) Proceedings of the 1993 \nWorkshop 011 Cooperating Knowledge Based Systems (CKBS-93), pp 13-26. DAKE Centre, University of \nKeele, UK. \n\nM. WOOLDRIDGE AND NICHOLAS JENNINGS 148 \nHalpern, JY, 1986. "Reasoning about knowledge: An overview" In: JY Halpern (ed.) Proceedings of the 1986 \nConference on Theoretical Aspects of Reasoning About Knowledge, pp 1-18, Morgan Kaufmann. \nHalpern, JY, 1987. "Using reasoning about knowledge to analyze distributed systems" Annual Review of \nComputer Science 2 37--68. \nHalpern, JY and Moses, Y, 1992. "A guide to completeness and complexity for modal logics of knowledge and \nbelief' Artificial Intelligence 54 319-379. \nHalpern, JY and Vardi, MY, 1989. "The complexity of reasoning about knowledge and time. I. Lower \nbounds" Journal of Computer and System Sciences 38 195-237. \nHare!, D, 1984. "Dynamic logic" In: D Gabbay and F Guenther (eds.) Handbook of Philosophical Logic \nVolume ll-Extensions of Classical Logic, pp 497-604, Reidel. \nHaugeneder, H, 1994. IMAGINE final project report. \nHaugeneder, Hand Steiner, D, 1994. "A multi-agent approach to cooperation in urban traffic" In: SM Deen \n(ed.) Proceedings of the 1993 Workshop on Cooperating Knowledge Based Systems (CKBS-93), pp 83-98, \nDAKE Centre, University of Keele, UK. \nHaugeneder, H, Steiner, D and McCabe, FG, 1994. "IMAGINE: A framework for building multi-agent \nsystems" In: SM Deen (ed.) Proceedings of the 1994 Jnternational Working Conference on Cooperating \nKnowledge Based Systems (CKBS-94), pp 31--64, DAKE Centre, University of Keele, UK. \nHayes-Roth, B, 1990. "Architectural foundations for real-time performance in intelligent agents" The \nJournal of Real-Time Systems 2 99-125. \nHendler, J (ed.) 1992. Artificial intelligence Planning: Proceedings of the First International Conference, \nMorgan Kaufmann. \nHenz, M, Smolka, G and Wuertz, J, 1993. "Oz-a programming language for multi-agent systems" ln: \nProceedings of the Thirteenth international Joint Conference on Artificial Intelligence ( IJCAI-93), pp 404-\n409, ChambCry, France. \nHewitt, C, 1977. "Viewing control structures as patterns of passing messages" Artificial intelligence 8 (3) 323-\n364. \nHintikka, J, 1962. Knowledge and Belief, Cornell University Press. \nHoulder, V, 1994. "Special agents" In: Financial Times, 15 August, p 12. \nHuang, J, Jennings, NR and Fox. J, 1995. "An ag-ent architecture for distributed medical care" In: M \nWooldridge and NR Jennings (eds.) Intelligent Agents: Theories, Architectures, and Languages (LNA! \nVolume 890), pp 219-232, Springer-Verlag. \nHughes, GE and Cresswell, MJ, 1968. lntroduction to Modal Logic, Methuen. \nHuhns, MN, Jacobs, N, Ksiezyk, T, Shen, WM, Singh, MP and Cannata, PE, 1992. "Integrating enterprise \ninformation models in Carnot" In: Proceedings of the International Conference on Intelligent and \nCooperative information Systems, pp 32-42, Rotterdam, The Netherlands. \nIsrael, DJ, 1993. "The role(s) of logic in artificial intelligence" In: DM Gabbay, CJ Hogger and JA Robinson \n(eds.) Handbook of Logic in Artificial Intelligence and Logic Programming, pp 1-29, Oxford University \nPress. \nJennings, NR, 1992. "On being responsible" In: E Werner and Y Demazeau (eds.) Decentralized Al]\nProceedings of the Third European Workshop on Modelling Autonomous Agents and Multi-Agent Worlds\n(MAAMA W-91), pp 93-102, Elsevier. \nJennings. NR, 1993a. "Commitments and conventions: The foundation of coordination in multi-agent \nsystems" Knowledge Engineering Review 8 (3) 223-250. \nJennings, NR, 1993b. "Specification and implementation of a belief desire joint-intention architecture for \ncollaborative problem solving" Journal of intelligent and Cooperative Information Systems 2 (3) 289-318. \nJennings, NR, 1995. "Controlling cooperative problem solving in industrial multi-agent systems using joint \nintentions" Artificial Intelligence 14 (2) (to appear). \nJennings, NR. Varga, LZ, Aarnts, RP, Fuchs, J and Skarek, P, 1993. "Transforming standalone expert \nsystems into a community of cooperating agents'' International Journal of Engineering Applications of \nArtificial Intelligence 6 ( 4) 317-331. \nKaelbling, LP, 1986. "An architecture for intelligent reactive systems" In: MP George ff and AL Lansky (eds.) \nReasoning About Actions and Plans-Proceedingofthe 1986 Workshop, pp 395-410, Morgan Kaufmann. \nKaelbling, LP, 1991. 'A situated automata approach to the design of embedded agents" SIG ART Bulletin 2 \n(4) 85-88. \nKaelbling, LP and Roscnschein, SJ, 1990. "Action and planning in embedded agents" In: P Maes (ed.) \nDesigning Autonomous Agents, pp 35-48, MIT Press. \nKinny, D, Ljungberg, M, Rao, AS, Sonenberg, E, Tidhar, G and Werner, E, 1992. "Planned team activity" \nIn: C Castelfranchi and E Werner (eds.) Artificial Social Systems-Selected Papers from the Fourth \nEuropean Workshop on Modelling Autonomous Agents and Multi-Agent Worlds, MAAAMA W-92 (LNA! \nVolume 830), pp 226-256, Springer-Verlag. \n\nIntelligent agents: theory and practice 149 \nKiss, G and Reichgelt, H, 1992. "Towards a semantics of desires" In: E Werner and Y Demazeau (eds.) \nDecentralized Al 3-Proceedings of the Third European Workshop on Modelling Autonomous Agents and \nMulti-Agent Worlds (MAAMAW-91), pp 115-128, Elsevier. \nKonolige, K, 1982. "A first-order formalization of knowledge and action for a multi-agent planning system" \nIn: JE Hayes, D Michie and Y Pao (eds.) Machine Intelligence JO, pp 41-72, Ellis Horwood. \nKonolige, K, 1986a. A Deduction Model of Belief, Pitman. \nKonolige, K, 1986b. "What awareness isn't: A sentential view of implicit and explicit belief (position paper)" \nIn: JY Halpern (ed.) Proceedings of the 1986 Conference on Theoretical Aspects of Reasoning About \nKnowledge, pp 241-250, Morgan Kaufmann. \nKonolige, Kand Pollack, ME, 1993. "A representationalist theory of intention" In: Proceedings of the \nThirteenth International Joint Conference on Artificial Intelligence (IJCAJ-93), pp 390--395, Chambefy, \nFrance. \nKraus, Sand Lehmann, D (1988) "Knowledge, belief and time" Theoretical Computer Science 58 155-174. \nKripke, S, 1963. "Semantical analysis of modal logic" Zeitschrift fUr Mathematische Logik und Grundlagen \nder Mathematik 9 67-96. \nLakemeyer, G, 1991. "A computationally attractive first-order logic of belief" In: JELIA-90: Proceedings of \nthe European Workshop on Logics in AI (LNAI Volume 478), pp 333-347, Springer-Verlag. \nLesperance, Y, 1989. "A formal account of self knowledge and action" In: Proceedings of the Eleventh \nImernational Joint Conference on Artificial Intelligence (IJCAI-89), pp 868-874, Detroit, Ml. \nLevesque, HJ, 1984. "A logic of implicit and explicit belief" In: Proceedings of the Fourth National Conference \non Artificial Intelligence (AAAI-84), pp 198-202, Austin, TX. \nLevesque, HJ, Cohen, PR and Nunes, JHT, 1990. "On acting together" In: Proceedings of the Eighth National \nConference on Artificial Intelligence (AAAJ-90), pp 94-99, Boston, MA. \nLevy, A Y, Sagiv, Y and Srivastava, D, 1994. "Towards efficient information gathering agents" In: 0 Etzioni \n(ed.) Software Agents-Papers from the 1994 Spring Symposium (Technical Report SS-94-03), pp 64-70, \nAAAI Press. \nMack, D, 1994. "A new formal model of belief" In: Proceedings of the Eleventh European Conference on \nArtificial Intelligence (ECAI-94), pp 573-577, Amsterdam, The Netherlands. \nMaes, P, 1989. "The dynamics of action selection" In: Proceedings of the Eleventh International Joint \nConference on Artificial Intelligence (IJCAI-89), pp 991-997, Detroit, MI. \nMaes, P (ed.) 1990a. Designing Autonomous Agents, MIT Press. \nMaes, P, 1990b. "Situated agents can have goals" In: P Maes (ed.) Designing Autonomous Agents, pp49-70, \nMIT Press. \nMaes, P, 1991. "The agent network architecture (ANA)" SIG ART Bulletin 2 (4) 115-120. \nMaes, P, 1994a. "Agents that reduce work and information overload" Communications of the ACM 37 (7) 31-\n40. \nMacs, P, 1994b. "Social interface agents: Acquiring competence by learning from users and other agents" In: \n0 Etzioni (ed.) Software Agents-Papers from the J 994 Spring Symposium (Technical Report SS-94-03), pp \n71-78, AAA! Press. \nMcCabe, FG and Clark, KL, 1995. "April-agent process interaction language" In: M Wooldridge and NR \nJennings (eds.) Intelligent Agents: Theories, Architectures, and Languages (LNAI Volume 890), pp 324-\n340, Springer-Verlag. \nMcCarthy, J, 1978. "Ascribing mental qualities to machines." Technical report, Stanford University Al Lab., \nStanford, CA 94305. \nMcGregor, SL, 1992. "Prescient agents" In: D Coleman (ed.) Proceedings of Groupware-92, pp 228-230. \nMontague, R, 1963. "Syntactical treatments of modality, with corollaries on reflexion principles and finite \naxiomatizations" Acta Philosophica Fennica 16153-167. \nMoore, RC, 1990. '"A formal theory of knowledge and action" In: JF Allen, J Hendler and A Tate (eds.) \nReadings in Planning, pp 480-519, Morgan Kaufmann. \nMorgenstern, L, 1987. "Knowledge preconditions for actions and plans" In: Proceedings of the Tenth \nInternational Joint Conference on Artificial Intelligence (JJCAI-87), pp 867-874, Milan, Italy. \nMori, K, Torikoshi, H, Nakai, Kand Masuda, T, 1988. "Computer control system for iron and steel plants" \nHitachi Review 37 (4) 251-258. \nMorley, RE and Schelberg, C, 1993. "An analysis of a plant-specific dynamic scheduler' In: Proceedings of the \nNSF Workshop on Dynamic Scheduling, Cocoa Beach, Florida. \nMukhopadhyay, U, Stephens, Land Huhns, M, 1986. "An intelligent system for document retrieval in \ndistributed office environments'' Journal of the American Society for Information Science 37 123-135. \nMillier, JP, 1994. "A conceptual model for agent interaction" In: SM Deen (ed.) Proceedings of the Second \nInternational Working Conference on Cooperating Knowledge Based Systems (CKBS-94), pp 213-234, \nDAKE Centre, University of Keele, UK. \n\nM. WOOLDRIDGE AND N[CHOLAS JENNINGS 150 \nMUiler, JP and Pischel, M, 1994. "Modelling interacting agents in dynamic environments .. In: Proceedings of \nthe Eleventh European Conference on Artificial Intelligence (ECAI-94), pp 709-713, Amsterdam, The \nNetherlands. \nMi.i\\ler, JP, Pischcl, M and Thiel, M, 1995. "Modelling reactive behaviour in vertically layered agent \narchitectures" In: M Wooldridge and NR Jennings (eds.) Intelligent Agents: Theories, Architectures, and \nLanguages (LNAI Volume 890), pp 261-276, Springer-Verlag. \nNewell, A and Simon, HA, 1976. "Computer science as empirical enquiry" Communications of the ACM 19 \n113-126. \nNilsson, NJ, 1992. "Towards agent programs with circuit semantics", Technical Report STAN-CS-92-1412, \nComputer Science Department, Stanford University, Stanford, CA 94305. \nNorman, TJ and Long, D, 1995. "Goal creation in motivated agents" In: M Wooldridge and NR Jennings \n(eds.) Intelligent Agents: Theories, Architectures, and Languages (LNAI Volume 890), pp 277-290, \nSpringer-Verlag. \nPapazoglou, MP, Laufman, SC and Sel\\is, TK, 1992. "An organizational framework for cooperating \nintelligent information systems" Journal of Intelligent and Cooperative Information Systems 1 (1) 169-202. \nParunak, HVD, 1995. "Applications of distributed artificial intelligence in industry'" In: GMP O'Hare and \nNR Jennings (eds.) Foundations of Distributed Al, John Wiley. \nPatil, RS, Fikes, RE, Patel-Schneider, PF, McKay, D, Finin, T, Gruber, T and Neches, R, 1992. "The \nDARPA knowledge sharing effort: Progress report" In: C Rich, W Swartout and B Nebel (eds.) \nProceedings of Knowledge Representation and Reasoning (KR&R-92), pp 777-788. \nPerlis, D, 1985. "Languages with self reference I: Foundations" Artificial Intelligence 25 301-322. \nPerlis, D, 1988. "Languages with self reference II: Knowledge, belief, and modality" Artificial Intelligence 34 \n179-212. \nPerloff, M, 1991. "STIT and the language of agency" Synthese 86 379-408. \nPoggi, A, 1995. "DAISY: An object-oriented system for distributed artificial intelligence" In M Wooldridge \nand NRJennings (eds.) Intelligent Agents: Theories, Architectures, and Languages (LNA!Volume890). pp \n341-354, Springer-Verlag. \nPollack, ME and Ringuette, M, 1990. "Introducing the Tileworid: Experimentally evaluating agent architec\ntures" In: Proceedings of the Eighth National Conference on Artificial Intelligence (AAAl-90), pp 183-189, \nBoston, MA. \nRao, AS and Georgeff, MP, 1991a. '"Asymmetry thesis and side-effect problems in linear time and branching \ntime intention logics'" In: Proceedings of the Twelfth International Joint Conference on Artificial Intelligence \n(IJCAl-91), pp 498-504, Sydney, Australia. \nRao, AS and Georgeff, MP, 1991b. "Modeling rational agents within a BDI-architccture In: R Fikes and E \nSandewal! (eds.) Proceedings of Knowledge Representation and Reasoning (KR&R-9/), pp 473-484, \nMorgan Kaufmann. \nRao, AS and Georgeff, MP, 1992a. "An abstract architecture for rational agents"' In: C Rich, W Swartout and \nB Nebel (eds.) Proceedings of Knowledge Representation and Reasoning (KR&R-92), pp 439-449. \nRao, AS and Georgeff, MP, 1992b. "Social plans: Preliminary report" In: E Werner and Y Demazeau (eds.) \nDecentralized Al 3-Proceedings of the Third European Workshop on Modelling Autonomous Agents and \nMulti-Agent Worlds (MAAMA W-9I), pp 57-76, Elsevier. \nRao, AS and Georgeff, MP, 1993. "A model-theoretic approach to the verification of situated reasoning \nsystems" In: Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence (IJCAI-\n93), pp 318-324, Chambery, France. \nReichgclt, H, 1989a "A comparison of first-order and modal logics of time" In: P Jackson, H Reichge!t and F \nvan Harmclen (eds.) Logic Based Knowledge Representation, pp 143-176, MIT Press. \nReichgelt. H, 198%. "Logics for reasoning about knowledge and belief" Knowledge Engineering Review4 (2) \n119-139. \nRosenschein, JS and Genesereth, MR, 1985 "Deals among rational agents In: Proceedings of the Ninth \nInternational Joint Conference on Anificial Intelligence (IlCAl-85), pp 91-99, Los Angeles, CA \nRosenschein, S, 1985. "Formal theories of knowledge in AI and robotics'' New Generation Computing, pp \n345-357. \nRosenschein, S and Kae\\bling, LP, 1986. 'Toe synthesis of digital machines with provable epistemic \nproperties" In: JY Halpern (ed.) Proceedings of the 1986 Conference on Theoretical Aspects of Reasoning \nAbout Knowledge, pp 83-98, Morgan Kaufmann. \nRussell, SJ and Wefald, E, 1991 Do the Right Thing-Studies in Limited Rationality. MIT Press. \nSacerdoti. E, 1974. "Planning in a hierarchy of abstraction spaces" Artificial Intelligence 5 115-135. \nSacerdoti, E, 1975. '"The non-linear nature of plans'' In: Proceedings of the Fourth International Joint \nConference on Artificial Intelligence ( IlCAl-75), pp 206--214. Stanford, CA. \nSadek. MD, 1992. 'A study in the logic of intention" In: C Rich, W Swartout and B Nebel (eds.) Proceedings \nof Knowledge Representation and Reasoning (KR&R-92). pp 462-473. \n\nIntelligent agents: theory and practice 151 \nSargent, P, 1992. "Back to school for a brand new ABC" In: The Guardian, 12 March, p 28. \nSchoppeN;, MJ, 1987. "Universal plans for reactive robots in unpredictable environments" In: Proceedings of \nthe Tenth International Joint Conference on Artificial Intelligence ( /JCAl-87), pp 1039-1046, Milan, Italy. \nSchwuttke, UM and Quan, AG, 1993. 'Enhancing performance of cooperating agents in real-time diagnostic \nsystems" In: Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence (JJCAl-\n93), pp 332-337, Chambfay, France. \nSearle, JR, 1969. Speech Acts: An Essay in the Philosophy of Language, Cambridge University Press. \nSeel, N, 1989. Agent Theories and Architectures, PhD thesis, Surrey University, Guildford, UK. \nSegerbcrg, K, 1989. "Bringing it about" Journal of Philosophical Logic 18 327-347. \nShardlow, N, 1990. "Action and agency in cognitive science", Master's thesis, Department of Psychology, \nUniversity of Manchester, Oxford Road, Manchester M13 9PL, UK. \nShoham, Y, 1988. Reasoning About Change: Time and Causation from the Standpoint of Artificial Intelligence, \nMIT Press. \nShoham, Y, 1989. "Time for action: on the relation between time, knowledge and action" In: Proceedings of \nthe Eleventh /nternationalJointConferenceonArtificial Intelligence (lJCAl-89), pp 954-959, Detroit, Ml. \nShoham, Y, 1990. "Agent-oriented programming", Technical Report STAN-CS-1335-90, Computer Science \nDepartment, Stanford University, Stanford, CA 94305. \nShoham, Y, 1993. "Agent-oriented programming" Artificial Intelligence 60 (1) 51-92. \nSingh, MP, l990a. 'Group intentions" In: Proceedings of the Tenth International Workshop on Distributed \nArtificial Intelligence (IWDAI-90). \nSingh, MP. 1990b. "Towards a theory of situated know-how" In: Proceedings of the Ninth European \nConference on Artificial Intelligence (ECAJ-90), pp 604-609, Stockholm, Sweden. \nSingh, MP, 1991a. "Group ability and structure" In: Y Dcmazeau and JP MUiler (eds.) Decentralized Al 2-\nProceedings of the Second European Workshop on Modelling Autonomous Agents and Multi-Agent Worlds \n(MAAMAW-90), pp 127-146, Elsevier. \nSingh, MP, 1991b 'Towards a formal theory of communication for multi-agent systems" In: Proceedings of rhe \nTwelfth International Joint Conference on Artificial Intelligence (lJCAI-91), pp 69-74, Sydney, Australia. \nSingh, MP, 1992. "A critical examination of the Cohen-Levesque theory of intention" In: Proceedings of the \nTenth European Conference on Artificial Intelligence (ECAl-92), pp 364-368. Vienna, Austria. \nSingh, MP, 1994. Multiagent Systems: A Theoretical Framework for Intentions, Know-How, and Communi\ncations (LNAI Volume 799), Springer-Verlag. \nSingh, MP and Asher, NM, 1991. "Towards a formal theory of intentions" In: Logics in Al-Proceedings of \nthe European Workshop JELIA-90 (LNA! Volume 478), pp 472-486, Springer-Verlag. \nSmith, RG, 1980. A Framework for Distributed Problem Solving, UMI Research Press. \nSteeb, R, Cammarata S, Hayes-Roth FA, Thorndyke PW and Wesson RB, 1988. "Distributed intelligence for \nair fleet control" In: AH Bond and L Gasser (eds.) Readings in Distributed Artificial Intelligence, pp 90-\n101, Morgan Kaufmann. \nSteels, L, 1990. "Cooperation between distributed agents through self organization" In: Y Demazeau and JP \nMUiler (eds.) Decentralized Al-Proceedings of the First European Workshop on Modelling Autonomous \nAgents in Multi-Agent Worlds (MAAMA W-89), pp 175-196, Elsevier. \nThomas, SR, 1993. PLACA, an Agent Oriented Programming Language, PhD thesis, Computer Science \nDepartment, Stanford University, Stanford, CA 94305. (Available as technical report STAN-CS-93-\n1487). \nThoma~, SR, Shoham Y, Schwartz A and Kraus S, 1991. "Preliminary thoughts on an agent description \nlanguage" International Journal of Intelligent Systems 6 497-508. \nThomason, R, 1980. "A note on syntactical treatments of modality" Synthese 44 391-395. \nTurner, R, 1990. Truth and Modality for Knowledge Representation, Pitman. \nVarga, LZ, Jennings, NR and Cockburn, D, 1994. "Integrating intelligent systems into a cooperating \ncommunity for electricity distribution management" International Journal of Expert Systems with Appli\ncations 1 (4) 563-579. \nVere, Sand Bickmore, T, 1990. "A basic agent" Computational Intelligence 6 41-60. \nVoorhees, EM, 1994. 'Software agents for information retrieval" In: 0 Etzioni (ed.) Software Agents\nPapers from the 1994 Spring Symposium (Technical Report SS-94-03), pp 126--129, AAAI Press. \nWainer, J, 1994. "Yet another semantics of goals and goal priorities" In: Proceedings of the Eleventh \nEuropean Conference on Artificial Intelligence (ECIA-94), pp 269-273, Amsterdam, The Netherlands. \nWavish, P, 1992. "Exploiting emergent behaviour in multi-agent systems" In: E Werner and Y Demazeau \n(eds.) Decentralized Al 3-Proceedings of the Third European Workshop on Modelling Autonomous \nAgents and Multi-Agent Worlds (MAAMA W-91), pp 297-310, Elsevier. \nWavish, P and Graham, M, 1995. "Role, skills, and behaviour: a situated action approach to organising \nsystems of interacting agents" In: M Wooldridge and NR Jennings (eds.) Intelligent Agents: Theories, \nArchi1ectures, and Languages (LNAI Volume 890), pp 371-385, Springer-Verlag. \n\nM. WOOLDRIDGE AND NICHOLAS JENNINGS 152 \nWeerasooriya, D, Rao, A and Ramamohanarao, K, 1995. "Design of a concurrent agent-oriented language" \nIn: M Wooldridge and NR Jennings (eds.) Intelligent Agents: Theories, Architectures, and Languages \n(LNAJ Volume 890), pp 386----402, Springer-Verlag. \nWeihmayer, Rand Velthuijsen, H, 1994. "Application of distributed AI and cooperative problem solving to \ntelecommunications" In: J Liebowitz and D Prcreau (eds.) Al Approaches to Telecommunications and \nNetwork Management, IOS Press. \nWerner, E, 1988. "Toward a theory of communication and cooperation for multiagent planning" In: MY \nVar di (ed.) Proceedings of the Second Conference on Theoretical Aspects of Reasoning About Knowledge, \npp 129-144, Morgan Kaufmann. \nWerner, E, 1989. "Cooperating agents: A unified theory of communication and social structure" lq: L Gasser \nand M Huhns (eds.) Distributed Artificial Intelligence Volume 11, pp 3---36, Pitman. \nWerner, E, 1990. "Wh.atcan agents do together: A semantics of cooperative ability" In: Proceedings of the \nNinth European Conference on Artificial Intelligence (ECAJ-90), pp 694-701, Stockholm, Sweden. \nWerner, E, 1991. "A unified view of information, intention and ability" In: Y Demazeau and JP Miiller (eds.) \nDecentralized Al 2-Proceedings of the Second European Workshop on Modelling Autonomous Agents \nand Multi-Agent Worlds (MAAMA W-90), pp 109-126, Elsevier. \nWhite, JE, 1994. "Telescript technology: The foundation for the electronic marketplace'', White paper, \nGeneral Magic, Inc., 2465 Latham Street, Mountain View, CA 94040. \nWilkins, D, 1988. Practical Planning: Extending the Classical Al Planning Paradigm, Morgan Kaufmann. \nWittig, T (ed.) 1992. ARCH ON: An Architecture for Multi-Agent Systems, Ellis Horwood. \nWood, S, 1993. Planning and Decision Making in Dynamic Domains, Ellis Horwood. \nWooldridge, M, 1992. The Logical Modelling of Computational Multi-Agent Systems, PhD thesis, Depart\nment of Computation, UMIST, Manchester, UK. (Also available as Technical Report MMU-DOC-94-01, \nDepartment of Computing, Manchester Metropolitan University, Chester Street, Manchester, UK.) \nWooldridge, M, 1994. "Coherent social action" In: Proceedings of the EleJ.'enth European Conference on \nArtificial Intelligence (ECAI-94), pp 279-283, Amsterdam, The Netherlands. \nWooldridge, M, 1995. "This is MYWORLD: The logic of an agent-oriented testbed for DAI" In: M \nWooldridge and NR Jennings (eds.) Intelligent Agents: Theories, Architectures, and Languages (LNAI \nVolume 890), pp 160-178, Springer-Verlag. \nWooldridge, M and Fisher M, 1992. "A first-order branching time logic of multi-agent systems" In: \nProceedings of the Tenth European Conference on Artificial Intelligence (ECAl-92), pp 234-238, Vienna, \nAustria. \nWooldridge, Mand Fisher M, 1994. "A decision procedure for a temporal belief logic" In: OM Gabbay and \nHJ Ohlbach. (eds.) Temporal Logic-Proceedings of the First International Conference (LNAI Volume \n827), pp 317-331, Springer-Verlag. \nWooldridge, M and Jennings NR, 1994. "Formalizing the cooperative problem solving process" In: \nProceedings of the Thirteenth International Workshop on Distributed Artificial Intelligence (IWDAJ-94), pp \n403---417, Lake Quinalt, WA. \nYonczawa, A (ed.) 1990. ABCL-An Object-Oriented Concurrent System, MIT Press. 	The Knowledge Engineering Review, Vol. 10:2, 1995, 115-152 \nIntelligent agents: theory and practice \nMICHAEL WOOLDRIDGE 1 and NICHOLAS R. JENNINGS 2 \n1 Deparrmenr of Compwing. Manche.,ter Metropolitan Univeni1y, Chester Street, Manches1er MI 5GD, UK \n(M. Wooldridge(ri)doc.mmu.ac.uk) \n2 nepartmmt of Electronic F.ngineering, Queen Mary & Westfield College, Mile End Road, London El 4NS, UK \n( N. R .JennmgI(0.1qm w. ac. uk) \nAbstract \nThe concept of an agent has become important in both artificial 	en	0.9	uploaded	24677	163745	2025-11-15 16:08:54.027695	2025-11-15 16:08:54.027699	\N	1	\N	\N	\N	1	original	\N	\N	\N	\N	\N	{}	47e80830-d806-4d8e-acb4-0f9b6be0a1ee
175	Black's Law Dictionary 11th Edition (2019) - Agent	file	reference	legal_dictionary	pdf	AGENT.pdf	uploads/f347cafa9db74feb_AGENT.pdf	166882	{"year": 2019, "discipline": "Law", "upload_order": 4}	AGENT, Black's Law Dictionary (11th ed. 2019)\n  2024 Thomson Reuters. No claim to original U.S. Government Works. 1\nBlack's Law Dictionary (11th ed. 2019), agent\nAGENT\nBryan A. Garner, Editor in Chief\nPreface | Guide | Legal Maxims | Bibliography\nagent (15c)  1. Something that produces an effect <an intervening agent>. See cause (1); electronic agent. 2. Someone who is\nauthorized to act for or in place of another; a representative <a professional athlete's agent>.  Also termed commissionaire.\nSee agency. Cf. principal, n. (1); employee.\nGenerally speaking, anyone can be an agent who is in fact capable of performing the functions involved. The agent normally\nbinds not himself but his principal by the contracts he makes; it is therefore not essential that he be legally capable to contract\n(although his duties and liabilities to his principal might be affected by his status). Thus an infant or a lunatic may be an agent,\nthough doubtless the court would disregard either's attempt to act if he were so young or so hopelessly devoid of reason as to\nbe completely incapable of grasping the function he was attempting to perform. Floyd R. Mechem, Outlines of the Law of\nAgency 89 (Philip Mechem ed., 4th ed. 1952).\nThe etymology of the word agent or agency tells us much. The words are derived from the Latin verb, ago, agere; the noun\nagens, agentis. The word agent denotes one who acts, a doer, force or power that accomplishes things. Harold Gill Reuschlein\n& William A. Gregory, The Law of Agency and Partnership  1, at 23 (2d ed. 1990).\n- agent not recognized. Patents. A patent applicant's appointed agent who is not registered to practice before the U.S. Patent\nand Trademark Office.  A power of attorney appointing an unregistered agent is void. See patent agent.\n- agent of necessity. (1857) An agent that the law empowers to act for the benefit of another in an emergency.  Also termed\nagent by necessity.\n- apparent agent. (1823) Someone who reasonably appears to have authority to act for another, regardless of whether actual\nauthority has been conferred.  Also termed ostensible agent; implied agent.\n- associate agent. Patents. An agent who is registered to practice before the U.S. Patent and Trademark Office, has been\nappointed by a primary agent, and is authorized to prosecute a patent application through the filing of a power of attorney.  An\nassociate agent is often used by outside counsel to assist in-house counsel. See patent agent.\n- bail-enforcement agent. See bounty hunter.\n- bargaining agent. (1935) A labor union in its capacity of representing employees in collective bargaining.\n- broker-agent. See broker.\n- business agent. See business agent.\n- case agent. See case agent.\n- clearing agent. (1937) Securities. A person or company acting as an intermediary in a securities transaction or providing\nfacilities for comparing data regarding securities transactions.  The term includes a custodian of securities in connection with\nthe central handling of securities. Securities Exchange Act  3(a)(23)(A) (15 USCA  78c(a)(23)(A)).  Also termed clearing\nagency.\n- closing agent.  (1922) An agent who represents the purchaser or buyer in the negotiation and closing of a real-property\ntransaction by handling financial calculations and transfers of documents.  Also termed settlement agent. See also settlement\nattorney under attorney.\n- co-agent. (16c) Someone who shares with another agent the authority to act for the principal.  Also termed dual agent.\nCf. common agent.\n- commercial agent. (18c)  1. broker. 2. A consular officer responsible for the commercial interests of his or her country at a\nforeign port. 3. See mercantile agent. 4. See commission agent.\n- commission agent. (1812) An agent whose remuneration is based at least in part on commissions, or percentages of actual\nsales.  Commission agents typically work as middlemen between sellers and buyers.  Also termed commercial agent.\n\nAGENT, Black's Law Dictionary (11th ed. 2019)\n  2024 Thomson Reuters. No claim to original U.S. Government Works. 2\n- common agent. (17c) An agent who acts on behalf of more than one principal in a transaction. Cf. co-agent.\n- corporate agent. (1819) An agent authorized to act on behalf of a corporation; broadly, all employees and officers who have\nthe power to bind the corporation.\n- county agent. See juvenile officer under officer (1).\n- del credere agent (del kred--ray or kray-d-ray) (1822) An agent who guarantees the solvency of the third party with whom\nthe agent makes a contract for the principal.  A del credere agent receives possession of the principal's goods for purposes\nof sale and guarantees that anyone to whom the agent sells the goods on credit will pay promptly for them. For this guaranty,\nthe agent receives a higher commission for sales. The promise of such an agent is almost universally held not to be within the\nstatute of frauds.  Also termed del credere factor.\n- diplomatic agent.  (18c) A national representative in one of four categories: (1) ambassadors, (2) envoys and ministers\nplenipotentiary, (3) ministers resident accredited to the sovereign, or (4) chargs d'affaires accredited to the minister of foreign\naffairs.\n- double agent. (1935)  1. A spy who finds out an enemy's secrets for his or her principal but who also gives secrets to the\nenemy. 2. See dual agent (2).\n- dual agent. (1881)  1. See co-agent. 2. An agent who represents both parties in a single transaction, esp. a buyer and a seller.\n Also termed (in sense 2) double agent.\n- emigrant agent. (1874) One engaged in the business of hiring laborers for work outside the country or state.\n- enrolled agent. See enrolled agent.\n- escrow agent. See escrow agent.\n- estate agent. See real-estate agent.\n- fiscal agent. (18c) A bank or other financial institution that collects and disburses money and services as a depository of\nprivate and public funds on another's behalf.\n- foreign agent. (1938) Someone who registers with the federal government as a lobbyist representing the interests of a foreign\ncountry or corporation.\n- forwarding agent.  (1837)  1. freight forwarder . 2. A freight-forwarder who assembles less-than-carload shipments (small\nshipments) into carload shipments, thus taking advantage of lower freight rates.\n- general agent. (17c) An agent authorized to transact all the principal's business of a particular kind or in a particular place. \nAmong the common types of general agents are factors, brokers, and partners. Cf. special agent.\n- government agent. (1805)  1. An employee or representative of a governmental body. 2. A law-enforcement official, such as\na police officer or an FBI agent. 3. An informant, esp. an inmate, used by law enforcement to obtain incriminating statements\nfrom another inmate.\n- gratuitous agent. (1822) An agent who acts without a right to compensation.\n- high-managerial agent. (1957)  1. An agent of a corporation or other business who has authority to formulate corporate policy\nor supervise employees.  Also termed superior agent. 2. See superior agent (1).\n- implied agent. See apparent agent.\n- independent agent. (17c) An agent who exercises personal judgment and is subject to the principal only for the results of\nthe work performed. Cf. nonservant agent.\n- innocent agent. (1805) Criminal law. A person whose action on behalf of a principal is unlawful but does not merit prosecution\nbecause the agent had no knowledge of the principal's illegal purpose; a person who lacks the mens rea for an offense but who\nis tricked or coerced by the principal into committing a crime.  Although the agent's conduct was unlawful, the agent might\nnot be prosecuted if the agent had no knowledge of the principal's illegal purpose. The principal is legally accountable for the\ninnocent agent's actions. See Model Penal Code  2.06(2)(a).\n- insurance agent. See insurance agent.\n- jural agent. See jural agent.\n- land agent. See land agent.\n- listing agent. (1927) The real-estate broker's representative who obtains a listing agreement with the owner. Cf. selling agent;\nshowing agent.\n- local agent. (1804)  1. An agent appointed to act as another's (esp. a company's) representative and to transact business within\na specified district. 2. See special agent.\n\nAGENT, Black's Law Dictionary (11th ed. 2019)\n  2024 Thomson Reuters. No claim to original U.S. Government Works. 3\n- managing agent. (1812) A person with general power involving the exercise of judgment and discretion, as opposed to an\nordinary agent who acts under the direction and control of the principal.  Also termed business agent.\n- mercantile agent. (18c) An agent employed to sell goods or merchandise on behalf of the principal.  Also termed commercial\nagent.\n- nonservant agent. (1920) An agent who agrees to act on the principal's behalf but is not subject to the principal's control\nover how the task is performed.  A principal is not liable for the physical torts of a nonservant agent. See independent contractor.\nCf. independent agent; servant.\n- ostensible agent. See apparent agent.\n- patent agent. (1859) A specialized legal professional  not necessarily a lawyer  who has fulfilled the U.S. Patent and\nTrademark Office requirements as a representative and is registered to prepare and prosecute patent applications before the\nPTO.  To be registered to practice before the PTO, a candidate must establish mastery of the relevant technology (by holding\na specified technical degree or equivalent training) in order to advise and assist patent applicants. The candidate must also pass\na written examination (the Patent Bar) that tests knowledge of patent law and PTO procedure.  Often shortened to agent.\n Also termed registered patent agent; patent solicitor. Cf. patent attorney.\n- primary agent. (18c) An agent who is directly authorized by a principal.  A primary agent generally may hire a subagent\nto perform all or part of the agency. Cf. subagent (1).\n- private agent. (17c) An agent acting for an individual in that person's private affairs.\n- process agent. (1886) A person authorized to accept service of process on behalf of another. See registered agent.\n- procuring agent. (1954) Someone who obtains drugs on behalf of another person and delivers the drugs to that person. \nIn criminal-defense theory, the procuring agent does not sell, barter, exchange, or make a gift of the drugs to the other person\nbecause the drugs already belong to that person, who merely employs the agent to pick up and deliver them.\n- public agent. (17c) A person appointed to act for the public in matters relating to governmental administration or public\nbusiness.\n- real-estate agent. (1844) An agent who represents a buyer or seller (or both, with proper disclosures) in the sale or lease of\nreal property.  A real-estate agent can be either a broker (whose principal is a buyer or seller) or a salesperson (whose principal\nis a broker).  Also termed estate agent. Cf. realtor.\n- record agent. See insurance agent.\n- registered agent. (1809) A person authorized to accept service of process for another person, esp. a foreign corporation, in\na particular jurisdiction.  Also termed resident agent. See process agent.\n- registered patent agent. See patent agent.\n- resident agent. See registered agent.\n- secret agent. See secret agent.\n- selling agent. (1839)  1. The real-estate broker's representative who sells the property, as opposed to the agent who lists the\nproperty for sale. 2. See showing agent. Cf. listing agent.\n- settlement agent. (1952) See closing agent.\n- showing agent. (1901) A real-estate broker's representative who markets property to a prospective purchaser.  A showing\nagent may be characterized as a subagent of the listing broker, as an agent who represents the purchaser, or as an intermediary\nwho owes an agent's duties to neither seller nor buyer.  Also termed selling agent. Cf. listing agent.\n- soliciting agent. (1855)  1. Insurance. An agent with authority relating to the solicitation or submission of applications to an\ninsurance company but usu. without authority to bind the insurer, as by accepting the applications on behalf of the company.\n2. An agent who solicits orders for goods or services for a principal.  3. A managing agent of a corporation for purposes of\nservice of process.\n- special agent. (17c)  1. An agent employed to conduct a particular transaction or to perform a specified act. Cf. general agent.\n2. See insurance agent.\n- specially accredited agent. (1888) An agent that the principal has specially invited a third party to deal with, in an implication\nthat the third party will be notified if the agent's authority is altered or revoked.\n- statutory agent. (1844) An agent designated by law to receive litigation documents and other legal notices for a nonresident\ncorporation.  In most states, the secretary of state is the statutory agent for such corporations. Cf. agency by operation of law\n(1) under agency (1).\n\nAGENT, Black's Law Dictionary (11th ed. 2019)\n  2024 Thomson Reuters. No claim to original U.S. Government Works. 4\n- stock-transfer agent. (1873) See transfer agent.\n- subagent. (18c)  1. A person to whom an agent has delegated the performance of an act for the principal; a person designated\nby an agent to perform some duty relating to the agency.  If the principal consents to a primary agent's employment of a\nsubagent, the subagent owes fiduciary duties to the principal, and the principal is liable for the subagent's acts.  Also termed\nsubservant. Cf. primary agent; subordinate agent.\nBy delegation  the agent is permitted to use agents of his own in performing the function he is employed to perform for\nhis principal, delegating to them the discretion which normally he would be expected to exercise personally. These agents are\nknown as subagents to indicate that they are the agent's agents and not the agents of the principal. Normally (though of course\nnot necessarily) they are paid by the agent. The agent is liable to the principal for any injury done him by the misbehavior of\nthe agent's subagents. Floyd R. Mechem, Outlines of the Law of Agency  79, at 51 (Philip Mechem ed., 4th ed. 1952).\n2. See buyer's broker under broker.\n- subordinate agent. (17c) An agent who acts subject to the direction of a superior agent.  Subordinate and superior agents\nare co-agents of a common principal. See superior agent. Cf. subagent (1).\n- successor agent. (1934) An agent who is appointed by a principal to act in a primary agent's stead if the primary agent is\nunable or unwilling to perform.\n- superior agent. (17c)  1. An agent on whom a principal confers the right to direct a subordinate agent. See subordinate agent.\n2. See high-managerial agent (1).\n- transfer agent. (1850) An organization (such as a bank or trust company) that handles transfers of shares for a publicly held\ncorporation by issuing new certificates and overseeing the cancellation of old ones and that usu. also maintains the record of\nshareholders for the corporation and mails dividend checks.  Generally, a transfer agent ensures that certificates submitted for\ntransfer are properly indorsed and that the transfer right is appropriately documented.  Also termed stock-transfer agent.\n- trustee-agent. A trustee who is subject to the control of the settlor or one or more beneficiaries of a trust. See trustee (1).\n- undercover agent. (1930)  1. An agent who does not disclose his or her role as an agent.  2. A police officer who gathers\nevidence of criminal activity without disclosing his or her identity to the suspect.\n- undisclosed agent. (1863) An agent who deals with a third party who has no knowledge that the agent is acting on a principal's\nbehalf. Cf. undisclosed principal under principal (1).\n- universal agent. (18c) An agent authorized to perform all acts that the principal could personally perform.\n- vice-commercial agent. (1800) Hist. In the consular service of the United States, a consular officer who was substituted\ntemporarily to fill the place of a commercial agent who was absent or had been relieved from duty.\nWestlaw.  2019 Thomson Reuters. No Claim to Orig. U.S. Govt. Works.\nEnd of Document  2024 Thomson Reuters. No claim to original U.S. Government Works.	AGENT, Black's Law Dictionary (11th ed. 2019)\n  2024 Thomson Reuters. No claim to original U.S. Government Works. 1\nBlack's Law Dictionary (11th ed. 2019), agent\nAGENT\nBryan A. Garner, Editor in Chief\nPreface | Guide | Legal Maxims | Bibliography\nagent (15c)  1. Something that produces an effect <an intervening agent>. See cause (1); electronic agent. 2. Someone who is\nauthorized to act for or in place of another; a representative <a professional athlete's agent>.  Also termed commissionaire.\n	en	0.9	uploaded	2683	16290	2025-11-15 16:08:54.082678	2025-11-15 16:08:54.082682	\N	1	\N	\N	\N	1	original	\N	\N	\N	\N	\N	{}	12ffbf71-f730-47e3-baed-f058bce316de
176	Intelligent Agents - Russell & Norvig AI: A Modern Approach (2020)	file	reference	academic	pdf	Chapter 2 (Agents) Artificial Intelligence_ A Modern Approach-Prentice Hall (2020).pdf	uploads/e7a191b54e6e4842_Chapter 2 (Agents) Artificial Intelligence_ A Modern Approach-Prentice Hall (2020).pdf	238548	{"year": 2020, "discipline": "Artificial Intelligence", "upload_order": 5}	CHAPTER 2\nINTELLIGENT AGENTS\nIn which we discuss the nature of agents, perfect or otherwis e, the diversity of environments,\nand the resulting menagerie of agent types.\nChapter 1 identied the concept of rational agents as central to our approach to articial\nintelligence. In this chapter, we make this notion more conc rete. W e will see that the concept\nof rationality can be applied to a wide variety of agents oper ating in any imaginable environ-\nment. Our plan in this book is to use this concept to develop a s mall set of design principles\nfor building successful agentssystems that can reasonabl y be called intelligent.\nW e begin by examining agents, environments, and the couplin g between them. The ob-\nservation that some agents behave better than others leads n aturally to the idea of a rational\nagentone that behaves as well as possible. How well an agent can behave depends on the\nnature of the environment; some environments are more difc ult than others. W e give a crude\ncategorization of environments and show how properties of a n environment inuence the de-\nsign of suitable agents for that environment. W e describe a n umber of basic skeleton agent\ndesigns, which we esh out in the rest of the book.\n2.1 Agents and Environments\nAn agent is anything that can be viewed as perceiving its environment through sensors andEnvironment\nSensor\nacting upon that environment through actuators. This simple idea is illustrated in Figure 2.1.\nActuator A human agent has eyes, ears, and other organs for sensors and hands, legs, vocal tract,\nand so on for actuators. A robotic agent might have cameras an d infrared range nders for\nsensors and various motors for actuators. A software agent r eceives le contents, network\npackets, and human input (keyboard/mouse/touchscreen/vo ice) as sensory inputs and acts on\nthe environment by writing les, sending network packets, a nd displaying information or\ngenerating sounds. The environment could be everythingth e entire universe! In practice it\nis just that part of the universe whose state we care about whe n designing this agentthe part\nthat affects what the agent perceives and that is affected by the agents actions.\nW e use the term percept to refer to the content an agents sensors are perceiving. AnPercept\nagents percept sequence is the complete history of everything the agent has ever perc eived.Percept sequence\nIn general, an agents choice of action at any given instant can depend on its built-in knowl-\nedge and on the entire percept sequence observed to date, but not on anything it hasnt per-\nceived. By specifying the agents choice of action for every possibl e percept sequence, we\nhave said more or less everything there is to say about the age nt. Mathematically speak-\ning, we say that an agents behavior is described by the agent function that maps any givenAgent function\npercept sequence to an action.\n\nSection 2.1 Agents and Environments 37\nAgent Sensors\nActuators\nEnvironment\nPercepts\nActions\n?\nFigure 2.1 Agents interact with environments through sensors and actu ators.\nW e can imagine tabulating the agent function that describes any given agent; for most\nagents, this would be a very large tableinnite, in fact, un less we place a bound on the\nlength of percept sequences we want to consider. Given an age nt to experiment with, we can,\nin principle, construct this table by trying out all possibl e percept sequences and recording\nwhich actions the agent does in response. 1 The table is, of course, an external characterization\nof the agent. Internally, the agent function for an articial agent will be implement ed by an\nagent program . It is important to keep these two ideas distinct. The agent f unction is an Agent program\nabstract mathematical description; the agent program is a c oncrete implementation, running\nwithin some physical system.\nT o illustrate these ideas, we use a simple examplethe vacuu m-cleaner world, which\nconsists of a robotic vacuum-cleaning agent in a world consi sting of squares that can be\neither dirty or clean. Figure 2.2 shows a conguration with j ust two squares, A and B. The\nvacuum agent perceives which square it is in and whether ther e is dirt in the square. The\nagent starts in square A. The available actions are to move to the right, move to the le ft, suck\nup the dirt, or do nothing. 2 One very simple agent function is the following: if the curre nt\nsquare is dirty , then suck; otherwise, move to the other squa re. A partial tabulation of this\nagent function is shown in Figure 2.3 and an agent program tha t implements it appears in\nFigure 2.8 on page 49.\nLooking at Figure 2.3, we see that various vacuum-world agen ts can be dened simply\nby lling in the right-hand column in various ways. The obvio us question, then, is this: What \nis the right way to ll out the table? In other words, what makes an agent good or bad,\nintelligent or stupid? W e answer these questions in the next section.\n1 If the agent uses some randomization to choose its actions, t hen we would have to try each sequence many\ntimes to identify the probability of each action. One might i magine that acting randomly is rather silly , but we\nshow later in this chapter that it can be very intelligent.\n2In a real robot, it would be unlikely to have an actions like m ove right and move left.  Instead the actions\nwould be spin wheels forward and spin wheels backward.  W e have chosen the actions to be easier to follow\non the page, not for ease of implementation in an actual robot .\n\n38 Chapter 2 Intelligent Agents\nA B\nFigure 2.2 A vacuum-cleaner world with just two locations. Each locati on can be clean or\ndirty, and the agent can move left or right and can clean the sq uare that it occupies. Different\nversions of the vacuum world allow for different rules about what the agent can perceive,\nwhether its actions always succeed, and so on.\nPercept sequence Action\n[A,Clean] Right\n[A,Dirty] Suck\n[B,Clean] Left\n[B,Dirty] Suck\n[A,Clean], [A,Clean] Right\n[A,Clean], [A,Dirty] Suck\n.\n.\n. .\n.\n.\n[A,Clean], [A,Clean], [A,Clean] Right\n[A,Clean], [A,Clean], [A,Dirty] Suck\n.\n.\n. .\n.\n.\nFigure 2.3 Partial tabulation of a simple agent function for the vacuum -cleaner world shown\nin Figure 2.2. The agent cleans the current square if it is dir ty, otherwise it moves to the other\nsquare. Note that the table is of unbounded size unless there is a restriction on the length of\npossible percept sequences.\nBefore closing this section, we should emphasize that the no tion of an agent is meant to\nbe a tool for analyzing systems, not an absolute characteriz ation that divides the world into\nagents and non-agents. One could view a hand-held calculato r as an agent that chooses the\naction of displaying 4 when given the percept sequence 2 + 2 =,  but such an analysis\nwould hardly aid our understanding of the calculator. In a se nse, all areas of engineering can\nbe seen as designing artifacts that interact with the world; AI operates at (what the authors\nconsider to be) the most interesting end of the spectrum, whe re the artifacts have signicant\ncomputational resources and the task environment requires nontrivial decision making.\n\nSection 2.2 Good Behavior: The Concept of Rationality 39\n2.2 Good Behavior: The Concept of Rationality\nA rational agent is one that does the right thing. Obviously , doing the right t hing is better Rational agent\nthan doing the wrong thing, but what does it mean to do the righ t thing?\n2.2.1 Performance measures\nMoral philosophy has developed several different notions o f the right thing,  but AI has\ngenerally stuck to one notion called consequentialism: we evaluate an agents behavior by its Consequentialism\nconsequences. When an agent is plunked down in an environmen t, it generates a sequence of\nactions according to the percepts it receives. This sequenc e of actions causes the environment\nto go through a sequence of states. If the sequence is desirab le, then the agent has performed\nwell. This notion of desirability is captured by a performance measure that evaluates any Performance\nmeasure\ngiven sequence of environment states.\nHumans have desires and preferences of their own, so the noti on of rationality as applied\nto humans has to do with their success in choosing actions tha t produce sequences of envi-\nronment states that are desirable from their point of view . Machines, on the other hand, do not\nhave desires and preferences of their own; the performance m easure is, initially at least, in the\nmind of the designer of the machine, or in the mind of the users the machine is designed for.\nW e will see that some agent designs have an explicit represen tation of (a version of) the per-\nformance measure, while in other designs the performance me asure is entirely implicitthe\nagent may do the right thing, but it doesnt know why .\nRecalling Norbert Wieners warning to ensure that the purp ose put into the machine is\nthe purpose which we really desire (page 33), notice that it can be quite hard to formulate\na performance measure correctly . Consider, for example, th e vacuum-cleaner agent from the\npreceding section. W e might propose to measure performance by the amount of dirt cleaned\nup in a single eight-hour shift. With a rational agent, of cou rse, what you ask for is what\nyou get. A rational agent can maximize this performance meas ure by cleaning up the dirt,\nthen dumping it all on the oor, then cleaning it up again, and so on. A more suitable per-\nformance measure would reward the agent for having a clean o or. For example, one point\ncould be awarded for each clean square at each time step (perh aps with a penalty for elec-\ntricity consumed and noise generated). As a general rule, it is better to design performance \nmeasures according to what one actually wants to be achieved in the environment, rather\nthan according to how one thinks the agent should behave.\nEven when the obvious pitfalls are avoided, some knotty prob lems remain. For example,\nthe notion of clean oor in the preceding paragraph is base d on average cleanliness over\ntime. Y et the same average cleanliness can be achieved by two different agents, one of which\ndoes a mediocre job all the time while the other cleans energe tically but takes long breaks.\nWhich is preferable might seem to be a ne point of janitorial science, but in fact it is a\ndeep philosophical question with far-reaching implicatio ns. Which is bettera reckless life\nof highs and lows, or a safe but humdrum existence? Which is be tteran economy where\neveryone lives in moderate poverty , or one in which some live in plenty while others are very\npoor? W e leave these questions as an exercise for the diligen t reader.\nFor most of the book, we will assume that the performance meas ure can be specied\ncorrectly . For the reasons given above, however, we must acc ept the possibility that we might\nput the wrong purpose into the machineprecisely the King Mi das problem described on\n\n40 Chapter 2 Intelligent Agents\npage 33. Moreover, when designing one piece of software, cop ies of which will belong to\ndifferent users, we cannot anticipate the exact preference s of each individual user. Thus, we\nmay need to build agents that reect initial uncertainty abo ut the true performance measure\nand learn more about it as time goes by; such agents are descri bed in Chapters 16, 18, and 22.\n2.2.2 Rationality\nWhat is rational at any given time depends on four things:\n The performance measure that denes the criterion of succe ss.\n The agents prior knowledge of the environment.\n The actions that the agent can perform.\n The agents percept sequence to date.\nThis leads to a denition of a rational agent :\nDenition of a\nrational agent\n F or each possible percept sequence, a rational agent should select an action that is ex-\npected to maximize its performance measure, given the evide nce provided by the percept\nsequence and whatever built-in knowledge the agent has.\nConsider the simple vacuum-cleaner agent that cleans a squa re if it is dirty and moves to the\nother square if not; this is the agent function tabulated in F igure 2.3. Is this a rational agent?\nThat depends! First, we need to say what the performance meas ure is, what is known about\nthe environment, and what sensors and actuators the agent ha s. Let us assume the following:\n The performance measure awards one point for each clean squ are at each time step,\nover a lifetime of 1000 time steps.\n The geography of the environment is known a priori (Figure 2.2) but the dirt distri-\nbution and the initial location of the agent are not. Clean sq uares stay clean and sucking\ncleans the current square. The Right and Left actions move the agent one square ex-\ncept when this would take the agent outside the environment, in which case the agent\nremains where it is.\n The only available actions are Right, Left, and Suck.\n The agent correctly perceives its location and whether tha t location contains dirt.\nUnder these circumstances the agent is indeed rational; its expected performance is at least\nas good as any other agents.\nOne can see easily that the same agent would be irrational und er different circumstances.\nFor example, once all the dirt is cleaned up, the agent will os cillate needlessly back and forth;\nif the performance measure includes a penalty of one point fo r each movement, the agent will\nfare poorly . A better agent for this case would do nothing onc e it is sure that all the squares\nare clean. If clean squares can become dirty again, the agent should occasionally check and\nre-clean them if needed. If the geography of the environment is unknown, the agent will need\nto explore it. Exercise 2. V AC R asks you to design agents for these cases.\n2.2.3 Omniscience, learning, and autonomy\nW e need to be careful to distinguish between rationality and omniscience. An omniscientOmniscience\nagent knows the actual outcome of its actions and can act accordingly; but omniscie nce is\nimpossible in reality . Consider the following example: I am walking along the Champs\nElys ees one day and I see an old friend across the street. The re is no trafc nearby and Im\n\nSection 2.2 Good Behavior: The Concept of Rationality 41\nnot otherwise engaged, so, being rational, I start to cross t he street. Meanwhile, at 33,000\nfeet, a cargo door falls off a passing airliner, 3 and before I make it to the other side of the\nstreet I am attened. W as I irrational to cross the street? It is unlikely that my obituary would\nread Idiot attempts to cross street. \nThis example shows that rationality is not the same as perfec tion. Rationality maximizes\nexpected performance, while perfection maximizes actual performance. Retreating from a\nrequirement of perfection is not just a question of being fai r to agents. The point is that if we\nexpect an agent to do what turns out after the fact to be the bes t action, it will be impossible\nto design an agent to fulll this specicationunless we imp rove the performance of crystal\nballs or time machines.\nOur denition of rationality does not require omniscience, then, because the rational\nchoice depends only on the percept sequence to date . W e must also ensure that we havent\ninadvertently allowed the agent to engage in decidedly unde rintelligent activities. For exam-\nple, if an agent does not look both ways before crossing a busy road, then its percept sequence\nwill not tell it that there is a large truck approaching at hig h speed. Does our denition of\nrationality say that its now OK to cross the road? Far from it !\nFirst, it would not be rational to cross the road given this un informative percept sequence:\nthe risk of accident from crossing without looking is too gre at. Second, a rational agent should\nchoose the looking action before stepping into the street , because looking helps maximize\nthe expected performance. Doing actions in order to modify future percepts sometimes\ncalled information gathering is an important part of rationality and is covered in depth i n Information\ngathering\nChapter 16. A second example of information gathering is pro vided by the exploration that\nmust be undertaken by a vacuum-cleaning agent in an initiall y unknown environment.\nOur denition requires a rational agent not only to gather in formation but also to learn as Learning\nmuch as possible from what it perceives. The agents initial conguration could reect some\nprior knowledge of the environment, but as the agent gains ex perience this may be modied\nand augmented. There are extreme cases in which the environm ent is completely known a\npriori and completely predictable. In such cases, the agent need no t perceive or learn; it\nsimply acts correctly .\nOf course, such agents are fragile. Consider the lowly dung b eetle. After digging its nest\nand laying its eggs, it fetches a ball of dung from a nearby hea p to plug the entrance. If the\nball of dung is removed from its grasp en route , the beetle continues its task and pantomimes\nplugging the nest with the nonexistent dung ball, never noti cing that it is missing. Evolu-\ntion has built an assumption into the beetles behavior, and when it is violated, unsuccessful\nbehavior results.\nSlightly more intelligent is the sphex wasp. The female sphe x will dig a burrow , go out\nand sting a caterpillar and drag it to the burrow , enter the bu rrow again to check all is well,\ndrag the caterpillar inside, and lay its eggs. The caterpill ar serves as a food source when the\neggs hatch. So far so good, but if an entomologist moves the ca terpillar a few inches away\nwhile the sphex is doing the check, it will revert to the drag the caterpillar step of its plan\nand will continue the plan without modication, re-checkin g the burrow , even after dozens of\ncaterpillar-moving interventions. The sphex is unable to l earn that its innate plan is failing,\nand thus will not change it.\n3 See N. Henderson, New door latches urged for Boeing 747 jumb o jets,  W ashington P ost, August 24, 1989.\n\n42 Chapter 2 Intelligent Agents\nT o the extent that an agent relies on the prior knowledge of it s designer rather than on its\nown percepts and learning processes, we say that the agent la cks autonomy. A rational agentAutonomy\nshould be autonomousit should learn what it can to compensa te for partial or incorrect\nprior knowledge. For example, a vacuum-cleaning agent that learns to predict where and\nwhen additional dirt will appear will do better than one that does not.\nAs a practical matter, one seldom requires complete autonom y from the start: when the\nagent has had little or no experience, it would have to act ran domly unless the designer gave\nsome assistance. Just as evolution provides animals with en ough built-in reexes to survive\nlong enough to learn for themselves, it would be reasonable t o provide an articial intelligent\nagent with some initial knowledge as well as an ability to lea rn. After sufcient experience\nof its environment, the behavior of a rational agent can beco me effectively independent of its\nprior knowledge. Hence, the incorporation of learning allo ws one to design a single rational\nagent that will succeed in a vast variety of environments.\n2.3 The Nature of Environments\nNow that we have a denition of rationality , we are almost rea dy to think about building\nrational agents. First, however, we must think about task environments , which are essen-Task environment\ntially the problems to which rational agents are the solu tions.  W e begin by showing how\nto specify a task environment, illustrating the process wit h a number of examples. W e then\nshow that task environments come in a variety of avors. The n ature of the task environment\ndirectly affects the appropriate design for the agent progr am.\n2.3.1 Specifying the task environment\nIn our discussion of the rationality of the simple vacuum-cl eaner agent, we had to specify\nthe performance measure, the environment, and the agents a ctuators and sensors. W e group\nall these under the heading of the task environment . For the acronymically minded, we call\nthis the PEAS (Performance, Environment, Actuators, Sensors) description. In designing anPEAS\nagent, the rst step must always be to specify the task enviro nment as fully as possible.\nThe vacuum world was a simple example; let us consider a more c omplex problem:\nan automated taxi driver. Figure 2.4 summarizes the PEAS des cription for the taxis task\nenvironment. W e discuss each element in more detail in the fo llowing paragraphs.\nFirst, what is the performance measure to which we would like our automated driver\nto aspire? Desirable qualities include getting to the corre ct destination; minimizing fuel con-\nsumption and wear and tear; minimizing the trip time or cost; minimizing violations of trafc\nlaws and disturbances to other drivers; maximizing safety a nd passenger comfort; maximiz-\ning prots. Obviously , some of these goals conict, so trade offs will be required.\nNext, what is the driving environment that the taxi will face? Any taxi driver must deal\nwith a variety of roads, ranging from rural lanes and urban al leys to 12-lane freeways. The\nroads contain other trafc, pedestrians, stray animals, ro ad works, police cars, puddles, and\npotholes. The taxi must also interact with potential and act ual passengers. There are also\nsome optional choices. The taxi might need to operate in Sout hern California, where snow\nis seldom a problem, or in Alaska, where it seldom is not. It co uld always be driving on the\nright, or we might want it to be exible enough to drive on the l eft when in Britain or Japan.\nObviously , the more restricted the environment, the easier the design problem.\n\nSection 2.3 The Nature of Environments 43\nAgent T ype Performance\nMeasure\nEnvironment Actuators Sensors\nT axi driver Safe, fast,\nlegal,\ncomfortable\ntrip, maximize\nprots,\nminimize\nimpact on\nother road\nusers\nRoads, other\ntrafc, police,\npedestrians,\ncustomers,\nweather\nSteering,\naccelerator,\nbrake, signal,\nhorn, display ,\nspeech\nCameras, radar,\nspeedometer, GPS, engine\nsensors, accelerometer,\nmicrophones, touchscreen\nFigure 2.4 PEAS description of the task environment for an automated ta xi driver.\nThe actuators for an automated taxi include those available to a human driv er : control\nover the engine through the accelerator and control over ste ering and braking. In addition, it\nwill need output to a display screen or voice synthesizer to t alk back to the passengers, and\nperhaps some way to communicate with other vehicles, polite ly or otherwise.\nThe basic sensors for the taxi will include one or more video cameras so that it c an see, as\nwell as lidar and ultrasound sensors to detect distances to o ther cars and obstacles. T o avoid\nspeeding tickets, the taxi should have a speedometer, and to control the vehicle properly ,\nespecially on curves, it should have an accelerometer. T o de termine the mechanical state of\nthe vehicle, it will need the usual array of engine, fuel, and electrical system sensors. Like\nmany human drivers, it might want to access GPS signals so tha t it doesnt get lost. Finally ,\nit will need touchscreen or voice input for the passenger to r equest a destination.\nIn Figure 2.5, we have sketched the basic PEAS elements for a n umber of additional\nagent types. Further examples appear in Exercise 2. PEA S . The examples include physical\nas well as virtual environments. Note that virtual task envi ronments can be just as complex\nas the real world: for example, a software agent (or software robot or softbot) that trades Software agent\nSoftbot\non auction and reselling W eb sites deals with millions of oth er users and billions of objects,\nmany with real images.\n2.3.2 Properties of task environments\nThe range of task environments that might arise in AI is obvio usly vast. W e can, however,\nidentify a fairly small number of dimensions along which tas k environments can be catego-\nrized. These dimensions determine, to a large extent, the ap propriate agent design and the\napplicability of each of the principal families of techniqu es for agent implementation. First\nwe list the dimensions, then we analyze several task environ ments to illustrate the ideas. The\ndenitions here are informal; later chapters provide more p recise statements and examples of\neach kind of environment.\nFully observable vs. partially observable : If an agents sensors give it access to the Fully observable\nPartially observable\ncomplete state of the environment at each point in time, then we say that the task environ-\nment is fully observable. A task environment is effectively fully observable if the sensors\ndetect all aspects that are relevant to the choice of action; relevance, in turn, depends on the\n\n44 Chapter 2 Intelligent Agents\nAgent T ype Performance\nMeasure\nEnvironment Actuators Sensors\nMedical\ndiagnosis system\nHealthy patient,\nreduced costs\nPatient, hospital,\nstaff\nDisplay of\nquestions, tests,\ndiagnoses,\ntreatments\nT ouchscreen/voice\nentry of\nsymptoms and\nndings\nSatellite image\nanalysis system\nCorrect\ncategorization of\nobjects, terrain\nOrbiting satellite,\ndownlink,\nweather\nDisplay of scene\ncategorization\nHigh-resolution\ndigital camera\nPart-picking\nrobot\nPercentage of\nparts in correct\nbins\nConveyor belt\nwith parts; bins\nJointed arm and\nhand\nCamera, tactile\nand joint angle\nsensors\nRenery\ncontroller\nPurity , yield,\nsafety\nRenery , raw\nmaterials,\noperators\nV alves, pumps,\nheaters, stirrers,\ndisplays\nT emperature,\npressure, ow ,\nchemical sensors\nInteractive\nEnglish tutor\nStudents score\non test\nSet of students,\ntesting agency\nDisplay of\nexercises,\nfeedback, speech\nKeyboard entry ,\nvoice\nFigure 2.5 Examples of agent types and their PEAS descriptions.\nperformance measure. Fully observable environments are co nvenient because the agent need\nnot maintain any internal state to keep track of the world. An environment might be partially\nobservable because of noisy and inaccurate sensors or becau se parts of the state are simply\nmissing from the sensor datafor example, a vacuum agent wit h only a local dirt sensor\ncannot tell whether there is dirt in other squares, and an aut omated taxi cannot see what other\ndrivers are thinking. If the agent has no sensors at all then t he environment is unobserv-\nable. One might think that in such cases the agents plight is hope less, but, as we discuss inUnobservable\nChapter 4, the agents goals may still be achievable, someti mes with certainty .\nSingle-agent vs. multiagent: The distinction between single-agent and multiagent en-Single-agent\nMultiagent\nvironments may seem simple enough. For example, an agent sol ving a crossword puzzle by\nitself is clearly in a single-agent environment, whereas an agent playing chess is in a two-\nagent environment. However, there are some subtle issues. F irst, we have described how an\nentity may be viewed as an agent, but we have not explained which entitie s must be viewed\nas agents. Does an agent A (the taxi driver for example) have to treat an object B (another\nvehicle) as an agent, or can it be treated merely as an object b ehaving according to the laws of\nphysics, analogous to waves at the beach or leaves blowing in the wind? The key distinction\nis whether Bs behavior is best described as maximizing a performance me asure whose value\ndepends on agent As behavior.\n\nSection 2.3 The Nature of Environments 45\nFor example, in chess, the opponent entity B is trying to maximize its performance mea-\nsure, which, by the rules of chess, minimizes agent As performance measure. Thus, chess is\na competitive multiagent environment. On the other hand, in the taxi-driv ing environment, Competitive\navoiding collisions maximizes the performance measure of a ll agents, so it is a partially co-\noperative multiagent environment. It is also partially competitive b ecause, for example, only Cooperative\none car can occupy a parking space.\nThe agent-design problems in multiagent environments are o ften quite different from\nthose in single-agent environments; for example, communic ation often emerges as a rational\nbehavior in multiagent environments; in some competitive e nvironments, randomized behav-\nior is rational because it avoids the pitfalls of predictabi lity .\nDeterministic vs. nondeterministic. If the next state of the environment is completely Deterministic\nNondeterministic\ndetermined by the current state and the action executed by th e agent(s), then we say the\nenvironment is deterministic; otherwise, it is nondetermi nistic. In principle, an agent need not\nworry about uncertainty in a fully observable, determinist ic environment. If the environment\nis partially observable, however, then it could appear to be nondeterministic.\nMost real situations are so complex that it is impossible to k eep track of all the unobserved\naspects; for practical purposes, they must be treated as non deterministic. T axi driving is\nclearly nondeterministic in this sense, because one can nev er predict the behavior of trafc\nexactly; moreover, ones tires may blow out unexpectedly an d ones engine may seize up\nwithout warning. The vacuum world as we described it is deter ministic, but variations can\ninclude nondeterministic elements such as randomly appear ing dirt and an unreliable suction\nmechanism (Exercise 2. V FIN ).\nOne nal note: the word stochastic is used by some as a synonym for nondeterministic,  Stochastic\nbut we make a distinction between the two terms; we say that a m odel of the environment\nis stochastic if it explicitly deals with probabilities (e. g., theres a 25% chance of rain to-\nmorrow) and nondeterministic if the possibilities are l isted without being quantied (e.g.,\ntheres a chance of rain tomorrow).\nEpisodic vs. sequential: In an episodic task environment, the agents experience is di- Episodic\nSequential\nvided into atomic episodes. In each episode the agent receiv es a percept and then performs\na single action. Crucially , the next episode does not depend on the actions taken in pre-\nvious episodes. Many classication tasks are episodic. For example, an agent that has to\nspot defective parts on an assembly line bases each decision on the current part, regardless\nof previous decisions; moreover, the current decision does nt affect whether the next part is\ndefective. In sequential environments, on the other hand, t he current decision could affect\nall future decisions. 4 Chess and taxi driving are sequential: in both cases, short- term actions\ncan have long-term consequences. Episodic environments ar e much simpler than sequential\nenvironments because the agent does not need to think ahead.\nStatic vs. dynamic: If the environment can change while an agent is deliberatin g, then Static\nDynamic\nwe say the environment is dynamic for that agent; otherwise, it is static. Static environments\nare easy to deal with because the agent need not keep looking a t the world while it is deciding\non an action, nor need it worry about the passage of time. Dyna mic environments, on the\nother hand, are continuously asking the agent what it wants t o do; if it hasnt decided yet,\n4 The word sequential is also used in computer science as the antonym of parallel.  The two meanings are\nlargely unrelated.\n\n46 Chapter 2 Intelligent Agents\nthat counts as deciding to do nothing. If the environment its elf does not change with the\npassage of time but the agents performance score does, then we say the environment is\nsemidynamic. T axi driving is clearly dynamic: the other cars and the taxi itself keep movingSemidynamic\nwhile the driving algorithm dithers about what to do next. Ch ess, when played with a clock,\nis semidynamic. Crossword puzzles are static.\nDiscrete vs. continuous: The discrete/continuous distinction applies to the state of theDiscrete\nContinuous\nenvironment, to the way time is handled, and to the percepts and actions of the agent. For\nexample, the chess environment has a nite number of distinc t states (excluding the clock).\nChess also has a discrete set of percepts and actions. T axi dr iving is a continuous-state and\ncontinuous-time problem: the speed and location of the taxi and of the other vehicles sweep\nthrough a range of continuous values and do so smoothly over t ime. T axi-driving actions are\nalso continuous (steering angles, etc.). Input from digita l cameras is discrete, strictly speak-\ning, but is typically treated as representing continuously varying intensities and locations.\nKnown vs. unknown: Strictly speaking, this distinction refers not to the envi ronmentKnown\nUnknown\nitself but to the agents (or designers) state of knowledge about the laws of physics of\nthe environment. In a known environment, the outcomes (or ou tcome probabilities if the\nenvironment is nondeterministic) for all actions are given . Obviously , if the environment is\nunknown, the agent will have to learn how it works in order to m ake good decisions.\nThe distinction between known and unknown environments is n ot the same as the one\nbetween fully and partially observable environments. It is quite possible for a known environ-\nment to be partially observablefor example, in solitaire card games, I know the rules but\nam still unable to see the cards that have not yet been turned o ver. Conversely , an unknown\nenvironment can be fully observablein a new video game, the screen may show the entir e\ngame state but I still dont know what the buttons do until I tr y them.\nAs noted on page 39, the performance measure itself may be unk nown, either because\nthe designer is not sure how to write it down correctly or beca use the ultimate userwhose\npreferences matteris not known. For example, a taxi driver usually wont know whether a\nnew passenger prefers a leisurely or speedy journey , a cauti ous or aggressive driving style.\nA virtual personal assistant starts out knowing nothing abo ut the personal preferences of its\nnew owner. In such cases, the agent may learn more about the pe rformance measure based on\nfurther interactions with the designer or user. This, in tur n, suggests that the task environment\nis necessarily viewed as a multiagent environment.\nThe hardest case is partially observable , multiagent, nondeterministic, sequential, dy-\nnamic, continuous, and unknown. T axi driving is hard in all these senses, except that the\ndrivers environment is mostly known. Driving a rented car i n a new country with unfamiliar\ngeography , different trafc laws, and nervous passengers i s a lot more exciting.\nFigure 2.6 lists the properties of a number of familiar envir onments. Note that the prop-\nerties are not always cut and dried. For example, we have list ed the medical-diagnosis task\nas single-agent because the disease process in a patient is n ot protably modeled as an agent;\nbut a medical-diagnosis system might also have to deal with r ecalcitrant patients and skepti-\ncal staff, so the environment could have a multiagent aspect . Furthermore, medical diagnosis\nis episodic if one conceives of the task as selecting a diagno sis given a list of symptoms; the\nproblem is sequential if the task can include proposing a ser ies of tests, evaluating progress\nover the course of treatment, handling multiple patients, a nd so on.\n\nSection 2.4 The Structure of Agents 47\nT ask Environment Observable Agents Deterministic Episodi c Static Discrete\nCrossword puzzle Fully Single Deterministic Sequential St atic Discrete\nChess with a clock Fully Multi Deterministic Sequential Sem i Discrete\nPoker Partially Multi Stochastic Sequential Static Discre te\nBackgammon Fully Multi Stochastic Sequential Static Discr ete\nT axi driving Partially Multi Stochastic Sequential Dynami c Continuous\nMedical diagnosis Partially Single Stochastic Sequential Dynamic Continuous\nImage analysis Fully Single Deterministic Episodic Semi Co ntinuous\nPart-picking robot Partially Single Stochastic Episodic D ynamic Continuous\nRenery controller Partially Single Stochastic Sequentia l Dynamic Continuous\nEnglish tutor Partially Multi Stochastic Sequential Dynam ic Discrete\nFigure 2.6 Examples of task environments and their characteristics.\nW e have not included a known/unknown column because, as ex plained earlier, this is\nnot strictly a property of the environment. For some environ ments, such as chess and poker,\nit is quite easy to supply the agent with full knowledge of the rules, but it is nonetheless\ninteresting to consider how an agent might learn to play thes e games without such knowledge.\nThe code repository associated with this book ( aima.cs.berkeley.edu) includes mul-\ntiple environment implementations, together with a genera l-purpose environment simulator\nfor evaluating an agents performance. Experiments are oft en carried out not for a single\nenvironment but for many environments drawn from an environment class . For example, to Environment class\nevaluate a taxi driver in simulated trafc, we would want to r un many simulations with dif-\nferent trafc, lighting, and weather conditions. W e are the n interested in the agents average\nperformance over the environment class.\n2.4 The Structure of Agents\nSo far we have talked about agents by describing behaviorthe action that is performed after\nany given sequence of percepts. Now we must bite the bullet an d talk about how the insides\nwork. The job of AI is to design an agent program that implements the agent function Agent program\nthe mapping from percepts to actions. W e assume this program will run on some sort of\ncomputing device with physical sensors and actuatorswe ca ll this the agent architecture : Agent architecture\nagent = architecture+ program.\nObviously , the program we choose has to be one that is appropr iate for the architecture. If the\nprogram is going to recommend actions like W alk, the architecture had better have legs. The\narchitecture might be just an ordinary PC, or it might be a rob otic car with several onboard\ncomputers, cameras, and other sensors. In general, the arch itecture makes the percepts from\nthe sensors available to the program, runs the program, and f eeds the programs action choices\nto the actuators as they are generated. Most of this book is ab out designing agent programs,\nalthough Chapters 25 and 26 deal directly with the sensors an d actuators.\n\n48 Chapter 2 Intelligent Agents\nfunction TA BL E -D RIV E N -A G E N T( percept) returns an action\npersistent: percepts, a sequence, initially empty\ntable, a table of actions, indexed by percept sequences, initiall y fully specied\nappend percept to the end of percepts\naction  LO O K U P( percepts, table)\nreturn action\nFigure 2.7 The T A BL E -D RIV E N -A G E N T program is invoked for each new percept and re-\nturns an action each time. It retains the complete percept se quence in memory.\n2.4.1 Agent programs\nThe agent programs that we design in this book all have the sam e skeleton: they take the\ncurrent percept as input from the sensors and return an actio n to the actuators. 5 Notice the\ndifference between the agent program, which takes the curre nt percept as input, and the agent\nfunction, which may depend on the entire percept history . Th e agent program has no choice\nbut to take just the current percept as input because nothing more is available from the envi-\nronment; if the agents actions need to depend on the entire p ercept sequence, the agent will\nhave to remember the percepts.\nW e describe the agent programs in the simple pseudocode lang uage that is dened in\nAppendix B. (The online code repository contains implement ations in real programming\nlanguages.) For example, Figure 2.7 shows a rather trivial a gent program that keeps track of\nthe percept sequence and then uses it to index into a table of a ctions to decide what to do.\nThe tablean example of which is given for the vacuum world in Figure 2.3represents\nexplicitly the agent function that the agent program embodi es. T o build a rational agent in\nthis way , we as designers must construct a table that contain s the appropriate action for every\npossible percept sequence.\nIt is instructive to consider why the table-driven approach to agent construction is doomed\nto failure. Let Pbe the set of possible percepts and let T be the lifetime of the agent (the total\nnumber of percepts it will receive). The lookup table will co ntain  T\nt =1 |P|t entries. Consider\nthe automated taxi: the visual input from a single camera (ei ght cameras is typical) comes\nin at the rate of roughly 70 megabytes per second (30 frames pe r second, 1080 720 pixels\nwith 24 bits of color information). This gives a lookup table with over 10 600,000,000,000 entries\nfor an hours driving. Even the lookup table for chessa tiny , well-behaved fragment of the\nreal worldhas (it turns out) at least 10 150 entries. In comparison, the number of atoms in\nthe observable universe is less than 10 80 . The daunting size of these tables means that (a) no\nphysical agent in this universe will have the space to store t he table; (b) the designer would\nnot have time to create the table; and (c) no agent could ever l earn all the right table entries\nfrom its experience.\nDespite all this, T A B LE -D R IV EN -AG E N T does do what we want, assuming the table is\nlled in correctly: it implements the desired agent functio n.\n5 There are other choices for the agent program skeleton; for e xample, we could have the agent programs be\ncoroutines that run asynchronously with the environment. Each such cor outine has an input and output port and\nconsists of a loop that reads the input port for percepts and w rites actions to the output port.\n\nSection 2.4 The Structure of Agents 49\nfunction RE FL E X -VACU U M -A G E N T( [location,status]) returns an action\nif status = Dirty then return Suck\nelse if location = A then return Right\nelse if location = B then return Left\nFigure 2.8 The agent program for a simple reex agent in the two-locatio n vacuum environ-\nment. This program implements the agent function tabulated in Figure 2.3.\nThe key challenge for AI is to nd out how to write programs tha t, to the extent possible,\nproduce rational behavior from a smallish program rather th an from a vast table.\nW e have many examples showing that this can be done successfu lly in other areas: for\nexample, the huge tables of square roots used by engineers an d schoolchildren prior to the\n1970s have now been replaced by a ve-line program for Newton s method running on elec-\ntronic calculators. The question is, can AI do for general in telligent behavior what Newton\ndid for square roots? W e believe the answer is yes.\nIn the remainder of this section, we outline four basic kinds of agent programs that em-\nbody the principles underlying almost all intelligent syst ems:\n Simple reex agents;\n Model-based reex agents;\n Goal-based agents; and\n Utility-based agents.\nEach kind of agent program combines particular components i n particular ways to generate\nactions. Section 2.4.6 explains in general terms how to conv ert all these agents into learning\nagents that can improve the performance of their components so as to generate better actions.\nFinally , Section 2.4.7 describes the variety of ways in whic h the components themselves can\nbe represented within the agent. This variety provides a maj or organizing principle for the\neld and for the book itself.\n2.4.2 Simple reex agents\nThe simplest kind of agent is the simple reex agent . These agents select actions on the basis Simple reex agent\nof the current percept, ignoring the rest of the percept history . For examp le, the vacuum agent\nwhose agent function is tabulated in Figure 2.3 is a simple re ex agent, because its decision\nis based only on the current location and on whether that loca tion contains dirt. An agent\nprogram for this agent is shown in Figure 2.8.\nNotice that the vacuum agent program is very small indeed com pared to the correspond-\ning table. The most obvious reduction comes from ignoring th e percept history , which cuts\ndown the number of relevant percept sequences from 4 T to just 4. A further, small reduc-\ntion comes from the fact that when the current square is dirty , the action does not depend on\nthe location. Although we have written the agent program usi ng if-then-else statements, it is\nsimple enough that it can also be implemented as a Boolean cir cuit.\nSimple reex behaviors occur even in more complex environme nts. Imagine yourself as\nthe driver of the automated taxi. If the car in front brakes an d its brake lights come on, then\nyou should notice this and initiate braking. In other words, some processing is done on the\n\n50 Chapter 2 Intelligent Agents\nAgent\nEnvironment\nSensors\nWhat action I\nshould do nowCondition-action rules\nActuators\nWhat the world\nis like now\nFigure 2.9 Schematic diagram of a simple reex agent. W e use rectangles to denote the\ncurrent internal state of the agents decision process, and ovals to represent the background\ninformation used in the process.\nvisual input to establish the condition we call The car in fr ont is braking.  Then, this triggers\nsome established connection in the agent program to the acti on initiate braking.  W e call\nsuch a connection a conditionaction rule ,6 written asConditionaction\nrule\nif car-in-front-is-braking then initiate-braking.\nHumans also have many such connections, some of which are lea rned responses (as for driv-\ning) and some of which are innate reexes (such as blinking wh en something approaches the\neye). In the course of the book, we show several different way s in which such connections\ncan be learned and implemented.\nThe program in Figure 2.8 is specic to one particular vacuum environment. A more\ngeneral and exible approach is rst to build a general-purp ose interpreter for condition\naction rules and then to create rule sets for specic task env ironments. Figure 2.9 gives the\nstructure of this general program in schematic form, showin g how the conditionaction rules\nallow the agent to make the connection from percept to action . Do not worry if this seems\ntrivial; it gets more interesting shortly .\nAn agent program for Figure 2.9 is shown in Figure 2.10. The I N TER PR ET -I N P U T\nfunction generates an abstracted description of the curren t state from the percept, and the\nRU LE -M ATC H function returns the rst rule in the set of rules that matche s the given state\ndescription. Note that the description in terms of rules a nd matching is purely concep-\ntual; as noted above, actual implementations can be as simpl e as a collection of logic gates\nimplementing a Boolean circuit. Alternatively , a neural circuit can be used, where the logic\ngates are replaced by the nonlinear units of articial neura l networks (see Chapter 21).\nSimple reex agents have the admirable property of being sim ple, but they are of limited\nintelligence. The agent in Figure 2.10 will work only if the correct decision can be made on\nthe basis of just the current perceptthat is, only if the env ironment is fully observable.\n6 Also called situationaction rules , productions, or ifthen rules .\n\nSection 2.4 The Structure of Agents 51\nfunction SIM P L E -R E FL E X -A G E N T( percept) returns an action\npersistent: rules, a set of conditionaction rules\nstate  IN T E RP RE T -I N P U T( percept)\nrule  RU L E -M AT CH(state, rules)\naction  rule.ACT IO N\nreturn action\nFigure 2.10 A simple reex agent. It acts according to a rule whose condit ion matches the\ncurrent state, as dened by the percept.\nEven a little bit of unobservability can cause serious troub le. For example, the braking\nrule given earlier assumes that the condition car-in-front-is-braking can be determined from\nthe current percepta single frame of video. This works if th e car in front has a centrally\nmounted (and hence uniquely identiable) brake light. Unfo rtunately , older models have\ndifferent congurations of taillights, brake lights, and t urn-signal lights, and it is not always\npossible to tell from a single image whether the car is brakin g or simply has its taillights\non. A simple reex agent driving behind such a car would eithe r brake continuously and\nunnecessarily , or, worse, never brake at all.\nW e can see a similar problem arising in the vacuum world. Supp ose that a simple reex\nvacuum agent is deprived of its location sensor and has only a dirt sensor. Such an agent\nhas just two possible percepts: [Dirty] and [Clean]. It can Suck in response to [Dirty]; what\nshould it do in response to [Clean]? Moving Left fails (forever) if it happens to start in square\nA, and moving Right fails (forever) if it happens to start in square B. Innite loops are often\nunavoidable for simple reex agents operating in partially observable environments.\nEscape from innite loops is possible if the agent can randomize its actions. For exam- Randomization\nple, if the vacuum agent perceives [Clean], it might ip a coin to choose between Right and\nLeft. It is easy to show that the agent will reach the other square i n an average of two steps.\nThen, if that square is dirty , the agent will clean it and the t ask will be complete. Hence, a\nrandomized simple reex agent might outperform a determini stic simple reex agent.\nW e mentioned in Section 2.3 that randomized behavior of the r ight kind can be rational in\nsome multiagent environments. In single-agent environmen ts, randomization is usually not\nrational. It is a useful trick that helps a simple reex agent in some situations, but in most\ncases we can do much better with more sophisticated determin istic agents.\n2.4.3 Model-based reex agents\nThe most effective way to handle partial observability is fo r the agent to keep track of the\npart of the world it cant see now . That is, the agent should maintain some sort of internal\nstate that depends on the percept history and thereby reects at le ast some of the unobserved Internal state\naspects of the current state. For the braking problem, the in ternal state is not too extensive\njust the previous frame from the camera, allowing the agent t o detect when two red lights at\nthe edge of the vehicle go on or off simultaneously . For other driving tasks such as changing\nlanes, the agent needs to keep track of where the other cars ar e if it cant see them all at once.\nAnd for any driving to be possible at all, the agent needs to ke ep track of where its keys are.\n\n52 Chapter 2 Intelligent Agents\nAgent\nEnvironment\nSensors\nHow the world evolves\nWhat my actions do\nCondition-action rules\nActuators\nWhat the world\nis like now\nIWhat action \nshould do now\nState\nFigure 2.11 A model-based reex agent.\nUpdating this internal state information as time goes by req uires two kinds of knowledge\nto be encoded in the agent program in some form. First, we need some information about how\nthe world changes over time, which can be divided roughly int o two parts: the effects of the\nagents actions and how the world evolves independently of t he agent. For example, when the\nagent turns the steering wheel clockwise, the car turns to th e right, and when its raining the\ncars cameras can get wet. This knowledge about how the worl d workswhether imple-\nmented in simple Boolean circuits or in complete scientic t heoriesis called a transition\nmodel of the world.Transition model\nSecond, we need some information about how the state of the wo rld is reected in the\nagents percepts. For example, when the car in front initiat es braking, one or more illumi-\nnated red regions appear in the forward-facing camera image , and, when the camera gets\nwet, droplet-shaped objects appear in the image partially o bscuring the road. This kind of\nknowledge is called a sensor model .Sensor model\nT ogether, the transition model and sensor model allow an age nt to keep track of the state\nof the worldto the extent possible given the limitations of the agents sensors. An agent\nthat uses such models is called a model-based agent .Model-based agent\nFigure 2.11 gives the structure of the model-based reex age nt with internal state, show-\ning how the current percept is combined with the old internal state to generate the updated\ndescription of the current state, based on the agents model of how the world works. The agent\nprogram is shown in Figure 2.12. The interesting part is the f unction U PDATE -S TATE , which\nis responsible for creating the new internal state descript ion. The details of how models and\nstates are represented vary widely depending on the type of e nvironment and the particular\ntechnology used in the agent design.\nRegardless of the kind of representation used, it is seldom p ossible for the agent to deter-\nmine the current state of a partially observable environmen t exactly. Instead, the box labeled\nwhat the world is like now (Figure 2.11) represents the age nts best guess (or sometimes\nbest guesses, if the agent entertains multiple possibiliti es). For example, an automated taxi\n\nSection 2.4 The Structure of Agents 53\nfunction MO D E L -BA S E D-R E FL E X -A G E N T( percept) returns an action\npersistent: state, the agents current conception of the world state\ntransition model, a description of how the next state depends on\nthe current state and action\nsensor model, a description of how the current world state is reected\nin the agents percepts\nrules, a set of conditionaction rules\naction, the most recent action, initially none\nstate  UP DAT E-S TAT E(state, action, percept, transition model, sensor model)\nrule  RU L E -M AT CH(state, rules)\naction  rule.ACT IO N\nreturn action\nFigure 2.12 A model-based reex agent. It keeps track of the current stat e of the world,\nusing an internal model. It then chooses an action in the same way as the reex agent.\nmay not be able to see around the large truck that has stopped i n front of it and can only guess\nabout what may be causing the hold-up. Thus, uncertainty abo ut the current state may be\nunavoidable, but the agent still has to make a decision.\n2.4.4 Goal-based agents\nKnowing something about the current state of the environmen t is not always enough to decide\nwhat to do. For example, at a road junction, the taxi can turn l eft, turn right, or go straight\non. The correct decision depends on where the taxi is trying t o get to. In other words,\nas well as a current state description, the agent needs some s ort of goal information that Goal\ndescribes situations that are desirablefor example, bein g at a particular destination. The\nagent program can combine this with the model (the same infor mation as was used in the\nmodel-based reex agent) to choose actions that achieve the goal. Figure 2.13 shows the\ngoal-based agents structure.\nSometimes goal-based action selection is straightforward for example, when goal sat-\nisfaction results immediately from a single action. Someti mes it will be more trickyfor\nexample, when the agent has to consider long sequences of twi sts and turns in order to nd a\nway to achieve the goal. Search (Chapters 3 to 5) and planning (Chapter 11) are the subelds\nof AI devoted to nding action sequences that achieve the age nts goals.\nNotice that decision making of this kind is fundamentally di fferent from the condition\naction rules described earlier, in that it involves conside ration of the futureboth What will\nhappen if I do such-and-such? and Will that make me happy? In the reex agent designs,\nthis information is not explicitly represented, because th e built-in rules map directly from\npercepts to actions. The reex agent brakes when it sees brak e lights, period. It has no idea\nwhy . A goal-based agent brakes when it sees brake lights beca use thats the only action that\nit predicts will achieve its goal of not hitting other cars.\nAlthough the goal-based agent appears less efcient, it is m ore exible because the\nknowledge that supports its decisions is represented expli citly and can be modied. For\nexample, a goal-based agents behavior can easily be change d to go to a different destination,\n\n54 Chapter 2 Intelligent Agents\nAgent\nEnvironment\nSensors\nWhat action I\nshould do now\nState\nHow the world evolves\nWhat my actions do\nActuators\nWhat the world\nis like now\nWhat it will be like\n  if I do action A\nGoals\nFigure 2.13 A model-based, goal-based agent. It keeps track of the world state as well as\na set of goals it is trying to achieve, and chooses an action th at will (eventually) lead to the\nachievement of its goals.\nsimply by specifying that destination as the goal. The reex agents rules for when to turn\nand when to go straight will work only for a single destinatio n; they must all be replaced to\ngo somewhere new .\n2.4.5 Utility-based agents\nGoals alone are not enough to generate high-quality behavio r in most environments. For\nexample, many action sequences will get the taxi to its desti nation (thereby achieving the\ngoal), but some are quicker, safer, more reliable, or cheape r than others. Goals just provide a\ncrude binary distinction between happy and unhappy sta tes. A more general performance\nmeasure should allow a comparison of different world states according to exactly how happy\nthey would make the agent. Because happy does not sound ver y scientic, economists and\ncomputer scientists use the term utility instead.7Utility\nW e have already seen that a performance measure assigns a sco re to any given sequence\nof environment states, so it can easily distinguish between more and less desirable ways of\ngetting to the taxis destination. An agents utility function is essentially an internalizationUtility function\nof the performance measure. Provided that the internal util ity function and the external per-\nformance measure are in agreement, an agent that chooses act ions to maximize its utility will\nbe rational according to the external performance measure.\nLet us emphasize again that this is not the only way to be rationalwe have already seen\na rational agent program for the vacuum world (Figure 2.8) th at has no idea what its utility\nfunction isbut, like goal-based agents, a utility-based a gent has many advantages in terms\nof exibility and learning. Furthermore, in two kinds of cas es, goals are inadequate but a\nutility-based agent can still make rational decisions. Fir st, when there are conicting goals,\nonly some of which can be achieved (for example, speed and saf ety), the utility function\nspecies the appropriate tradeoff. Second, when there are s everal goals that the agent can\n7 The word utility here refers to the quality of being usefu l,  not to the electric company or waterworks.\n\nSection 2.4 The Structure of Agents 55\nAgent\nEnvironment\nSensors\nHow happy I will be\nin such a state\nState\nHow the world evolves\nWhat my actions do\nUtility\nActuators\nWhat action I\nshould do now\nWhat it will be like\nif I do action A\nWhat the world\nis like now\nFigure 2.14 A model-based, utility-based agent. It uses a model of the wo rld, along with a\nutility function that measures its preferences among state s of the world. Then it chooses the\naction that leads to the best expected utility, where expect ed utility is computed by averaging\nover all possible outcome states, weighted by the probabili ty of the outcome.\naim for, none of which can be achieved with certainty , utilit y provides a way in which the\nlikelihood of success can be weighed against the importance of the goals.\nPartial observability and nondeterminism are ubiquitous i n the real world, and so, there-\nfore, is decision making under uncertainty . T echnically sp eaking, a rational utility-based\nagent chooses the action that maximizes the expected utility of the action outcomesthat Expected utility\nis, the utility the agent expects to derive, on average, give n the probabilities and utilities of\neach outcome. (Appendix A denes expectation more precisel y .) In Chapter 16, we show\nthat any rational agent must behave as if it possesses a utility function whose expected value\nit tries to maximize. An agent that possesses an explicit utility function can make rational de-\ncisions with a general-purpose algorithm that does not depe nd on the specic utility function\nbeing maximized. In this way , the global denition of rati onalitydesignating as rational\nthose agent functions that have the highest performanceis turned into a local constraint\non rational-agent designs that can be expressed in a simple p rogram.\nThe utility-based agent structure appears in Figure 2.14. U tility-based agent programs\nappear in Chapters 16 and 17, where we design decision-makin g agents that must handle the\nuncertainty inherent in nondeterministic or partially obs ervable environments. Decision mak-\ning in multiagent environments is also studied in the framew ork of utility theory , as explained\nin Chapter 18.\nAt this point, the reader may be wondering, Is it that simple ? W e just build agents that\nmaximize expected utility , and were done? Its true that s uch agents would be intelligent,\nbut its not simple. A utility-based agent has to model and ke ep track of its environment,\ntasks that have involved a great deal of research on percepti on, representation, reasoning,\nand learning. The results of this research ll many of the cha pters of this book. Choosing\nthe utility-maximizing course of action is also a difcult t ask, requiring ingenious algorithms\nthat ll several more chapters. Even with these algorithms, perfect rationality is usually\n\n56 Chapter 2 Intelligent Agents\nPerformance standard\nAgent\nEnvironment\nSensors\nPerformance\nelement\nchanges\nknowledge\nlearning\n  goals\nProblem\ngenerator \nfeedback\n  Learning  \nelement\nCritic\nActuators\nFigure 2.15 A general learning agent. The performance element box rep resents what we\nhave previously considered to be the whole agent program. No w , the learning element box\ngets to modify that program to improve its performance.\nunachievable in practice because of computational complex ity , as we noted in Chapter 1. W e\nalso note that not all utility-based agents are model-based ; we will see in Chapters 22 and 26\nthat a model-free agent can learn what action is best in a particular situation witho ut everModel-free agent\nlearning exactly how that action changes the environment.\nFinally , all of this assumes that the designer can specify th e utility function correctly;\nChapters 17, 18, and 22 consider the issue of unknown utility functions in more depth.\n2.4.6 Learning agents\nW e have described agent programs with various methods for se lecting actions. W e have\nnot, so far, explained how the agent programs come into being . In his famous early paper,\nTuring (1950) considers the idea of actually programming hi s intelligent machines by hand.\nHe estimates how much work this might take and concludes, So me more expeditious method\nseems desirable.  The method he proposes is to build learnin g machines and then to teach\nthem. In many areas of AI, this is now the preferred method for creating state-of-the-art\nsystems. Any type of agent (model-based, goal-based, utili ty-based, etc.) can be built as a\nlearning agent (or not).\nLearning has another advantage, as we noted earlier: it allo ws the agent to operate in ini-\ntially unknown environments and to become more competent th an its initial knowledge alone\nmight allow . In this section, we briey introduce the main id eas of learning agents. Through-\nout the book, we comment on opportunities and methods for lea rning in particular kinds of\nagents. Chapters 1922 go into much more depth on the learnin g algorithms themselves.\nA learning agent can be divided into four conceptual compone nts, as shown in Fig-\nure 2.15. The most important distinction is between the learning element , which is re-Learning element\nsponsible for making improvements, and the performance element , which is responsible forPerformance\nelement\nselecting external actions. The performance element is wha t we have previously considered\n\nSection 2.4 The Structure of Agents 57\nto be the entire agent: it takes in percepts and decides on act ions. The learning element uses\nfeedback from the critic on how the agent is doing and determines how the performance Critic\nelement should be modied to do better in the future.\nThe design of the learning element depends very much on the de sign of the performance\nelement. When trying to design an agent that learns a certain capability , the rst question is\nnot How am I going to get it to learn this? but What kind of pe rformance element will my\nagent use to do this once it has learned how? Given a design fo r the performance element,\nlearning mechanisms can be constructed to improve every par t of the agent.\nThe critic tells the learning element how well the agent is do ing with respect to a xed\nperformance standard. The critic is necessary because the p ercepts themselves provide no\nindication of the agents success. For example, a chess prog ram could receive a percept\nindicating that it has checkmated its opponent, but it needs a performance standard to know\nthat this is a good thing; the percept itself does not say so. I t is important that the performance\nstandard be xed. Conceptually , one should think of it as bei ng outside the agent altogether\nbecause the agent must not modify it to t its own behavior.\nThe last component of the learning agent is the problem generator . It is responsible Problem generator\nfor suggesting actions that will lead to new and informative experiences. If the performance\nelement had its way , it would keep doing the actions that are b est, given what it knows, but\nif the agent is willing to explore a little and do some perhaps suboptimal actions in the short\nrun, it might discover much better actions for the long run. T he problem generators job is to\nsuggest these exploratory actions. This is what scientists do when they carry out experiments.\nGalileo did not think that dropping rocks from the top of a tow er in Pisa was valuable in itself.\nHe was not trying to break the rocks or to modify the brains of u nfortunate pedestrians. His\naim was to modify his own brain by identifying a better theory of the motion of objects.\nThe learning element can make changes to any of the knowledg e components shown\nin the agent diagrams (Figures 2.9, 2.11, 2.13, and 2.14). Th e simplest cases involve learning\ndirectly from the percept sequence. Observation of pairs of successive states of the environ-\nment can allow the agent to learn What my actions do and How the world evolves in\nresponse to its actions. For example, if the automated taxi e xerts a certain braking pressure\nwhen driving on a wet road, then it will soon nd out how much de celeration is actually\nachieved, and whether it skids off the road. The problem gene rator might identify certain\nparts of the model that are in need of improvement and suggest experiments, such as trying\nout the brakes on different road surfaces under different co nditions.\nImproving the model components of a model-based agent so tha t they conform better\nwith reality is almost always a good idea, regardless of the e xternal performance standard.\n(In some cases, it is better from a computational point of vie w to have a simple but slightly\ninaccurate model rather than a perfect but endishly comple x model.) Information from the\nexternal standard is needed when trying to learn a reex comp onent or a utility function.\nFor example, suppose the taxi-driving agent receives no tip s from passengers who have\nbeen thoroughly shaken up during the trip. The external perf ormance standard must inform\nthe agent that the loss of tips is a negative contribution to i ts overall performance; then the\nagent might be able to learn that violent maneuvers do not con tribute to its own utility . In\na sense, the performance standard distinguishes part of the incoming percept as a reward Reward\n(or penalty) that provides direct feedback on the quality of the agents behavior. Hard-wired Penalty\nperformance standards such as pain and hunger in animals can be understood in this way .\n\n58 Chapter 2 Intelligent Agents\nMore generally , human choices can provide information about human preferences. For\nexample, suppose the taxi does not know that people generall y dont like loud noises, and\nsettles on the idea of blowing its horn continuously as a way o f ensuring that pedestrians\nknow its coming. The consequent human behaviorcovering e ars, using bad language, and\npossibly cutting the wires to the hornwould provide eviden ce to the agent with which to\nupdate its utility function. This issue is discussed furthe r in Chapter 22.\nIn summary , agents have a variety of components, and those co mponents can be repre-\nsented in many ways within the agent program, so there appear s to be great variety among\nlearning methods. There is, however, a single unifying them e. Learning in intelligent agents\ncan be summarized as a process of modication of each compone nt of the agent to bring the\ncomponents into closer agreement with the available feedba ck information, thereby improv-\ning the overall performance of the agent.\n2.4.7 How the components of agent programs work\nW e have described agent programs (in very high-level terms) as consisting of various compo-\nnents, whose function it is to answer questions such as: Wha t is the world like now? What\naction should I do now? What do my actions do? The next ques tion for a student of AI\nis, How on Earth do these components work? It takes about a t housand pages to begin to\nanswer that question properly , but here we want to draw the re aders attention to some basic\ndistinctions among the various ways that the components can represent the environment that\nthe agent inhabits.\nRoughly speaking, we can place the representations along an axis of increasing complex-\nity and expressive poweratomic, factored, and structured . T o illustrate these ideas, it helps\nto consider a particular agent component, such as the one tha t deals with What my actions\ndo.  This component describes the changes that might occur i n the environment as the result\nof taking an action, and Figure 2.16 provides schematic depi ctions of how those transitions\nmight be represented.\nB C\n(a) Atomic (b) Factored (c) Structured\nB C\nFigure 2.16 Three ways to represent states and the transitions between t hem. (a) Atomic\nrepresentation: a state (such as B or C) is a black box with no i nternal structure; (b) Factored\nrepresentation: a state consists of a vector of attribute va lues; values can be Boolean, real-\nvalued, or one of a xed set of symbols. (c) Structured repres entation: a state includes\nobjects, each of which may have attributes of its own as well a s relationships to other objects.\n\nSection 2.4 The Structure of Agents 59\nIn an atomic representation each state of the world is indivisibleit has no internal Atomic\nrepresentation\nstructure. Consider the task of nding a driving route from o ne end of a country to the other\nvia some sequence of cities (we address this problem in Figur e 3.1 on page 64). For the pur-\nposes of solving this problem, it may sufce to reduce the sta te of the world to just the name of\nthe city we are ina single atom of knowledge, a black box wh ose only discernible prop-\nerty is that of being identical to or different from another b lack box. The standard algorithms\nunderlying search and game-playing (Chapters 35), hidden Markov models (Chapter 14),\nand Markov decision processes (Chapter 17) all work with ato mic representations.\nA factored representation splits up each state into a xed set of variables or attributes, Factored\nrepresentation\nVariable\nAttribute\neach of which can have a value. Consider a higher-delity description for the same drivin g\nValue\nproblem, where we need to be concerned with more than just ato mic location in one city or\nanother; we might need to pay attention to how much gas is in th e tank, our current GPS\ncoordinates, whether or not the oil warning light is working , how much money we have for\ntolls, what station is on the radio, and so on. While two diffe rent atomic states have nothing in\ncommonthey are just different black boxestwo different f actored states can share some\nattributes (such as being at some particular GPS location) a nd not others (such as having lots\nof gas or having no gas); this makes it much easier to work out h ow to turn one state into an-\nother. Many important areas of AI are based on factored repre sentations, including constraint\nsatisfaction algorithms (Chapter 6), propositional logic (Chapter 7), planning (Chapter 11),\nBayesian networks (Chapters 1216), and various machine le arning algorithms.\nFor many purposes, we need to understand the world as having things in it that are re-\nlated to each other, not just variables with values. For example, w e might notice that a large\ntruck ahead of us is reversing into the driveway of a dairy far m, but a loose cow is block-\ning the trucks path. A factored representation is unlikely to be pre-equipped with the at-\ntribute TruckAheadBackingIntoDairyF armDrivewayBlockedByLooseCow with value true or\nfalse. Instead, we would need a structured representation , in which objects such as cows Structured\nrepresentation\nand trucks and their various and varying relationships can b e described explicitly (see Fig-\nure 2.16(c)). Structured representations underlie relati onal databases and rst-order logic\n(Chapters 8, 9, and 10), rst-order probability models (Cha pter 15), and much of natural lan-\nguage understanding (Chapters 23 and 24). In fact, much of wh at humans express in natural\nlanguage concerns objects and their relationships.\nAs we mentioned earlier, the axis along which atomic, factor ed, and structured repre-\nsentations lie is the axis of increasing expressiveness. Roughly speaking, a more expressive Expressiveness\nrepresentation can capture, at least as concisely , everyth ing a less expressive one can capture,\nplus some more. Often, the more expressive language is much more concise; for example, the\nrules of chess can be written in a page or two of a structured-r epresentation language such\nas rst-order logic but require thousands of pages when writ ten in a factored-representation\nlanguage such as propositional logic and around 10 38 pages when written in an atomic lan-\nguage such as that of nite-state automata. On the other hand , reasoning and learning become\nmore complex as the expressive power of the representation i ncreases. T o gain the benets\nof expressive representations while avoiding their drawba cks, intelligent systems for the real\nworld may need to operate at all points along the axis simulta neously .\nAnother axis for representation involves the mapping of con cepts to locations in physical\nmemory , whether in a computer or in a brain. If there is a one-t o-one mapping between\nconcepts and memory locations, we call that a localist representation . On the other hand, Localist\nrepresentation\n\n60 Chapter 2 Intelligent Agents\nif the representation of a concept is spread over many memory locations, and each memory\nlocation is employed as part of the representation of multip le different concepts, we call\nthat a distributed representation . Distributed representations are more robust against nois eDistributed\nrepresentation\nand information loss. With a localist representation, the m apping from concept to memory\nlocation is arbitrary , and if a transmission error garbles a few bits, we might confuse Truck\nwith the unrelated concept Truce. But with a distributed representation, you can think of eac h\nconcept representing a point in multidimensional space, an d if you garble a few bits you move\nto a nearby point in that space, which will have similar meani ng.\nSummary\nThis chapter has been something of a whirlwind tour of AI, whi ch we have conceived of as\nthe science of agent design. The major points to recall are as follows:\n An agent is something that perceives and acts in an environment. The agent function\nfor an agent species the action taken by the agent in respons e to any percept sequence.\n The performance measure evaluates the behavior of the agent in an environment. A\nrational agent acts so as to maximize the expected value of the performance m easure,\ngiven the percept sequence it has seen so far.\n A task environment specication includes the performance measure, the extern al en-\nvironment, the actuators, and the sensors. In designing an a gent, the rst step must\nalways be to specify the task environment as fully as possibl e.\n T ask environments vary along several signicant dimensio ns. They can be fully or par-\ntially observable, single-agent or multiagent, determini stic or nondeterministic, episodic\nor sequential, static or dynamic, discrete or continuous, a nd known or unknown.\n In cases where the performance measure is unknown or hard to specify correctly , there\nis a signicant risk of the agent optimizing the wrong object ive. In such cases the agent\ndesign should reect uncertainty about the true objective.\n The agent program implements the agent function. There exists a variety of bas ic\nagent program designs reecting the kind of information mad e explicit and used in the\ndecision process. The designs vary in efciency , compactne ss, and exibility . The\nappropriate design of the agent program depends on the natur e of the environment.\n Simple reex agents respond directly to percepts, whereas model-based reex agents\nmaintain internal state to track aspects of the world that ar e not evident in the current\npercept. Goal-based agents act to achieve their goals, and utility-based agents try to\nmaximize their own expected happiness. \n All agents can improve their performance through learning.\nBibliographical and Historical Notes\nThe central role of action in intelligencethe notion of pra ctical reasoninggoes back at\nleast as far as Aristotles Nicomachean Ethics . Practical reasoning was also the subject of\nMcCarthys inuential paper Programs with Common Sense ( 1958). The elds of robotics\nand control theory are, by their very nature, concerned prin cipally with physical agents. The\n\nBibliographical and Historical Notes 61\nconcept of a controller in control theory is identical to that of an agent in AI. Perha ps sur- Controller\nprisingly , AI has concentrated for most of its history on iso lated components of agents\nquestion-answering systems, theorem-provers, vision sys tems, and so onrather than on\nwhole agents. The discussion of agents in the text by Geneser eth and Nilsson (1987) was an\ninuential exception. The whole-agent view is now widely ac cepted and is a central theme in\nrecent texts (Padgham and Winikoff, 2004; Jones, 2007; Pool e and Mackworth, 2017).\nChapter 1 traced the roots of the concept of rationality in ph ilosophy and economics. In\nAI, the concept was of peripheral interest until the mid-198 0s, when it began to suffuse many\ndiscussions about the proper technical foundations of the  eld. A paper by Jon Doyle (1983)\npredicted that rational agent design would come to be seen as the core mission of AI, while\nother popular topics would spin off to form new disciplines.\nCareful attention to the properties of the environment and t heir consequences for ra-\ntional agent design is most apparent in the control theory tr aditionfor example, classical\ncontrol systems (Dorf and Bishop, 2004; Kirk, 2004) handle f ully observable, deterministic\nenvironments; stochastic optimal control (Kumar and V arai ya, 1986; Bertsekas and Shreve,\n2007) handles partially observable, stochastic environme nts; and hybrid control (Henzinger\nand Sastry , 1998; Cassandras and Lygeros, 2006) deals with e nvironments containing both\ndiscrete and continuous elements. The distinction between fully and partially observable en-\nvironments is also central in the dynamic programming literature developed in the eld of\noperations research (Puterman, 1994), which we discuss in C hapter 17.\nAlthough simple reex agents were central to behaviorist ps ychology (see Chapter 1),\nmost AI researchers view them as too simple to provide much le verage. (Rosenschein (1985)\nand Brooks (1986) questioned this assumption; see Chapter 2 6.) A great deal of work\nhas gone into nding efcient algorithms for keeping track o f complex environments (Bar-\nShalom et al. , 2001; Choset et al. , 2005; Simon, 2006), most of it in the probabilistic setting .\nGoal-based agents are presupposed in everything from Arist otles view of practical rea-\nsoning to McCarthys early papers on logical AI. Shakey the R obot (Fikes and Nilsson,\n1971; Nilsson, 1984) was the rst robotic embodiment of a log ical, goal-based agent. A\nfull logical analysis of goal-based agents appeared in Gene sereth and Nilsson (1987), and a\ngoal-based programming methodology called agent-oriente d programming was developed by\nShoham (1993). The agent-based approach is now extremely po pular in software engineer-\ning (Ciancarini and W ooldridge, 2001). It has also inltrat ed the area of operating systems,\nwhere autonomic computing refers to computer systems and networks that monitor and con - Autonomic\ncomputing\ntrol themselves with a perceiveact loop and machine learni ng methods (Kephart and Chess,\n2003). Noting that a collection of agent programs designed t o work well together in a true\nmultiagent environment necessarily exhibits modularity the programs share no internal state\nand communicate with each other only through the environmen tit is common within the\neld of multiagent systems to design the agent program of a single agent as a collection o f\nautonomous sub-agents. In some cases, one can even prove tha t the resulting system gives\nthe same optimal solutions as a monolithic design.\nThe goal-based view of agents also dominates the cognitive p sychology tradition in the\narea of problem solving, beginning with the enormously inu ential Human Problem Solv-\ning (Newell and Simon, 1972) and running through all of Newells later work (Newell, 1990).\nGoals, further analyzed as desires (general) and intentions (currently pursued), are central to\nthe inuential theory of agents developed by Michael Bratma n (1987).\n\n62 Chapter 2 Intelligent Agents\nAs noted in Chapter 1, the development of utility theory as a b asis for rational behavior\ngoes back hundreds of years. In AI, early research eschewed u tilities in favor of goals, with\nsome exceptions (Feldman and Sproull, 1977). The resurgenc e of interest in probabilistic\nmethods in the 1980s led to the acceptance of maximization of expected utility as the most\ngeneral framework for decision making (Horvitz et al. , 1988). The text by Pearl (1988) was\nthe rst in AI to cover probability and utility theory in dept h; its exposition of practical meth-\nods for reasoning and decision making under uncertainty was probably the single biggest\nfactor in the rapid shift towards utility-based agents in th e 1990s (see Chapter 16). The for-\nmalization of reinforcement learning within a decision-th eoretic framework also contributed\nto this shift (Sutton, 1988). Somewhat remarkably , almost a ll AI research until very recently\nhas assumed that the performance measure can be exactly and c orrectly specied in the form\nof a utility function or reward function (Hadeld-Menell et al. , 2017a; Russell, 2019).\nThe general design for learning agents portrayed in Figure 2 .15 is classic in the machine\nlearning literature (Buchanan et al. , 1978; Mitchell, 1997). Examples of the design, as em-\nbodied in programs, go back at least as far as Arthur Samuels (1959, 1967) learning program\nfor playing checkers. Learning agents are discussed in dept h in Chapters 1922.\nSome early papers on agent-based approaches are collected b y Huhns and Singh (1998)\nand W ooldridge and Rao (1999). T exts on multiagent systems p rovide a good introduction to\nmany aspects of agent design (W eiss, 2000a; W ooldridge, 200 9). Several conference series\ndevoted to agents began in the 1990s, including the Internat ional W orkshop on Agent The-\nories, Architectures, and Languages (A T AL), the Internati onal Conference on Autonomous\nAgents (AGENTS), and the International Conference on Multi -Agent Systems (ICMAS). In\n2002, these three merged to form the International Joint Con ference on Autonomous Agents\nand Multi-Agent Systems (AAMAS). From 2000 to 2012 there wer e annual workshops on\nAgent-Oriented Software Engineering (AOSE). The journal Autonomous Agents and Multi-\nAgent Systems was founded in 1998. Finally , Dung Beetle Ecology (Hanski and Cambefort,\n1991) provides a wealth of interesting information on the be havior of dung beetles. Y ouTube\nhas inspiring video recordings of their activities.	CHAPTER 2\nINTELLIGENT AGENTS\nIn which we discuss the nature of agents, perfect or otherwis e, the diversity of environments,\nand the resulting menagerie of agent types.\nChapter 1 identied the concept of rational agents as central to our approach to articial\nintelligence. In this chapter, we make this notion more conc rete. W e will see that the concept\nof rationality can be applied to a wide variety of agents oper ating in any imaginable environ-\nment. Our plan in this book is to use this conc	en	0.9	uploaded	14410	84725	2025-11-15 16:08:54.671528	2025-11-15 16:08:54.671533	\N	1	\N	\N	\N	1	original	\N	\N	\N	\N	\N	{}	bc904d76-b2b3-48ff-be0b-572acb1e20c9
177	Black's Law Dictionary 12th Edition (2024) - Agent	file	reference	legal_dictionary	pdf	AGENT 2024.pdf	uploads/23c56c04e8ad4a16_AGENT 2024.pdf	185574	{"year": 2024, "discipline": "Law", "upload_order": 6}	AGENT, Black's Law Dictionary (12th ed. 2024)\n  2024 Thomson Reuters. No claim to original U.S. Government Works. 1\nBlack's Law Dictionary (12th ed. 2024), agent\nAGENT\nBryan A. Garner, Editor in Chief\nPreface to the Twelfth Edition | Guide to the Dictionary | Legal Maxims | Bibliography of Books Cited\nagent (15c)  1. Something that produces an effect <an intervening agent>. See cause (1); electronic agent. 2. Someone who is\nauthorized to act for or in place of another; a representative <a professional athlete's agent>.  Also termed commissionaire.\nSee agency. Cf. principal, n.(1); employee.\nGenerally speaking, anyone can be an agent who is in fact capable of performing the functions involved. The agent normally\nbinds not himself but his principal by the contracts he makes; it is therefore not essential that he be legally capable to contract\n(although his duties and liabilities to his principal might be affected by his status). Thus an infant or a lunatic may be an agent,\nthough doubtless the court would disregard either's attempt to act if he were so young or so hopelessly devoid of reason as to\nbe completely incapable of grasping the function he was attempting to perform. Floyd R. Mechem, Outlines of the Law of\nAgency 89 (Philip Mechem ed., 4th ed. 1952).\nThe etymology of the word agent or agency tells us much. The words are derived from the Latin verb, ago, agere; the noun\nagens, agentis. The word agent denotes one who acts, a doer, force or power that accomplishes things. Harold Gill Reuschlein\n& William A. Gregory, The Law of Agency and Partnership  1, at 23 (2d ed. 1990).\n- agent not recognized (2002) Patents. A patent applicant's appointed agent who is not registered to practice before the U.S.\nPatent and Trademark Office.  A power of attorney appointing an unregistered agent is void. See patent agent.\n- agent of necessity (1857) An agent that the law empowers to act for the benefit of another in an emergency.  Also termed\nagent by necessity.\n- apparent agent (1823) Someone who reasonably appears to have authority to act for another, regardless of whether actual\nauthority has been conferred.  Also termed ostensible agent; implied agent.\n- associate agent (1993) Patents. An agent who is registered to practice before the U.S. Patent and Trademark Office, has been\nappointed by a primary agent, and is authorized to prosecute a patent application through the filing of a power of attorney.  An\nassociate agent is often used by outside counsel to assist in-house counsel. See patent agent.\n- bail-enforcement agent See bounty hunter.\n- bargaining agent (1935) A labor union in its capacity of representing employees in collective bargaining.\n- broker-agent See broker.\n- business agent See business agent.\n- case agent See case agent.\n- clearing agent (1937) Securities. A person or company acting as an intermediary in a securities transaction or providing\nfacilities for comparing data regarding securities transactions.  The term includes a custodian of securities in connection with\nthe central handling of securities. Securities Exchange Act  3(a)(23)(A) (15 USCA  78c(a)(23)(A)).  Also termed clearing\nagency.\n- closing agent  (1922) An agent who represents the purchaser or buyer in the negotiation and closing of a real-property\ntransaction by handling financial calculations and transfers of documents.  Also termed settlement agent. See also settlement\nattorney under attorney.\n- coagent (16c) Someone who shares with another agent the authority to act for the principal.  A coagent may be appointed by\nthe principal or by another agent who has been authorized to make the appointment.  Also termed dual agent. Cf. common\nagent.\n- commercial agent (18c)  1. broker. 2. A consular officer responsible for the commercial interests of his or her country at a\nforeign port. 3. See mercantile agent. 4. See commission agent.\n\nAGENT, Black's Law Dictionary (12th ed. 2024)\n  2024 Thomson Reuters. No claim to original U.S. Government Works. 2\n- commission agent (1812) An agent whose remuneration is based at least in part on commissions, or percentages of actual\nsales.  Commission agents typically work as middlemen between sellers and buyers.  Also termed commercial agent.\n- common agent (17c) An agent who acts on behalf of more than one principal in a transaction. Cf. coagent.\n- corporate agent (1819) An agent authorized to act on behalf of a corporation; broadly, all employees and officers who have\nthe power to bind the corporation.\n- county agent See juvenile officer under officer (1).\n- del credere agent (del kred--ray or kray-d-ray) (1822) An agent who guarantees the solvency of the third party with whom\nthe agent makes a contract for the principal.  A del credere agent receives possession of the principal's goods for purposes\nof sale and guarantees that anyone to whom the agent sells the goods on credit will pay promptly for them. For this guaranty,\nthe agent receives a higher commission for sales. The promise of such an agent is almost universally held not to be within the\nstatute of frauds.  Also termed del credere factor.\n- diplomatic agent  (18c) A national representative in one of four categories: (1) ambassadors, (2) envoys and ministers\nplenipotentiary, (3) ministers resident accredited to the sovereign, or (4) chargs d'affaires accredited to the minister of foreign\naffairs.\n- double agent (1935)  1. A spy who finds out an enemy's secrets for his or her principal but who also gives secrets to the\nenemy. 2. See dual agent (2).\n- dual agent (1881)  1. See coagent. 2. An agent who represents both parties in a single transaction, esp. a buyer and a seller.\n Also termed (in sense 2) double agent.\n- emigrant agent (1874) One engaged in the business of hiring laborers for work outside the country or state.\n- enrolled agent See enrolled agent.\n- escrow agent See escrow agent.\n- estate agent See real-estate agent.\n- fiscal agent (18c) A bank or other financial institution that collects and disburses money and services as a depository of private\nand public funds on another's behalf.\n- foreign agent (1938) Someone who registers with the federal government as a lobbyist representing the interests of a foreign\ncountry or corporation.\n- forwarding agent  (1837)  1. freight forwarder . 2. A freight-forwarder who assembles less-than-carload shipments (small\nshipments) into carload shipments, thus taking advantage of lower freight rates.\n- general agent (17c)  1. An agent authorized to transact all the principal's business of a particular kind or in a particular place.\n Among the common types of general agents are factors, brokers, and partners. Cf. special agent (1). 2. Insurance. An agent\nwith the general power of making insurance contracts on behalf of an insurer.\n- government agent (1805)  1. An employee or representative of a governmental body. 2. A law-enforcement official, such as\na police officer or an FBI agent. 3. An informant, esp. an inmate, used by law enforcement to obtain incriminating statements\nfrom another inmate.\n- gratuitous agent (1822) An agent who acts without a right to compensation.\n- high-managerial agent (1957)  1. An agent of a corporation or other business who has authority to formulate corporate policy\nor supervise employees.  Also termed superior agent. 2. See superior agent (1).\n- implied agent See apparent agent.\n- independent agent (17c) An agent who exercises personal judgment and is subject to the principal only for the results of the\nwork performed. Cf. nonservant agent.\n- innocent agent (1805) Criminal law. A person whose action on behalf of a principal is unlawful but does not merit prosecution\nbecause the agent had no knowledge of the principal's illegal purpose; a person who lacks the mens rea for an offense but who\nis tricked or coerced by the principal into committing a crime.  Although the agent's conduct was unlawful, the agent might\nnot be prosecuted if the agent had no knowledge of the principal's illegal purpose. The principal is legally accountable for the\ninnocent agent's actions. See Model Penal Code  2.06(2)(a).\n- insurance agent (1866) Someone authorized by an insurer to sell its policies; specif., an insurer's representative who solicits or\nprocures insurance business, including the continuance, renewal, and revival of policies.  Also termed producer; (in property\ninsurance) recording agent; record agent.\n- jural agent See jural agent.\n\nAGENT, Black's Law Dictionary (12th ed. 2024)\n  2024 Thomson Reuters. No claim to original U.S. Government Works. 3\n- land agent See land agent.\n- listing agent (1927) The real-estate broker's representative who obtains a listing agreement with the owner. Cf. selling agent;\nshowing agent.\n- local agent (1804)  1. An agent appointed to act as another's (esp. a company's) representative and to transact business within\na specified district. 2. See special agent (2).\n- managing agent (1812)  1. A person with general power involving the exercise of judgment and discretion, as opposed to an\nordinary agent who acts under the direction and control of the principal.  Also termed business agent. 2. See underwriting\nagent (2).\n- managing general agent (1954) Insurance. A wholesale insurance intermediary who is vested with underwriting authority\nfrom an insurer.  Managing general agents allow small insurers to purchase underwriting expertise. They typically become\ninvolved in policies that require specialized expertise, as with those for professional liability.  Abbr. MGA.\n- member's agent See underwriting agent (3).\n- mercantile agent (18c) An agent employed to sell goods or merchandise on behalf of the principal.  Also termed commercial\nagent.\n- nonservant agent (1920) An agent who agrees to act on the principal's behalf but is not subject to the principal's control over\nhow the task is performed.  A principal is not liable for the physical torts of a nonservant agent. See independent contractor. Cf.\nindependent agent; servant.\n- ostensible agent See apparent agent.\n- patent agent (1859) A specialized legal professional  not necessarily a lawyer  who has fulfilled the U.S. Patent and\nTrademark Office requirements as a representative and is registered to prepare and prosecute patent applications before the\nPTO.  To be registered to practice before the PTO, a candidate must establish mastery of the relevant technology (by holding\na specified technical degree or equivalent training) in order to advise and assist patent applicants. The candidate must also pass\na written examination (the Patent Bar) that tests knowledge of patent law and PTO procedure.  Often shortened to agent.\n Also termed registered patent agent; patent solicitor. Cf. patent attorney.\n- policywriting agent See underwriting agent (1).\n- primary agent (18c) An agent who is directly authorized by a principal.  A primary agent generally may hire a subagent to\nperform all or part of the agency. Cf. subagent (1).\n- private agent (17c) An agent acting for an individual in that person's private affairs.\n- process agent (1886) A person authorized to accept service of process on behalf of another. See registered agent.\n- procuring agent (1954) Someone who obtains drugs on behalf of another person and delivers the drugs to that person.  In\ncriminal-defense theory, the procuring agent does not sell, barter, exchange, or make a gift of the drugs to the other person\nbecause the drugs already belong to that person, who merely employs the agent to pick up and deliver them.\n- public agent (17c) A person appointed to act for the public in matters relating to governmental administration or public\nbusiness.\n- real-estate agent (1844) An agent who represents a buyer or seller (or both, with proper disclosures) in the sale or lease of\nreal property.  A real-estate agent can be either a broker (whose principal is a buyer or seller) or a salesperson (whose principal\nis a broker).  Also termed estate agent. Cf. realtor; real-estate broker under broker.\n- record agent See insurance agent.\n- registered agent (1809) A person authorized to accept service of process for another person, esp. a foreign corporation, in a\nparticular jurisdiction.  Also termed resident agent. See process agent.\n- registered patent agent See patent agent.\n- resident agent See registered agent.\n- secret agent See secret agent.\n- self-appointed agent (18c)  1. Someone who is not authorized to act on behalf of another person or entity but who behaves\nas if such authority has been granted. 2. An agent appointed directly by a principal who also has a statutory agent. 3. A plaintiff\nin a class-action lawsuit who purports to represent the entire class.\n- selling agent (1839)  1. The real-estate broker's representative who sells the property, as opposed to the agent who lists the\nproperty for sale. 2. See showing agent. Cf. listing agent.\n- settlement agent (1952) See closing agent.\n\nAGENT, Black's Law Dictionary (12th ed. 2024)\n  2024 Thomson Reuters. No claim to original U.S. Government Works. 4\n- showing agent (1901) A real-estate broker's representative who markets property to a prospective purchaser.  A showing\nagent may be characterized as a subagent of the listing broker, as an agent who represents the purchaser, or as an intermediary\nwho owes an agent's duties to neither seller nor buyer.  Also termed selling agent. Cf. listing agent.\n- soliciting agent (1855)  1. Insurance. An agent with authority relating to the solicitation or submission of applications to an\ninsurance company but usu. without authority to bind the insurer, as by accepting the applications on behalf of the company.\n2. An agent who solicits orders for goods or services for a principal.  3. A managing agent of a corporation for purposes of\nservice of process.\n- special agent (17c)  1. An agent employed to conduct a particular transaction or to perform a specified act. Cf. general agent\n(1). 2. Insurance. An agent whose powers are usu. confined to soliciting applications for insurance, taking initial premiums,\nand delivering policies when issued.  Also termed (in sense 2) local agent; solicitor.\n- specially accredited agent (1888) An agent that the principal has specially invited a third party to deal with, in an implication\nthat the third party will be notified if the agent's authority is altered or revoked.\n- statutory agent (1844) An agent designated by law to receive litigation documents and other legal notices for a nonresident\ncorporation.  In most states, the secretary of state is the statutory agent for such corporations. Cf. agency by operation of law\n(1) under agency (1).\n- stock-transfer agent (1873) See transfer agent.\n- subagent (18c)  1. A person to whom an agent has delegated the performance of an act for the principal; a person designated by\nan agent to perform some duty relating to the agency.  If the principal consents to a primary agent's employment of a subagent,\nthe subagent owes fiduciary duties to the principal, and the principal is liable for the subagent's acts.  Also termed subservant.\nCf. primary agent; subordinate agent.\nBy delegation  the agent is permitted to use agents of his own in performing the function he is employed to perform for\nhis principal, delegating to them the discretion which normally he would be expected to exercise personally. These agents are\nknown as subagents to indicate that they are the agent's agents and not the agents of the principal. Normally (though of course\nnot necessarily) they are paid by the agent. The agent is liable to the principal for any injury done him by the misbehavior of\nthe agent's subagents. Floyd R. Mechem, Outlines of the Law of Agency  79, at 51 (Philip Mechem ed., 4th ed. 1952).\n2. See buyer's broker under broker.\n- subordinate agent (17c) An agent who acts subject to the direction of a superior agent.  Subordinate and superior agents are\ncoagents of a common principal. See superior agent. Cf. subagent (1).\n- successor agent (1934) An agent who is appointed by a principal to act in a primary agent's stead if the primary agent is\nunable or unwilling to perform.\n- superior agent (17c)  1. An agent on whom a principal confers the right to direct a subordinate agent. See subordinate agent.\n2. See high-managerial agent (1).\n- transfer agent (1850) An organization (such as a bank or trust company) that handles transfers of shares for a publicly held\ncorporation by issuing new certificates and overseeing the cancellation of old ones and that usu. also maintains the record of\nshareholders for the corporation and mails dividend checks.  Generally, a transfer agent ensures that certificates submitted for\ntransfer are properly indorsed and that the transfer right is appropriately documented.  Also termed stock-transfer agent.\n- trustee-agent A trustee who is subject to the control of the settlor or one or more beneficiaries of a trust. See trustee (1).\n- undercover agent (1930)  1. An agent who does not disclose his or her role as an agent.  2. A police officer who gathers\nevidence of criminal activity without disclosing his or her identity to the suspect.\n- underwriting agent (1905) Insurance.  1. An agent who acts on behalf of an insurance company to provide insurance to a\ncustomer.  Also termed policywriting agent. 2. An agent who acts for an individual Lloyd's underwriter and manages the\nunderwriting syndicate of which the underwriter is a member.  Also termed managing agent. See lloyd's underwriters. 3. An\nagent who acts for an individual Lloyd's underwriter in all respects except for managing the underwriting syndicate.  Also\ntermed (in sense 3) member's agent. See lloyd's underwriters.\n- undisclosed agent (1863) An agent who deals with a third party who has no knowledge that the agent is acting on a principal's\nbehalf. Cf. undisclosed principal under principal (1).\n- universal agent (18c) An agent authorized to perform all acts that the principal could personally perform.\n\nAGENT, Black's Law Dictionary (12th ed. 2024)\n  2024 Thomson Reuters. No claim to original U.S. Government Works. 5\n- vice-commercial agent (1800) Hist. In the consular service of the United States, a consular officer who was substituted\ntemporarily to fill the place of a commercial agent who was absent or had been relieved from duty.\nWestlaw.  2024 Thomson Reuters. No Claim to Orig. U.S. Govt. Works.\nEnd of Document  2024 Thomson Reuters. No claim to original U.S. Government Works.	AGENT, Black's Law Dictionary (12th ed. 2024)\n  2024 Thomson Reuters. No claim to original U.S. Government Works. 1\nBlack's Law Dictionary (12th ed. 2024), agent\nAGENT\nBryan A. Garner, Editor in Chief\nPreface to the Twelfth Edition | Guide to the Dictionary | Legal Maxims | Bibliography of Books Cited\nagent (15c)  1. Something that produces an effect <an intervening agent>. See cause (1); electronic agent. 2. Someone who is\nauthorized to act for or in place of another; a representative <a profe	en	0.9	uploaded	3035	18473	2025-11-15 16:08:54.728977	2025-11-15 16:08:54.728981	\N	1	\N	\N	\N	1	original	\N	\N	\N	\N	\N	{}	4469ef88-987f-4325-bc0b-9208499e0131
178	OED Entry: agent, n. & adj. (2024)	file	reference	dictionary	pdf	agent, n. & adj. meanings, etymology and more _ Oxford English Dictionary.pdf	uploads/2b28c36a7ec645e2_agent, n. & adj. meanings, etymology and more _ Oxford English Dictionary.pdf	1401982	{"year": 2024, "discipline": "Lexicography", "upload_order": 7}	Etymology\nSummary\nOf multiple origins. Partly a borrowing from French. Partly a borrowing from Latin.\nEtymons:Frenchagent; Latinagent-, age ns, agere.\n< (i) Middle Frenchagent (Frenchagent) (noun) person acting on behalf of another, representative,\nemissary (1332 in an isolated attestation, subsequently (apparently after Italian) from 1578), person who\nor thing which acts upon someone or something (c1370, originally and frequently in philosophical\ncontexts), substance that brings about a chemical effect or causes a chemical reaction (1612 (in the\npassage translated in quot. 1624 at sense A.4) or earlier; rare before early 19th cent.), person who\nintrigues (1640), (adjective) that acts, that exerts power (1337; c1450 in grammar; second half of the 15th\ncent. in cause agent (compare quot. 1535 at sense B)),\nand its etymon (ii) classical Latinagent-, age ns acting, active, (masculine noun) pleader, advocate, in\npost-classical Latin also representative, ofcial (4th cent.), administrator of an estate, employee of a\nchurch (6th cent.), (neuter noun) (in philosophy) instrumentality, cause (from 8th cent. in British\nsources; also in continental sources), uses as adjective and noun of present participle of agere to act, do\n(see actv.).\nWith sense A.1a and corresponding adjectival use compare earlier patientn. and patientadj.\nNotes\nParallels in other European languages.\nCompare Catalanagent, adjective and noun (14th cent.), Spanishagente (late 14th cent. as noun, early\n15th cent. as adjective), Portugueseagente, adjective and noun (15th cent.), Italianagente (a1294 as\nadjective, a1328 as noun). Compare also Dutchagent (noun) ofcial, representative (1570), GermanAgent\n(masculine noun) representative, emissary (1546), spy (18th cent., now the usual sense), Agens (neuter\nnoun) person who or thing which acts upon someone or something (1598).\nMeaning & use\nNOUN\nagent\nNOUN & ADJECTIVE1\nOxford University Press uses cookies to enhance your experience on our website. By selecting accept all you\nare agreeing to our use of cookies. You can change your cookie settings at any time. More information can be\nfound in our Cookie Policy.\nAccept All Cookie Settings\n10/9/24, 7:26 PM agent, n. & adj. meanings, etymology and more | Oxford English Dictionary\nhttps://www.oed.com/dictionary/agent_n1?tab=meaning_and_use#8694696 1/21\n\n1.a.A person who or thing which acts upon someone or something; one who or that which\nexerts power; the doer of an action. Sometimes contrasted with the patient (instrument,\netc.) undergoing the action. Cf. actorn. 3a.\nEarliest in Alchemy: a force capable of acting upon matter, an active principle. Now chiey in philosophical and\nsociological contexts.\n1.b.A person or thing that operates in a particular direction, or produces a specied effect; the\ncause of some process or change. Frequently with for, in, of.\nSometimes dicult to distinguish from the means or agency by which an eect is produced: cf. sense A.3.\na1500\nThe fyrst [kind of combining] is callyd by phylosophers dyptatyve be-twyxte ye agent & ye\npacyent.\nG. Ripley, Compend of Alchemy (Ashmole MS.) l. 718\nThe forgeuenes of oure sinnes..is onely gods worke & we nothing els but patientes & not\nagentes.\nJ. Bradford, Godlie Medit. Lordes Prayer (1562) sig. Q.ii\nFor he maketh foure originals, whereof three are agents, and the last passiue and\nmateriall.\nW. Raleigh, History of Worldi.i. i. 6. 6\nNor are we to be meer instruments moved by the will of those in authority..but are morall\nAgents.\nS. Bolton, Arraignment of Errour 295\nHe that is not free is not an agent, but a patient.\nJ. Wesley, Serm. Several Occasionsvol. V. 177\nAgent and Patient, when the same person is the doer of a thing, and the party to whom\ndone: as where a woman endows herself of the best part of her husband's possessions.\nT. E. Tomlins, Jacob's Law-dictionary\nIn conformity with this view, the distinction between agent and patient, between\nsomething which acts and some other thing which is acted upon, is formally abolished.\nF. C. Bowen, Logic xii. 401\nWe are..conversant with the fact in human aairs that whenever purpose is involved there\nis an intelligent agent.\nPopular Science Monthly April 379\nIt is silly to berate the hurricane for irresponsibility... It..cannot be a true agent; it cannot\nauthor or own an action.\nC. T. Sistare, Responsibility & Criminal Liability ii. iv. 15\nIt is only an exercise of power if the agent gets the subject to do something whether or not\nthe subject wants to do it.\nJ. R. Searle, Making Social World vii. 152\na1500\n(1471)\na1555\n1614\n1646\n1788\n1809\n1870\n1909\n1989\n2010\n1571\nOxford University Press uses cookies to enhance your experience on our website. By selecting accept all you\nare agreeing to our use of cookies. You can change your cookie settings at any time. More information can be\nfound in our Cookie Policy.\n10/9/24, 7:26 PM agent, n. & adj. meanings, etymology and more | Oxford English Dictionary\nhttps://www.oed.com/dictionary/agent_n1?tab=meaning_and_use#8694696 2/21\n\n1.c.Grammar. The doer of an action, typically expressed as the subject of an active verb or in\na by-phrase with a passive verb.\nCf. agent nounn.\nFaieth is produced and brought foorth by the grace of God, as chiefe agent and worker\nthereof.\nW. Fulke, Confut. Popishe Libelle (new edition) f. 108\nI stepped back againe into the garden,..leauing them still agents of these vnkind villanies.\nR. Greene, Philomela sig. F4\nThe re warmeth more at a neer, then at a remoter distance: Naturall agents work not in\ndistans.\nA. Ross, Philosophicall Touch-stone 35\nWhether or no the Shape can by Physical Agents be alter'd.., yet mentally both..can be done.\nR. Boyle, Origine of Formes & Qualities 9\nWhen the Samians invaded Zancle, a..great Agent in that aair was Hippocrates.\nR. Bentley, Dissertation upon Epistles of Phalaris (new edition) 155\nI was still to be the wilful Agent of all my own Miseries.\nD. Defoe, Life Robinson Crusoe 43\nNor can I think, that any body has such an idea of chance, as to make it an agent or really\nexisting and acting cause of any thing.\nW. Wollaston, Religion of Nature v. 60\nSuccessful production..depends more on the qualities of the human agents, than on the\ncircumstances in which they work.\nJ. S. Mill, Principles of Political Economyvol. I.i. vii. 123\nThe Rhizopods were important agents in the accumulation of beds of limestone.\nJ. W. Dawson, Life's Dawn on Earth vi. 134\nThe glacier will be ecient as the agent for dbris removal.\nJournal of Geology (Chicago) vol. 12 574\nThe key idea of man as the agent for the whole future of evolution.\nJ. S. Huxley, Human Crisis 19\nAt Cambridge..I had no theories about theatre as an agent of social or political change.\nS. Fry, Fry Chronicles 94\n1571\nv\n1592\nv\n1645\n1666\n1699\n1719\n1722\n1848\n1875\n1904\n1963\n2010\nc1620\nThe active verb adheres to the person of the agent; As, Christ hath conquered hel and\ndeath.\nA. Hume, Of Orthographie Britan Tongue (1870) ii. x. 8\nJohn and Peter (1 The Agent.) travelled together to (2 The Verb.) Rome.\nF. Lodowyck, Ground-work New Perfect Language 15\nc1620\n1652\nOxford University Press uses cookies to enhance your experience on our website. By selecting accept all you\nare agreeing to our use of cookies. You can change your cookie settings at any time. More information can be\nfound in our Cookie Policy.\n10/9/24, 7:26 PM agent, n. & adj. meanings, etymology and more | Oxford English Dictionary\nhttps://www.oed.com/dictionary/agent_n1?tab=meaning_and_use#8694696 3/21\n\ngrammar\n1.d.Parapsychology. In telepathy: the person who originates an impression (opposed to the\npercipient who receives it).\nparapsychology\n2.A person acting on behalf of another.\n2.a.A person who acts as a substitute for another; one who undertakes negotiations or\ntransactions on behalf of a superior, employer, or principal; a deputy, steward,\nrepresentative; (in early use) an ambassador, emissary. Also gurative. Now chiey in legal\ncontexts.\nIn Scots Law: a solicitor, advocate (now rare).\narmy, crown, land, parliamentary agent, etc.: see the rst element.\nReective..action returns upon the agent that produces it, as, I atter myself & c.\nC. Wiseman, Compl. English Grammar 155\nAn active verb..necessarily supposes an agent, and an object acted upon; as..I praise John.\nD. Fenning, New Gram. English Tongue 32\nIt often becomes necessary to state the object of a verb active, or the agent of a verb\npassive. Hence arises the necessity for..the accusative and the ablative.\nEncyclopdia Metropolitana (1847) vol. I. 33/1\nWith an intransitive verb the subject is as much a patient as an agent. I walk is as much I\ncause my walking as I experience my walking.\nW. J. Entwhistle, Aspects of Language vi. 179\nTruck driver is an acceptable (and existing) compound..but child-driver is not\nacceptable..since child is the agent of the verb.\nN. Tsujimura, Introd. Japanese Linguisticsiv. vii. 166\n1764\n1771\n1845\n1953\n2007\n1883\nIn Thought-transference..both parties (whom, for convenience' sake, we will call the Agent\nand the Percipient) are supposed to be in a normal state.\nProceedings of Society for Psychical Research 18823vol. 1 119\nWe call the owner of the impressing mind the agent, and the owner of the impressed mind\nthe percipient.\nE. Gurney et al., Phantasms of Livingvol. I. 6\nSpontaneous cases [of telepathy] do occasionally occur in which no such connection\nbetween apparent agent and apparent percipient can be traced.\nW. H. Salter, Zoar xi. 149\nAnalytical attention..has shifted down the years from agent (sender) to percipient (receiver).\nL. Picknett, Encycl. Paranormal 218/1\n1883\n1886\n1961\n1990\n1523\nOxford University Press uses cookies to enhance your experience on our website. By selecting accept all you\nare agreeing to our use of cookies. You can change your cookie settings at any time. More information can be\nfound in our Cookie Policy.\n10/9/24, 7:26 PM agent, n. & adj. meanings, etymology and more | Oxford English Dictionary\nhttps://www.oed.com/dictionary/agent_n1?tab=meaning_and_use#8694696 4/21\n\n2.b.In commercial use: a person or company that provides a particular service, typically one\nthat involves arranging transactions between two other parties; (also) a person or\ncompany that represents an organization, esp. in a particular region; a business or sales\nrepresentative. Cf. agencyn. I.1b.\nWe have ben with the Cardinall de Medices agentes.\nin State Papers Henry VIII (1849) vol. VI. 181\nIoanna the wyfe of Chusa Herodes agent and factour [Latin procuratoris].\nN. Udall et al., translation of Erasmus, Paraphrase Newe Testamentevol. I. Luke xxiv. f. clxxxiiii\nGoe call the English Agent hether strait.\nC. Marlowe, Massacre at Paris (c1600) sig. D5\nDioclesian..was agent for the Romans in France.\nE. Topsell, Historie of Foure-footed Beastes 698\nMade themselves a prey to their sollicitors and Agents.\nJ. Howell, Instructions for Forreine Travell xix. 230\nMr. John Pain, Agent to the Regiment.\nLondon Gazette mmmmxxviii/4\nAgent, that is, rent-gatherer, to the dean.\nM. Delany, Autobiography & Correspondence (1861) vol. II. 362\nHe..employed a certain Mr. Crabtree as his agent, steward, etc.\nM. R. Mitford in A. G. L'Estrange, Life of Mary Russell Mitford (1870) vol. II. xi. 22\nSince the devil fell from Heaven, he never wanted agents on earth.\nW. Scott, Woodstockvol. I. iii. 93\nI told them atly..that, as Mr. Egerton's agent, I would allow no proceedings that might\nvitiate the election.\nE. Bulwer-Lytton, My Novelvol. IV.xii. xxvii. 191\nAn agent who signs his name to a promissory note, etc. without indicating thereon that\nhe signs as agent, is liable personally on the instrument.\nNegotiable Instruments Act (India) 40\nIn inviting tenders for a target contract the employer, or his agent, prepares a rough\nestimate of the proposed work.\nW. T. Cresswell in R. Greenhalgh, Practical Builder xv. 420/1\nFurthermore, that individual must be..an ocer, employee or agent of such an issuing\nmanager.\nB. A. K. Rider, Insider Trading i. 42\nIn the event of illness, a durable power of attorney enabled her nephew to act as her\nagent.\nM. M. Shenkman, Complete Living Trusts Prog.i. 20\n1523\n1548\na1593\n1607\n1642\n1704\n1745\n1818\n1826\n1853\n1882\n1944\n1983\n2000\n1707\nOxford University Press uses cookies to enhance your experience on our website. By selecting accept all you\nare agreeing to our use of cookies. You can change your cookie settings at any time. More information can be\nfound in our Cookie Policy.\n10/9/24, 7:26 PM agent, n. & adj. meanings, etymology and more | Oxford English Dictionary\nhttps://www.oed.com/dictionary/agent_n1?tab=meaning_and_use#8694696 5/21\n\nFrequently with modifying word or phrase specifying the product or service.\nadvertising, employment, estate, insurance, letting, railroad, shipping, tourist, travel agent, etc.: see the rst\nelement.\n2.c.In colonial North America and subsequently the United States: an ofcial appointed to\nrepresent the government in dealing with an Indigenous people; = Indian agentn. Now\nhistorical.\nMost Bills of Exchange are ordinarily Negotiated by the..Interposition of a certain Set of\nMen commonly called Agents, or Brokers of Exchange.\nA. Justice, General Treatise of Monies 19\nTo prevent trouble, it is requested, that no..advertising Agent, will apply.\nWorld 11 December\nI hope then the agent will give you encouragement about them mines.\nM. Edgeworth, Absentee xii, in Tales of Fashionable Lifevol. VI. 214\nShip Agent and Broker.\nRep. Commissioners 1841 Census: Occup. Abstractsvol. XIII. 281\nRailway Excursion Agents..Cook Thomas & Son.\nList of Subscribers Exchange Syst. (United Telephone Co.) (ed. 6) 181\nWe are agents for the celebrated Scotch Wool Art Rugs.\nRotarian May 64 (advertisement)\nThe rm of Lewisohn Brothers..had been the selling agents for the Montana Copper\nCompany's product.\nQuarterly Journal of Economicsvol. 41 268\nBe completely honest with your agent about how much you really want..to spend. If you\nhave only $500 for a vacation, say so.\nNew York Magazine 31 May 38\nFew..callers have to wait more than 30 seconds to speak to an agent.\nNetwork World 14 December 69\nIt's the house of our dreams... I told the agent we'd meet him three at ve o'clock.\nJ. Mansell, Thinking of You xlvi. 332\n1707\n1789\n1812\n1844\n1885\n1913\n1927\n1971\n1992\n2007\n1707\nThomas Nairne..is..appointed yAgent to reside among y Indians.\nAct regulating Indian Trade (S. Carolina Dept. Archives: S 165001)\nThe agent for Indian aairs in the middle department, be empowered to purchase for\nCaptain White Eyes, two horses.\nJournals Continental Congr. 177489 (Library of Congr.) (1906) vol. IV. 268\nThe agent to the Creek nation, [salary] one thousand eight hundred dollars.\nin Public & General Statutes U.S.A. (1827) vol. III. 1707\n1707 e e\n1776\n1818\nOxford University Press uses cookies to enhance your experience on our website. By selecting accept all you\nare agreeing to our use of cookies. You can change your cookie settings at any time. More information can be\nfound in our Cookie Policy.\n10/9/24, 7:26 PM agent, n. & adj. meanings, etymology and more | Oxford English Dictionary\nhttps://www.oed.com/dictionary/agent_n1?tab=meaning_and_use#8694696 6/21\n\nhistorical\n2.d.A person who works secretly to obtain information for a government or other ofcial\nbody; a spy.\ndouble, secret, treble agent, etc.: see the rst element.\nespionage\n2.e.A person who negotiates and manages business, nancial, publicity, or contractual\nmatters for an actor, performer, writer, etc.\nIn earliest use: a theatrical agent. literary, press, publicity, sports agent, etc.: see the rst element.\nThere can be but one head to an Indian agency, and the agent should be that head, if\ndiscipline is to be maintained.\nCaptain Bell, Report in Nation (1888) 15 March 211/1\nAs one former Peigan chief expressed it, the agents pulled us back instead of pushing us\nforward.\nLethbridge (Alberta) Herald 9 November 5/4\nPressure on the Ioways to sell their land was also increased, as the agent continued to\ninquire if all the agency tribes would not care to sell.\nM. R. Blaine, Ioway Indians viii. 246\nWhen a new Tulalip agent found unmarried men and women living together, he\nthreatened to separate them.\nA. Harmon, Indians in Making vi. 116\n1886\n1968\n1979\n2000\n1804\nThis agent[French agent], spy, and emigrant, who has received his pardon, was already\nknown to the Police.\ntranslation of C. Regnier, Letter 18 April in Revolutionary Plutarch (new edition) vol. III. 215\nWhen despatches from an agent of the enemy are carried by a neutral ship,..the plea of\nignorance [etc.].\nW. Hazlitt & H. P. Roche, Manual Law Maritime Warfare 235\nThe agent clutching his side collapsed at our feet, Sorry! They got me!\nW. H. Auden, Oratorsiii. 108\nSwitzerland..had been full of German agents.\nAnnual Register 1945 230\nRelaying secret information to Russian agents.\nA. H. Compton, Atomic Quest ii. 117\nStudying at various K.G.B. schools, ending at Moscow institute that prepared agents for\nwork abroad.\nVanity Fair (New York) October 382/3\n1804\n1854\n1932\n1946\n1956\n2008\n1825\nOxford University Press uses cookies to enhance your experience on our website. By selecting accept all you\nare agreeing to our use of cookies. You can change your cookie settings at any time. More information can be\nfound in our Cookie Policy.\n10/9/24, 7:26 PM agent, n. & adj. meanings, etymology and more | Oxford English Dictionary\nhttps://www.oed.com/dictionary/agent_n1?tab=meaning_and_use#8694696 7/21\n\ntheatre nance\n2.f.U.S. A stagecoach robber; = road agentn. Now historical.\nU.S. English historical\n3.The means by which something is done; the material cause or instrument through which an\neffect is produced (often implying a rational employer or contriver).\nMr. Schemer, the agent, had no situation for our hero upon his books, but Proteus\nheard..that Mr. Make-a-bill..was in great want of a person at his theatre.\nP. Egan, Life of Actor vi. 220\nBy an early hour of the numbered evening I might have been observed..dining with my\nagent.\nR. L. Stevenson & L. Osbourne, Wrecker vi. 95\nThe name on the door was Abe Riesbitter, Vaudeville Agent, and from the other side of\nthe door came the sound of many voices.\nP. G. Wodehouse, Man with Two Left Feet 34\nMr Watt, my agent, and Mr Faber, my publisher, have Daimlers and country cottages.\nP. Larkin, Letter 28 July in Selected Letters (1992) 120\nHer agent..was nonplussed. Look, baby, he gently chided, we're walking away with one\nmillion..dollars a picture.\nT. Southern, Blue Moviei. viii. 64\nAgents, often to justify their percentage when all they really do for a big star is make a\nphone call, are geniuses when it comes to new things to ask for.\nW. Goldman, Adventures in Screen Trade 18\n[She] was no longer the timid, inexperienced ingnue..protected by her agent.\nC. Field in K. Ferrier, Letters & Diaries iii. 58\n1825\n1892\n1917\n1946\n1970\n1983\n2003\n1876\nThe driver nally succeeded in satisfying the agent that no express box was carried by\nSan Andreas.\nWeekly Calaveras Chronicle (Mokelumne Hill, California) 29 July 3/1\nWe reached it before long, and concluded that the agents, or robbers, had an excellent\neye for position.\nA. A. Hayes, New Colorado (1881) xi. 154\nNex' time I drives stage some of these yere agents massacrees me from behind a bush.\nS. E. White, Blazed Trail Storiesii. iii. 155\nThe agents developed a system of marking departing stagecoaches that were carrying\ntreasure so that confederates would know which ones to stop.\nH. S. Drago, Great Range Wars xviii. 207\n1876\n1880\n1904\n1970\n1579\nOxford University Press uses cookies to enhance your experience on our website. By selecting accept all you\nare agreeing to our use of cookies. You can change your cookie settings at any time. More information can be\nfound in our Cookie Policy.\n10/9/24, 7:26 PM agent, n. & adj. meanings, etymology and more | Oxford English Dictionary\nhttps://www.oed.com/dictionary/agent_n1?tab=meaning_and_use#8694696 8/21\n\nSometimes overlapping with sense A.1b.\n4.Chemistry. A substance that brings about a chemical or physical effect or causes a chemical\nreaction. In later use chiey with preceding modifying word specifying the nature of the\neffect or reaction. Cf. reagentn. 2.\nalkylating, oxidizing, reducing, wetting agent, etc.: see the rst element.\nThe gallowes is no agent or doer in those good thinges.\nW. Fulke, Heskins Parleament Repealed in D. Heskins Ouerthrowne 621\nNot a nayle in it [sc. the Crosse] but is a necessary Agent in the Worlds redemption.\nT. Nashe, Christs Teares 21/1\nHere is her hand, the agent of her heart.\nW. Shakespeare, Two Gentlemen of Verona (1623) i. iii. 46\nGod doth often good works by ill agents.\nJ. Bramhall, Just Vindication of Church of England iii. 43\nWar, which is the agent which must in general be employed upon these occasions,\npresents..an uncertain court of judicature.\nB. Vaughan, Letters Concert of Princes p. iii\nNature..Thro' many agents making strong, Matures the individual form.\nLord Tennyson, Love thou thy Land in Poems (new edition) vol. I. 225\nWhatever thus furnishes us with the rst requisite of production is called a natural agent,\nthat is, something which acts for us and assists us.\nW. S. Jevons, Political Economy 26\nMoney is the agent through which good purposes are made eective.\nIntellectvol. 12 233/2\n[In Marlowe's physiology] the arteries..carry the vital spirit..which is the agent by which the\nsoul eects movement.\nY. Takahashi in S. W. Wells, Shakespeare Surv. 181\n1579\n1593\na1616\n1654\n1793\n1842\n1878\n1920\n2002\n1624\nThe vinegre..is the onely Agent[French l'vnique agent; Latin solum medium aptum] in the whole\nWorld for this Art, that can resolue and reincrudate, or make raw againe the Mettallicke\nBodies.\nE. Orandus, translation of N. Flamel, Expos. Hieroglyphicall Figures St. Innocent's Church-yard 159\nThe agent in the change wrought by Petrication, is..a petric Seed, consisting only in a\nsaxeous odour, or invisible ferment.\nJ. Webster, Metallographia 365\nWater is a most useful agent in chemistry.\nC. Lucas, Essay on Watersi. 81\n1624\n1671\n1756\nOxford University Press uses cookies to enhance your experience on our website. By selecting accept all you\nare agreeing to our use of cookies. You can change your cookie settings at any time. More information can be\nfound in our Cookie Policy.\n10/9/24, 7:26 PM agent, n. & adj. meanings, etymology and more | Oxford English Dictionary\nhttps://www.oed.com/dictionary/agent_n1?tab=meaning_and_use#8694696 9/21\n\nchemistry\n5.Computing. A program that (autonomously) performs a task such as information retrieval or\nprocessing on behalf of a client or user. More fully software agent, user agent.\ncomputing\nADJECTIVE\nActing, exerting power (sometimes contrasted with patientadj. A.2a).\n party agentnounObsoleteLaw the person or party bringing a suit.\nSome observations on the sthenic or asthenic virtue of chemical agents, that is to say, their\nability or impotence to produce irritation.\nMonthly Magazinevol. 3 350/2\nThis quantity is..wholly available in the liquid when used as a bleaching agent.\nM. Faraday, Exper. Research xli. 12. 226\nOxalic acid is mostly to be preferred as the precipitating agent.\nT. Graham, Elements of Chemistry (ed. 2) vol. II. viii. 361\nPiperoxane hydrochloride is an adrenergic-blocking agent of short duration of action.\nDispensatory U.S.A. (ed. 24) vol. II. 2016/2\nXenon tetrauoride, when dissolved in hydrogen uoride, is a moderately strong uorinating\nagent.\nScience 12 October 137/2\nBleaches marketed as color-safe..use weaker oxidizing agents.\nWisconsin State Journal (Nexis) 3 October a2\n1797\n1827\n1858\n1950\n1962\n2011\n1970\nAn algorithm is a set of instructions of nite size, requires a computing agent to react to the\ninstructions, requires facilities (resources) for storage and control, [etc.].\nAdvances in Computersvol. 10 15\nThe model consists of a Message Transfer System and a number of User Agents.\nRequest for Comments (Network Working Group) (Electronic text) No.806. 8\nLittle software agents that scurry back and forth between human and program to retrieve\nprecisely what the human wants.\nWashington Post (Nexis) 2 September g2\nThis can provide a user agent capability so that, for example, the user is informed as soon as\nhe logs in if there is mail waiting for him.\nICL Technical Journalvol. 7 424\n5 per cent is how much more prot a sharetrading software agent can make by learning when\nto bid aggressively and when to cut its losses.\nLondon Lite (Nexis) 3 July 13\n1970\n1981\n1984\n1990\n2008\n1535\nOxford University Press uses cookies to enhance your experience on our website. By selecting accept all you\nare agreeing to our use of cookies. You can change your cookie settings at any time. More information can be\nfound in our Cookie Policy.\n10/9/24, 7:26 PM agent, n. & adj. meanings, etymology and more | Oxford English Dictionary\nhttps://www.oed.com/dictionary/agent_n1?tab=meaning_and_use#8694696 10/21\n\nPronunciation\nBRITISH ENGLISH\n/ed()nt/\nAY-juhnt\nU.S. ENGLISH\n/ed()nt/\nAY-juhnt\nPronunciation keys \nForms\nVariant forms\nlate Middle English agent\n1500s1600s agente\n1600s agentt\nThe fynall necessytie also, and the cause agent[Latin causam agentem] or eectyue wherof.\nW. Marshall, translation of Marsilius of Padua, Defence of Peacei. viii. f. 67\nThe ayre being more thin and liquide then the water, and more vnable to resist, is sooner and\nmore easily aected by externall and agent[Latin agentibus] qualities.\ntranslation of L. Daneau, Dialogue Witches iii. sig. E.vii\nHughe Mill and Elinor his wife the parties agentes in this cause and William delve defendent.\nin B. Cusack, Everyday English 15001700 (1998) 24\nWhat a hot fellow Sol (whom all Agent Causes follow).\nJ. Melton, Astrologaster 13\nThe proper oce of this agent intellect, to serve as an under-labourer to that which is patient.\nJ. Norris, Essay Ideal Worldvol. II. vii. 350\nAgent or patient, singly or one of a crowd.\nT. De Quincey, Confessions Eng. Opium-eater (revised edition) in Selections Grave & Gayvol. V. 83\nThe Philosopher is speaking in that passage not of the agent cause but of the formal cause.\nM. C. Fitzpatrick, translation of St. Thomas Aquinas, On Spiritual Creatures i. 25\nThe [Philippine] people have transmogried from..an identication with the Christ child..into an\nagent force of revolution.\nN. X. M. Tadiar, Things fall Away vii. 290\n1535\nv\n1575\n1615\n1620\n1704\n1856\n1949\n2009\nOxford University Press uses cookies to enhance your experience on our website. By selecting accept all you\nare agreeing to our use of cookies. You can change your cookie settings at any time. More information can be\nfound in our Cookie Policy.\n10/9/24, 7:26 PM agent, n. & adj. meanings, etymology and more | Oxford English Dictionary\nhttps://www.oed.com/dictionary/agent_n1?tab=meaning_and_use#8694696 11/21\n\nFrequency\nagent is one of the 1,000 most common words in modern written English. It is similar in frequency to\nwords like agree, distribution, kill, military, and sell.\nIt typically occurs about 100 times per million words in modern written English.\nagent is in frequency band 7, which contains words occurring between 100 and 1,000 times per million\nwords in modern written English. More about OED's frequency bands\nFrequency data is computed programmatically, and should be regarded as an estimate.\nFrequency of agent, n. & adj., 17502010\n* Occurrences per million words in written English\nHistorical frequency series are derived from Google Books Ngrams (version 2), a data set based on the Google\nBooks corpus of several million books printed in English between 1500 and 2010.\nThe overall frequency for a given word is calculated by summing frequencies for the main form of the word,\nany plural or inected forms, and any major spelling variations.\nFor sets of homographs (distinct entries that share the same word-form, e.g. mole, n., mole, n., mole, n.,\netc.), we have estimated the frequency of each homograph entry as a fraction of the total Ngrams frequency\nfor the word-form. This may result in inaccuracies.Oxford University Press uses cookies to enhance your experience on our website. By selecting accept all you\nare agreeing to our use of cookies. You can change your cookie settings at any time. More information can be\nfound in our Cookie Policy.\n10/9/24, 7:26 PM agent, n. & adj. meanings, etymology and more | Oxford English Dictionary\nhttps://www.oed.com/dictionary/agent_n1?tab=meaning_and_use#8694696 12/21\n\nFrequency of agent, n. & adj., 20172023\n* Occurrences per million words in written English\nModern frequency series are derived from a corpus of 20 billion words, covering the period from 2017 to the\npresent. The corpus is mainly compiled from online news sources, and covers all major varieties of World\nEnglish.\nCompounds & derived words\nSort byDate (oldest rst)\nnihilagent, n. 157980\nA person who does nothing.\nagentry, n. 1590\nThe ofce or function of an agent (in various senses); the activity or occupation of an agent or\nagency; the process or fact of being an agent\nvice-agent, n. 1597\nco-agent, n. & adj. a1600\nJoint agent.\nagentship, n. 1608\nThe position, role, or function of an agent (in various senses); agency. Also: an instance of this.\nOxford University Press uses cookies to enhance your experience on our website. By selecting accept all you\nare agreeing to our use of cookies. You can change your cookie settings at any time. More information can be\nfound in our Cookie Policy.\n10/9/24, 7:26 PM agent, n. & adj. meanings, etymology and more | Oxford English Dictionary\nhttps://www.oed.com/dictionary/agent_n1?tab=meaning_and_use#8694696 13/21\n\nmis-agent, n. 1625\nnon-agent, n. 1632\nsmock-agent, n. 1632\nagent, v. 1637\ntransitive. To act as agent in (some business or process); to conduct or carry out as agent. Also: to\nact as an agent for (a person or project).\nagenting, n. 1646\nThe business or process of acting as an agent (in various senses); the profession of an agent.\nforeign agent, n. 1646\nA person who represents or acts on behalf of one country while located in another; (in later use\nspec.) a person who works secretly to obtain\nfree agent, n. 1649\na. A person able to act freely, as by the exercise of free will, or because of the absence of\nrestriction, constraint, or responsibilities; b. Sport\nreagent, n. 1656\nChemistry. A substance used in testing for other substances, or for reacting with them in a\nparticular way; (more widely) any substance used in\nagent general, n. 1659\nspec. (sometimes with capital initials). Formerly: the representative of a British colony in London\n(now historical). Later: the representative of an\nunder-agent, n. 1677\nA sub-agent.\nsubagent, n. 1683\nA subordinate agent; (U.S. Law) an agent authorized to transact business or otherwise act on\nbehalf of another.\nchemical agent, n. 1728\nA chemical substance producing a specic effect, esp. when intentionally used for this reason;\n(now often) spec. a substance used to incapacitate\ninter-agent, n. 1728\nAn intermediate agent; a go-between, intermediary.\ncommercial agent, n. 1737\nA person or organization authorized to act on another's behalf in matters relating to commerce\nor trade; spec. (U.S.) an ofcial authorized to\ntravelling agent, n. 1737\nA travelling salesperson; a representative who travels on behalf of a company.\nOxford University Press uses cookies to enhance your experience on our website. By selecting accept all you\nare agreeing to our use of cookies. You can change your cookie settings at any time. More information can be\nfound in our Cookie Policy.\n10/9/24, 7:26 PM agent, n. & adj. meanings, etymology and more | Oxford English Dictionary\nhttps://www.oed.com/dictionary/agent_n1?tab=meaning_and_use#8694696 14/21\n\ncrown agent, n. 1753\nAn agent for the Crown; spec. (usually with capital initials) (a) in Scotland, a law ofcer who takes\ncharge of criminal proceedings, acting under\nagentess, n. 1757\nA female agent.\nnavy agent, n. 1765\nA person or rm responsible for managing the nancial affairs of naval ofcers; (formerly also)  a\npaymaster or purser in the U.S. navy (obsolete).\nIndian agent, n. 1766\nAn ofcial authorized to represent the U.S. federal government in its dealings with an Indigenous\npeople; (in Canada) the chief government\nprize agent, n. 1766\nAn agent appointed for the sale of prizes taken in maritime war.\neld agent, n. 1773\nAn agent (now esp. an intelligence agent) who works away from a central ofce or headquarters.\nadvertising agent, n. 1775\npurchasing agent, n. 1777\ncoal agent, n. 1778\nfederal agent, n. 1781\nA representative of the U.S. federal government, (now) esp. a federal law-enforcement ofcer.\nnewspaper agent, n. 1781\nagent noun, n. 1782\nA noun (in English typically one ending in -er or -or) denoting someone or something that\nperforms the action of a verb, as worker, accelerator, etc.\nestate agent, n. 1787\nA person or company involved in the business or profession of arranging the sale, purchase, or\nrental of buildings and land for clients. Also (also\nrevenue agent, n. 1787\nrecruiting agent, n. 1792\nhouse agent, n. 1793\nAn agent employed (by the landlord or owner) in letting or selling a house, collecting rents, etc.;\n(now esp.) an estate agent.\nliterary agent, n. 1794\nOxford University Press uses cookies to enhance your experience on our website. By selecting accept all you\nare agreeing to our use of cookies. You can change your cookie settings at any time. More information can be\nfound in our Cookie Policy.\n10/9/24, 7:26 PM agent, n. & adj. meanings, etymology and more | Oxford English Dictionary\nhttps://www.oed.com/dictionary/agent_n1?tab=meaning_and_use#8694696 15/21\n\nAn agent (now typically a professional one) who acts on behalf of an author in dealing with\npublishers and others involved in promoting his or her\ntheatrical agent, n. 1797\nAn agent whose business is to act as an intermediary between actors looking for work and those\nseeking to employ them.\ncommission agent, n. 1798\n a. = commission broker, n. (a) (obsolete); b. an agent who conducts business or trade for another\nparty on the principle of commission (commission\nbook agent, n. 1810\nA person who promotes the sale of books; (now) spec. a literary agent (cf. agent, n. A.2e).\nforwarding agent, n. 1810\nA person or business that organizes the shipment or transportation of goods.\nnewsagent, n. 1811\nA dealer in newspapers and periodicals, esp. the owner of a shop where these are sold; (now also)\nthe shop itself, usually also selling tobacco\npolice agent, n. 1813\nship-agent, n. 1813\nA shipping agent.\noxidizing agent, n. 1814\nA substance that brings about oxidation and in the process is itself reduced.\npress agent, n. 1814\nA person employed to organize advertising and publicity in the press on behalf of an organization\nor person.\nreducing agent, n. 1816\nA substance that brings about chemical reduction and in the process is itself oxidized; cf.\noxidizing agent, n.\nparliamentary agent, n. 1819\nA person professionally employed to take charge of the interests of a party concerned in or\naffected by any private legislation.\ncounter-agent, n. 1821\nA counteracting agent or force; a counteractant.\nadvertisement agent, n. 1827\nagentless, adj. 1831\nLacking an agent (in various senses); without an agent.\nOxford University Press uses cookies to enhance your experience on our website. By selecting accept all you\nare agreeing to our use of cookies. You can change your cookie settings at any time. More information can be\nfound in our Cookie Policy.\n10/9/24, 7:26 PM agent, n. & adj. meanings, etymology and more | Oxford English Dictionary\nhttps://www.oed.com/dictionary/agent_n1?tab=meaning_and_use#8694696 16/21\n\nbusiness agent, n. 1831\ncustoms agent, n. 1838\n= customs ofcer, n.\nmine agent, n. 1839\nagentive, adj. & n. 1840\nOf or relating to an agent or agency (see agent, n. A.1c); indicating or having the semantic role of\nan agent.\nrailroad agent, n. 1840\nroad agent, n. 1840\n a. An agent or driver for a stagecoach company (obsolete); b. a robber who steals from travellers\nor holds up vehicles on the road (now historical).\nrogue agent, n. 1840\nstation agent, n. 1840\na. Chiey U.S. a person in charge of a railway or (formerly) stagecoach station; b. a person who\nworks for (a particular branch of) an intelligence\nning agent, n. 1843\nA substance used to clarify a liquid; spec. (a) a substance used to remove organic compounds\nfrom a liquid, esp. beer or wine, to improve the clarity\nfreight agent, n. 1843\nshipping-agent, n. 1843\nA licensed agent who transacts a ship's business for the owner.\ngoods agent, n. 1844\nintelligence agent, n. 1844\npatent agent, n. 1845\nland-agent, n. 1846\nA steward or manager of landed property; also, an agent for the sale of land, an estate agent.\nchange agent, n. 1847\nA person who initiates social or political change within a group or institution.\npay agent, n. 1847\nAn ofcial who pays wages; (now chiey U.S., more fully the President's pay agent) a panel\nresponsible for advising the U.S. president on rates of\nbureau agent, n. 1848\nAn agent or ofcial who works for a bureau; esp. an FBI agent (cf. bureau, n. I.2d).\nOxford University Press uses cookies to enhance your experience on our website. By selecting accept all you\nare agreeing to our use of cookies. You can change your cookie settings at any time. More information can be\nfound in our Cookie Policy.\n10/9/24, 7:26 PM agent, n. & adj. meanings, etymology and more | Oxford English Dictionary\nhttps://www.oed.com/dictionary/agent_n1?tab=meaning_and_use#8694696 17/21\n\nbooking agent, n. 1849\na. A person who or a business which arranges transport or travel for goods or passengers, or sells\ntickets in advance for concerts, plays, or other Frequently derogatory in early use, denoting\nagents for railway or shipping companies who issued tickets or passes which were greatly\noverpriced or invalid; cf. booker, n. 3a.\npassenger agent, n. 1852\nxing agent, n. 1855\nbaggage-agent, n. 1858\nemployment agent, n. 1859\nAn individual acting as a professional intermediary between applicants for work and employers.\nclaim-agent, n. 1860\nmatrimonial agent, n. 1860\npersonation agent, n. 1864\nAn ofcial employed at an election to detect people attempting to vote under a false name.\nadvance agent, n. 1865\nAn agent who is sent on ahead of a main party (cf. advance man, n.); also gurative.\ntransfer agent, n. 1869\ninformation agent, n. 1871\nmission-agent, n. 1871\nlecture agent, n. 1873\npublicity agent, n. 1877\nagent word, n. 1879\nA word that indicates agency or active force; esp. a word that denotes the doer of an action; =\nagent noun, n.\npersonating agent, n. 1879\n= personation agent, n.\nrental agent, n. 1880\nbittering agent, n. 1883\na. Any substance added to beer or other alcohol to give it a bitter avour; cf. bittering, n. 2; b. a\nsubstance added to a (typically toxic)\ntourist agent, n. 1884\nraising agent, n. 1885\nOxford University Press uses cookies to enhance your experience on our website. By selecting accept all you\nare agreeing to our use of cookies. You can change your cookie settings at any time. More information can be\nfound in our Cookie Policy.\n10/9/24, 7:26 PM agent, n. & adj. meanings, etymology and more | Oxford English Dictionary\nhttps://www.oed.com/dictionary/agent_n1?tab=meaning_and_use#8694696 18/21\n\nA substance, such as yeast or baking powder, which is used in dough or batter to make it rise\nduring (and sometimes before) baking.\ntravel agent, n. 1885\nA person who owns or works for a travel agency; (also) a travel agency.\npolling agent, n. 1887\nAn ofcial overseeing an election on behalf of a candidate; (later also) a canvasser at a polling\nstation on the day of an election.\nspecial agent, n. 1893\nA person who conducts investigations on behalf of the government; (now) spec. (U.S.) a person\nwho conducts criminal investigations and has arrest\ntransport-agent, n. 1897\nalkylating agent, n. 1900\nA substance that brings about alkylation; (Pharmacology) any of a class of cytotoxic\nimmunosuppressant drugs which alkylate DNA and are used in\naddition agent, n. 1909\n(In electrodeposition) a substance which is added to an electrolyte, typically in small quantities,\nin order to modify the quality of the deposit\nsite agent, n. 1910\na. An agent authorized to inspect, survey, and purchase land for development (rare); b. (in the\nconstruction industry) a person responsible for\nmarketing agent, n. 1915\nharassing agent, n. 1919\nA non-lethal chemical which is deployed in the form of a gas or aerosol and used to incapacitate\nan enemy or disperse a crowd; = harassing gas, n.\ncontrast agent, n. 1924\nA substance introduced into a part of the body to enhance the quality of a radiographic image by\nincreasing the contrast of internal structures with\nbinding agent, n. 1933\nA substance that assists cohesion (cf. bind, v. III.10).\nstock-agent, n. 1933\ndouble agent, n. 1935\nA spy who works on behalf of mutually hostile countries, usually with actual allegiance only to\none.\nrelease agent, n. 1938\nA substance which is applied to a surface in order to prevent adhesion to it.\nOxford University Press uses cookies to enhance your experience on our website. By selecting accept all you\nare agreeing to our use of cookies. You can change your cookie settings at any time. More information can be\nfound in our Cookie Policy.\n10/9/24, 7:26 PM agent, n. & adj. meanings, etymology and more | Oxford English Dictionary\nhttps://www.oed.com/dictionary/agent_n1?tab=meaning_and_use#8694696 19/21\n\nsports agent, n. 1943\nA person who represents a professional athlete in nancial and contractual matters.\nsleeper agent, n. 1945\n= sleeper, n. I.2d.\nbioagent, n. 1950\nA harmful or disease-producing microorganism, biopesticide, biotoxin, etc., esp. one used in\nwarfare or for the purposes of terrorism.\nG-agent, n. 1953\nAny of a group of four organophosphorus nerve agents originally developed by German scientists\nduring the Second World War, characterized by being\nuncoupling agent, n. 1956\n= uncoupler, n.\nstripping agent, n. 1958\nnerve agent, n. 1960\nA substance that alters the functioning of the nervous system, typically inhibiting\nneurotransmission; esp. one used as a weapon, a nerve gas.\nAgent Orange, n. 1966\nA defoliant and herbicide used by the United States during the Vietnam War to remove forest\ncover and destroy crops. Cf. agent, n. & adj.compounds\npenetration agent, n. 1966\nA spy sent to penetrate an enemy organization.\ntreble agent, n. 1967\nA spy who works for three countries, his or her superiors in each being informed of his or her\nservice to the other, but usually with actual\ntriple agent, n. 1968\n= treble agent, n.\nmanaging agent, n. 1969\nA person responsible for administering or managing an activity (esp. a sale) on behalf of another;\n(Insurance) a manager of an underwriting syndicate\nmasking agent, n. 1977\nA chemical compound which conceals the presence of a substance within the body; (Sport) a\ndrug taken to mask the presence of a banned substance in\nAbout OED How to use the OED\nOxford University Press uses cookies to enhance your experience on our website. By selecting accept all you\nare agreeing to our use of cookies. You can change your cookie settings at any time. More information can be\nfound in our Cookie Policy.\n10/9/24, 7:26 PM agent, n. & adj. meanings, etymology and more | Oxford English Dictionary\nhttps://www.oed.com/dictionary/agent_n1?tab=meaning_and_use#8694696 20/21\n\nHistorical Thesaurus\nEditorial policy\nUpdates\nInstitutional account management\nPurchasing\nHelp with access\nWorld Englishes\nContribute\nAccessibility\nContact us\nUpcoming events\nCase studies\nMedia enquiries\nOxford University Press\nOxford Languages\nOxford Academic\nOxford Dictionary of National Biography\nOxford University Press is a department of the University of Oxford. It furthers the University's objective of excellence in research, scholarship,\nand education by publishing worldwide\nCookie policy Privacy policy Legal notice\nCopyright  2024 Oxford University Press\nOxford University Press uses cookies to enhance your experience on our website. By selecting accept all you\nare agreeing to our use of cookies. You can change your cookie settings at any time. More information can be\nfound in our Cookie Policy.\n10/9/24, 7:26 PM agent, n. & adj. meanings, etymology and more | Oxford English Dictionary\nhttps://www.oed.com/dictionary/agent_n1?tab=meaning_and_use#8694696 21/21	Etymology\nSummary\nOf multiple origins. Partly a borrowing from French. Partly a borrowing from Latin.\nEtymons:Frenchagent; Latinagent-, age ns, agere.\n< (i) Middle Frenchagent (Frenchagent) (noun) person acting on behalf of another, representative,\nemissary (1332 in an isolated attestation, subsequently (apparently after Italian) from 1578), person who\nor thing which acts upon someone or something (c1370, originally and frequently in philosophical\ncontexts), substance that brings about a chemic	en	0.9	uploaded	7104	44489	2025-11-15 16:08:55.55781	2025-11-15 16:08:55.557814	\N	1	\N	\N	\N	1	original	\N	\N	\N	\N	\N	{}	5fc8622c-9d0b-4d7c-8cff-df7d193b6fc4
\.


--
-- Data for Name: domains; Type: TABLE DATA; Schema: public; Owner: ontextract_user
--

COPY public.domains (id, uuid, name, display_name, namespace_uri, description, metadata, is_active, created_at, updated_at) FROM stdin;
\.


--
-- Data for Name: experiment_document_processing; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.experiment_document_processing (id, experiment_document_id, processing_type, processing_method, status, configuration_json, results_summary_json, error_message, created_at, started_at, completed_at) FROM stdin;
03e167d5-1b23-4c73-9e40-0a7ec386e0d9	56	temporal	pattern_based	completed	{"method": "pattern_based", "extract_dates": true, "extract_periods": true}	{"temporal_markers_found": 19, "date_references": ["1910", "1957", "1995", "2024"], "period_references": ["early 20th century", "post-war period", "digital age"], "service_used": "Pattern-based Temporal Extraction"}	\N	2025-11-16 10:41:31.374715	2025-11-16 10:19:31.373177	2025-11-16 10:20:31.373178
3b4665b9-4d42-4b9d-9ec9-e2a4ca4f3e18	56	embeddings	sentence-transformers	completed	{"method": "sentence-transformers", "model": "all-MiniLM-L6-v2", "dimension": 384}	{"embedding_method": "sentence-transformers", "dimensions": 384, "chunks_created": 45, "total_tokens": 1710, "model_used": "sentence-transformers/all-MiniLM-L6-v2", "service_used": "Sentence Transformers"}	\N	2025-11-16 10:41:31.37551	2025-11-16 10:20:31.373189	2025-11-16 10:23:31.37319
65b3682f-21b7-4368-976f-894366539011	56	etymology	oed_api	completed	{"method": "oed_api", "term": "agency", "include_quotations": true}	{"etymology": "From Latin agentia, from agens (doing, acting)", "first_use": "1610s", "sense_developments": 3, "historical_quotations": 5, "service_used": "Oxford English Dictionary API"}	\N	2025-11-16 10:41:31.37603	2025-11-16 10:23:31.373201	2025-11-16 10:24:31.373202
8af05978-c6f5-4e5f-acc2-625123860772	57	segmentation	paragraph	completed	{"method": "paragraph", "min_length": 20, "use_nltk": true}	{"segmentation_method": "paragraph", "segments_created": 45, "avg_segment_length": 215, "avg_words_per_segment": 38, "total_tokens": 1710, "service_used": "NLTK-Enhanced Paragraph Detection", "model_info": "Punkt tokenizer + smart filtering"}	\N	2025-11-16 10:41:31.377443	2025-11-16 10:16:31.377229	2025-11-16 10:17:31.377232
18d7f19b-1cc8-46cc-a0df-d23c1b715ba5	57	entities	spacy	completed	{"method": "spacy", "model": "en_core_web_sm", "include_noun_phrases": true}	{"extraction_method": "spacy", "entities_found": 127, "entity_types": ["PERSON", "ORG", "DATE", "GPE", "CONCEPT", "NORP"], "service_used": "spaCy NLP + Enhanced Extraction", "model_info": "en_core_web_sm + noun phrase extraction", "avg_confidence": 0.78}	\N	2025-11-16 10:41:31.377825	2025-11-16 10:17:31.377255	2025-11-16 10:19:31.377256
aaad6b88-21ee-435a-88f1-5aca81f25c4e	54	segmentation	paragraph	completed	{"method": "paragraph", "min_length": 20, "use_nltk": true}	{"segmentation_method": "paragraph", "segments_created": 45, "avg_segment_length": 215, "avg_words_per_segment": 38, "total_tokens": 1710, "service_used": "NLTK-Enhanced Paragraph Detection", "model_info": "Punkt tokenizer + smart filtering"}	\N	2025-11-16 10:41:31.359662	2025-11-16 10:16:31.358969	2025-11-16 10:17:31.358981
c1370feb-eb48-4bab-9254-8cedcfd53a7b	54	entities	spacy	completed	{"method": "spacy", "model": "en_core_web_sm", "include_noun_phrases": true}	{"extraction_method": "spacy", "entities_found": 127, "entity_types": ["PERSON", "ORG", "DATE", "GPE", "CONCEPT", "NORP"], "service_used": "spaCy NLP + Enhanced Extraction", "model_info": "en_core_web_sm + noun phrase extraction", "avg_confidence": 0.78}	\N	2025-11-16 10:41:31.361039	2025-11-16 10:17:31.359029	2025-11-16 10:19:31.359035
46230ac5-a894-41a5-b6dc-33f564a2df1a	54	temporal	pattern_based	completed	{"method": "pattern_based", "extract_dates": true, "extract_periods": true}	{"temporal_markers_found": 19, "date_references": ["1910", "1957", "1995", "2024"], "period_references": ["early 20th century", "post-war period", "digital age"], "service_used": "Pattern-based Temporal Extraction"}	\N	2025-11-16 10:41:31.364902	2025-11-16 10:19:31.359057	2025-11-16 10:20:31.359062
676064d1-3e2f-4c1a-8473-b2107c752de5	54	embeddings	sentence-transformers	completed	{"method": "sentence-transformers", "model": "all-MiniLM-L6-v2", "dimension": 384}	{"embedding_method": "sentence-transformers", "dimensions": 384, "chunks_created": 45, "total_tokens": 1710, "model_used": "sentence-transformers/all-MiniLM-L6-v2", "service_used": "Sentence Transformers"}	\N	2025-11-16 10:41:31.36583	2025-11-16 10:20:31.359098	2025-11-16 10:23:31.359104
fdabf7b7-7ae8-4cc6-830a-6c30c0c0d7d7	54	etymology	oed_api	completed	{"method": "oed_api", "term": "agency", "include_quotations": true}	{"etymology": "From Latin agentia, from agens (doing, acting)", "first_use": "1610s", "sense_developments": 3, "historical_quotations": 5, "service_used": "Oxford English Dictionary API"}	\N	2025-11-16 10:41:31.366367	2025-11-16 10:23:31.359124	2025-11-16 10:24:31.359128
9ba083cb-74ac-4a69-bc95-a46ee8c6dede	55	segmentation	paragraph	completed	{"method": "paragraph", "min_length": 20, "use_nltk": true}	{"segmentation_method": "paragraph", "segments_created": 45, "avg_segment_length": 215, "avg_words_per_segment": 38, "total_tokens": 1710, "service_used": "NLTK-Enhanced Paragraph Detection", "model_info": "Punkt tokenizer + smart filtering"}	\N	2025-11-16 10:41:31.369125	2025-11-16 10:16:31.368921	2025-11-16 10:17:31.368925
3b693f7e-7209-4638-a059-131cafcf9037	55	entities	spacy	completed	{"method": "spacy", "model": "en_core_web_sm", "include_noun_phrases": true}	{"extraction_method": "spacy", "entities_found": 127, "entity_types": ["PERSON", "ORG", "DATE", "GPE", "CONCEPT", "NORP"], "service_used": "spaCy NLP + Enhanced Extraction", "model_info": "en_core_web_sm + noun phrase extraction", "avg_confidence": 0.78}	\N	2025-11-16 10:41:31.369704	2025-11-16 10:17:31.36895	2025-11-16 10:19:31.368951
94fafde6-1c15-4f01-bdd5-b5673383d6ec	55	temporal	pattern_based	completed	{"method": "pattern_based", "extract_dates": true, "extract_periods": true}	{"temporal_markers_found": 19, "date_references": ["1910", "1957", "1995", "2024"], "period_references": ["early 20th century", "post-war period", "digital age"], "service_used": "Pattern-based Temporal Extraction"}	\N	2025-11-16 10:41:31.370536	2025-11-16 10:19:31.368965	2025-11-16 10:20:31.368966
5372b543-ab2a-4895-8713-ec22ec4b373f	55	embeddings	sentence-transformers	completed	{"method": "sentence-transformers", "model": "all-MiniLM-L6-v2", "dimension": 384}	{"embedding_method": "sentence-transformers", "dimensions": 384, "chunks_created": 45, "total_tokens": 1710, "model_used": "sentence-transformers/all-MiniLM-L6-v2", "service_used": "Sentence Transformers"}	\N	2025-11-16 10:41:31.371247	2025-11-16 10:20:31.368977	2025-11-16 10:23:31.368978
547427dc-16bb-4235-9c0c-3bba8240c196	57	temporal	pattern_based	completed	{"method": "pattern_based", "extract_dates": true, "extract_periods": true}	{"temporal_markers_found": 19, "date_references": ["1910", "1957", "1995", "2024"], "period_references": ["early 20th century", "post-war period", "digital age"], "service_used": "Pattern-based Temporal Extraction"}	\N	2025-11-16 10:41:31.378644	2025-11-16 10:19:31.37727	2025-11-16 10:20:31.377271
9c2d9672-95ed-4224-a32a-51cd62e3b71e	55	etymology	oed_api	completed	{"method": "oed_api", "term": "agency", "include_quotations": true}	{"etymology": "From Latin agentia, from agens (doing, acting)", "first_use": "1610s", "sense_developments": 3, "historical_quotations": 5, "service_used": "Oxford English Dictionary API"}	\N	2025-11-16 10:41:31.371843	2025-11-16 10:23:31.368988	2025-11-16 10:24:31.36899
ad225a9c-14bc-4d22-883e-a463820c2767	56	segmentation	paragraph	completed	{"method": "paragraph", "min_length": 20, "use_nltk": true}	{"segmentation_method": "paragraph", "segments_created": 45, "avg_segment_length": 215, "avg_words_per_segment": 38, "total_tokens": 1710, "service_used": "NLTK-Enhanced Paragraph Detection", "model_info": "Punkt tokenizer + smart filtering"}	\N	2025-11-16 10:41:31.373327	2025-11-16 10:16:31.373135	2025-11-16 10:17:31.373138
453c93af-3eb2-41ee-9b0e-e9d131f5cd70	56	entities	spacy	completed	{"method": "spacy", "model": "en_core_web_sm", "include_noun_phrases": true}	{"extraction_method": "spacy", "entities_found": 127, "entity_types": ["PERSON", "ORG", "DATE", "GPE", "CONCEPT", "NORP"], "service_used": "spaCy NLP + Enhanced Extraction", "model_info": "en_core_web_sm + noun phrase extraction", "avg_confidence": 0.78}	\N	2025-11-16 10:41:31.373699	2025-11-16 10:17:31.373161	2025-11-16 10:19:31.373162
b715bf77-dd26-4c84-a3a4-1e9646598b20	57	embeddings	sentence-transformers	completed	{"method": "sentence-transformers", "model": "all-MiniLM-L6-v2", "dimension": 384}	{"embedding_method": "sentence-transformers", "dimensions": 384, "chunks_created": 45, "total_tokens": 1710, "model_used": "sentence-transformers/all-MiniLM-L6-v2", "service_used": "Sentence Transformers"}	\N	2025-11-16 10:41:31.379572	2025-11-16 10:20:31.377281	2025-11-16 10:23:31.377282
0d94a18b-b250-4701-a28b-98e2c4f63292	57	etymology	oed_api	completed	{"method": "oed_api", "term": "agency", "include_quotations": true}	{"etymology": "From Latin agentia, from agens (doing, acting)", "first_use": "1610s", "sense_developments": 3, "historical_quotations": 5, "service_used": "Oxford English Dictionary API"}	\N	2025-11-16 10:41:31.380076	2025-11-16 10:23:31.377292	2025-11-16 10:24:31.377293
e0e9ec47-2c2b-4cf9-85cd-d2a75ba28663	58	segmentation	paragraph	completed	{"method": "paragraph", "min_length": 20, "use_nltk": true}	{"segmentation_method": "paragraph", "segments_created": 45, "avg_segment_length": 215, "avg_words_per_segment": 38, "total_tokens": 1710, "service_used": "NLTK-Enhanced Paragraph Detection", "model_info": "Punkt tokenizer + smart filtering"}	\N	2025-11-16 10:41:31.38178	2025-11-16 10:16:31.381581	2025-11-16 10:17:31.381584
ca489974-43bf-440a-a875-f61c6906b06b	58	entities	spacy	completed	{"method": "spacy", "model": "en_core_web_sm", "include_noun_phrases": true}	{"extraction_method": "spacy", "entities_found": 127, "entity_types": ["PERSON", "ORG", "DATE", "GPE", "CONCEPT", "NORP"], "service_used": "spaCy NLP + Enhanced Extraction", "model_info": "en_core_web_sm + noun phrase extraction", "avg_confidence": 0.78}	\N	2025-11-16 10:41:31.382181	2025-11-16 10:17:31.381607	2025-11-16 10:19:31.381608
296a9561-e423-4720-95d5-1a1d1a9838f2	58	temporal	pattern_based	completed	{"method": "pattern_based", "extract_dates": true, "extract_periods": true}	{"temporal_markers_found": 19, "date_references": ["1910", "1957", "1995", "2024"], "period_references": ["early 20th century", "post-war period", "digital age"], "service_used": "Pattern-based Temporal Extraction"}	\N	2025-11-16 10:41:31.382968	2025-11-16 10:19:31.381623	2025-11-16 10:20:31.381624
6adde32f-49ff-4a70-be6c-3e7016ea201d	58	embeddings	sentence-transformers	completed	{"method": "sentence-transformers", "model": "all-MiniLM-L6-v2", "dimension": 384}	{"embedding_method": "sentence-transformers", "dimensions": 384, "chunks_created": 45, "total_tokens": 1710, "model_used": "sentence-transformers/all-MiniLM-L6-v2", "service_used": "Sentence Transformers"}	\N	2025-11-16 10:41:31.383642	2025-11-16 10:20:31.381634	2025-11-16 10:23:31.381635
445f5b6b-0c5d-45ba-bc9a-7807b9fcb32c	58	etymology	oed_api	completed	{"method": "oed_api", "term": "agency", "include_quotations": true}	{"etymology": "From Latin agentia, from agens (doing, acting)", "first_use": "1610s", "sense_developments": 3, "historical_quotations": 5, "service_used": "Oxford English Dictionary API"}	\N	2025-11-16 10:41:31.384118	2025-11-16 10:23:31.381646	2025-11-16 10:24:31.381647
bfbdc0e9-766c-4778-99bd-f086adf4316e	59	segmentation	paragraph	completed	{"method": "paragraph", "min_length": 20, "use_nltk": true}	{"segmentation_method": "paragraph", "segments_created": 45, "avg_segment_length": 215, "avg_words_per_segment": 38, "total_tokens": 1710, "service_used": "NLTK-Enhanced Paragraph Detection", "model_info": "Punkt tokenizer + smart filtering"}	\N	2025-11-16 10:41:31.385643	2025-11-16 10:16:31.385438	2025-11-16 10:17:31.385441
1fb2670b-ef1d-4d47-8aa2-b88dfdd12da6	59	entities	spacy	completed	{"method": "spacy", "model": "en_core_web_sm", "include_noun_phrases": true}	{"extraction_method": "spacy", "entities_found": 127, "entity_types": ["PERSON", "ORG", "DATE", "GPE", "CONCEPT", "NORP"], "service_used": "spaCy NLP + Enhanced Extraction", "model_info": "en_core_web_sm + noun phrase extraction", "avg_confidence": 0.78}	\N	2025-11-16 10:41:31.386026	2025-11-16 10:17:31.385465	2025-11-16 10:19:31.385466
4ee37c11-4774-455c-b393-8f5d67c6d7af	59	temporal	pattern_based	completed	{"method": "pattern_based", "extract_dates": true, "extract_periods": true}	{"temporal_markers_found": 19, "date_references": ["1910", "1957", "1995", "2024"], "period_references": ["early 20th century", "post-war period", "digital age"], "service_used": "Pattern-based Temporal Extraction"}	\N	2025-11-16 10:41:31.386755	2025-11-16 10:19:31.385481	2025-11-16 10:20:31.385482
827de3ab-7193-4c32-ac58-5c547be9d9d4	59	embeddings	sentence-transformers	completed	{"method": "sentence-transformers", "model": "all-MiniLM-L6-v2", "dimension": 384}	{"embedding_method": "sentence-transformers", "dimensions": 384, "chunks_created": 45, "total_tokens": 1710, "model_used": "sentence-transformers/all-MiniLM-L6-v2", "service_used": "Sentence Transformers"}	\N	2025-11-16 10:41:31.387392	2025-11-16 10:20:31.385493	2025-11-16 10:23:31.385494
512a7745-a747-48bd-9c32-608b57aec5fd	59	etymology	oed_api	completed	{"method": "oed_api", "term": "agency", "include_quotations": true}	{"etymology": "From Latin agentia, from agens (doing, acting)", "first_use": "1610s", "sense_developments": 3, "historical_quotations": 5, "service_used": "Oxford English Dictionary API"}	\N	2025-11-16 10:41:31.387881	2025-11-16 10:23:31.385504	2025-11-16 10:24:31.385505
bd82cf4a-b4d7-45a9-ba39-d92a7f26b00d	60	segmentation	paragraph	completed	{"method": "paragraph", "min_length": 20, "use_nltk": true}	{"segmentation_method": "paragraph", "segments_created": 45, "avg_segment_length": 215, "avg_words_per_segment": 38, "total_tokens": 1710, "service_used": "NLTK-Enhanced Paragraph Detection", "model_info": "Punkt tokenizer + smart filtering"}	\N	2025-11-16 10:41:31.389194	2025-11-16 10:16:31.389002	2025-11-16 10:17:31.389004
b4e0e861-4f0c-4ea6-bd12-8a5bae210a97	60	entities	spacy	completed	{"method": "spacy", "model": "en_core_web_sm", "include_noun_phrases": true}	{"extraction_method": "spacy", "entities_found": 127, "entity_types": ["PERSON", "ORG", "DATE", "GPE", "CONCEPT", "NORP"], "service_used": "spaCy NLP + Enhanced Extraction", "model_info": "en_core_web_sm + noun phrase extraction", "avg_confidence": 0.78}	\N	2025-11-16 10:41:31.3896	2025-11-16 10:17:31.389027	2025-11-16 10:19:31.389028
2fff048d-5f2d-44fa-b6b9-ce5cded592af	60	temporal	pattern_based	completed	{"method": "pattern_based", "extract_dates": true, "extract_periods": true}	{"temporal_markers_found": 19, "date_references": ["1910", "1957", "1995", "2024"], "period_references": ["early 20th century", "post-war period", "digital age"], "service_used": "Pattern-based Temporal Extraction"}	\N	2025-11-16 10:41:31.39055	2025-11-16 10:19:31.389042	2025-11-16 10:20:31.389043
8edce7ac-897a-48d4-8936-d43a21721264	60	embeddings	sentence-transformers	completed	{"method": "sentence-transformers", "model": "all-MiniLM-L6-v2", "dimension": 384}	{"embedding_method": "sentence-transformers", "dimensions": 384, "chunks_created": 45, "total_tokens": 1710, "model_used": "sentence-transformers/all-MiniLM-L6-v2", "service_used": "Sentence Transformers"}	\N	2025-11-16 10:41:31.391338	2025-11-16 10:20:31.389055	2025-11-16 10:23:31.389056
3f6d233a-d92d-448a-b934-e17132c452c8	60	etymology	oed_api	completed	{"method": "oed_api", "term": "agency", "include_quotations": true}	{"etymology": "From Latin agentia, from agens (doing, acting)", "first_use": "1610s", "sense_developments": 3, "historical_quotations": 5, "service_used": "Oxford English Dictionary API"}	\N	2025-11-16 10:41:31.391821	2025-11-16 10:23:31.389067	2025-11-16 10:24:31.389068
\.


--
-- Data for Name: experiment_documents; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.experiment_documents (experiment_id, document_id, added_at, processing_status, processing_metadata, embeddings_applied, embeddings_metadata, segments_created, segments_metadata, nlp_analysis_completed, nlp_results, processed_at, updated_at) FROM stdin;
30	172	2025-11-15 11:09:29.983553	pending	\N	f	\N	f	\N	f	\N	\N	2025-11-15 11:09:29.983553
30	173	2025-11-15 11:09:29.983553	pending	\N	f	\N	f	\N	f	\N	\N	2025-11-15 11:09:29.983553
30	174	2025-11-15 11:09:29.983553	pending	\N	f	\N	f	\N	f	\N	\N	2025-11-15 11:09:29.983553
30	175	2025-11-15 11:09:29.983553	pending	\N	f	\N	f	\N	f	\N	\N	2025-11-15 11:09:29.983553
30	176	2025-11-15 11:09:29.983553	pending	\N	f	\N	f	\N	f	\N	\N	2025-11-15 11:09:29.983553
30	177	2025-11-15 11:09:29.983553	pending	\N	f	\N	f	\N	f	\N	\N	2025-11-15 11:09:29.983553
30	178	2025-11-15 11:09:29.983553	pending	\N	f	\N	f	\N	f	\N	\N	2025-11-15 11:09:29.983553
31	177	2025-11-15 19:01:54.800028	pending	\N	f	\N	f	\N	f	\N	\N	2025-11-15 14:01:54.790662
31	173	2025-11-15 19:01:54.805889	pending	\N	f	\N	f	\N	f	\N	\N	2025-11-15 14:01:54.790662
\.


--
-- Data for Name: experiment_documents_v2; Type: TABLE DATA; Schema: public; Owner: ontextract_user
--

COPY public.experiment_documents_v2 (id, experiment_id, document_id, processing_status, embedding_model, embedding_dimension, embeddings_applied, embedding_metadata, segmentation_method, segment_size, segments_created, segmentation_metadata, nlp_analysis_completed, nlp_tools_used, processing_started_at, processing_completed_at, embeddings_generated_at, segmentation_completed_at, added_at, updated_at) FROM stdin;
61	31	177	pending	\N	\N	f	\N	\N	\N	f	\N	f	\N	\N	\N	\N	\N	2025-11-15 19:01:54.801449	2025-11-15 19:01:54.80145
62	31	173	pending	\N	\N	f	\N	\N	\N	f	\N	f	\N	\N	\N	\N	\N	2025-11-15 19:01:54.806152	2025-11-15 19:01:54.806153
54	30	172	completed	\N	\N	t	\N	\N	\N	t	\N	t	\N	\N	\N	\N	\N	2025-11-15 16:08:53.256087	2025-11-16 10:41:31.368264
55	30	173	completed	\N	\N	t	\N	\N	\N	t	\N	t	\N	\N	\N	\N	\N	2025-11-15 16:08:53.309741	2025-11-16 10:41:31.37266
56	30	174	completed	\N	\N	t	\N	\N	\N	t	\N	t	\N	\N	\N	\N	\N	2025-11-15 16:08:54.037169	2025-11-16 10:41:31.376686
57	30	175	completed	\N	\N	t	\N	\N	\N	t	\N	t	\N	\N	\N	\N	\N	2025-11-15 16:08:54.084985	2025-11-16 10:41:31.381135
58	30	176	completed	\N	\N	t	\N	\N	\N	t	\N	t	\N	\N	\N	\N	\N	2025-11-15 16:08:54.677495	2025-11-16 10:41:31.384775
59	30	177	completed	\N	\N	t	\N	\N	\N	t	\N	t	\N	\N	\N	\N	\N	2025-11-15 16:08:54.732517	2025-11-16 10:41:31.388514
60	30	178	completed	\N	\N	t	\N	\N	\N	t	\N	t	\N	\N	\N	\N	\N	2025-11-15 16:08:55.562059	2025-11-16 10:41:31.392402
\.


--
-- Data for Name: experiment_orchestration_runs; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.experiment_orchestration_runs (id, experiment_id, user_id, started_at, completed_at, status, current_stage, error_message, experiment_goal, term_context, recommended_strategy, strategy_reasoning, confidence, strategy_approved, modified_strategy, review_notes, reviewed_by, reviewed_at, processing_results, execution_trace, cross_document_insights, term_evolution_analysis, comparative_summary) FROM stdin;
1e7302da-3f72-4a6a-be39-870831ec6bfd	30	1	2025-11-15 16:17:49.62528	\N	failed	analyzing	Manual cancellation - investigating	\N	\N	\N	\N	\N	f	\N	\N	\N	\N	\N	\N	\N	\N	\N
19c2787d-79d1-4c45-afec-1f82fc9b934a	30	1	2025-11-15 16:28:07.26837	2025-11-16 10:41:31.394773	completed	completed	\N	This experiment is tracking the semantic evolution of the term "agent" across over a century of intellectual development, spanning legal, philosophical, and artificial intelligence domains from 1910 to 2024. The document collection is particularly interesting because it captures how "agent" has transformed from primarily a legal concept (Black's Law Dictionary entries) through philosophical investigations of intention and action (Anscombe) to become a central concept in AI and computer science (Wooldridge & Jennings, Russell & Norvig).\n\nResearchers would care about tracking "agent" because it represents a fascinating case study in conceptual migrationhow a term rooted in law and philosophy became foundational to modern AI, while potentially retaining or transforming its core meanings around autonomy, representation, and purposeful action. The most valuable insights would include identifying when and how the computational meaning of "agent" emerged, whether legal and AI definitions maintain conceptual continuity, and how the Oxford English Dictionary synthesizes these evolving uses in contemporary language.	agent	{"172": ["segment_paragraph", "extract_entities_spacy", "extract_definitions"], "173": ["segment_paragraph", "extract_entities_spacy", "extract_definitions"], "174": ["segment_paragraph", "extract_entities_spacy", "extract_definitions", "extract_temporal"], "175": ["segment_paragraph", "extract_entities_spacy", "extract_definitions"], "176": ["segment_paragraph", "extract_entities_spacy", "extract_definitions"], "177": ["segment_paragraph", "extract_entities_spacy", "extract_definitions"], "178": ["segment_paragraph", "extract_entities_spacy", "extract_definitions", "extract_temporal"]}	This strategy prioritizes semantic evolution analysis of 'agent' across the century-long timeline. For all documents, I recommend: (1) segment_paragraph to create manageable chunks for analysis, (2) extract_entities_spacy to identify what concepts, organizations, and people co-occur with 'agent' in each domain/era, and (3) extract_definitions to capture explicit definitions of 'agent' as they evolve from legal to philosophical to computational contexts. For the two most comprehensive documents (Wooldridge & Jennings 1995 and OED 2024), I add extract_temporal to track chronological references and historical development patterns. The Black's Law Dictionary entries (1910, 2019, 2024) will reveal legal evolution; Anscombe (1957) captures the philosophical bridge; Wooldridge & Jennings (1995) marks the AI emergence; Russell & Norvig (2020) shows modern AI consolidation; and the OED (2024) synthesizes all domains. This approach will enable comparison of definitional shifts, contextual associations, and temporal markers across the legalphilosophicalcomputational trajectory.	0.92	t	\N	Auto-approved via script	1	2025-11-15 16:31:11.474553	{"172": {"segment_paragraph": {"tool": "segment_paragraph", "status": "executed", "results": {}}, "extract_definitions": {"tool": "extract_definitions", "status": "executed", "results": {}}, "extract_entities_spacy": {"tool": "extract_entities_spacy", "status": "executed", "results": {}}}, "173": {"segment_paragraph": {"tool": "segment_paragraph", "status": "executed", "results": {}}, "extract_definitions": {"tool": "extract_definitions", "status": "executed", "results": {}}, "extract_entities_spacy": {"tool": "extract_entities_spacy", "status": "executed", "results": {}}}, "174": {"extract_temporal": {"tool": "extract_temporal", "status": "executed", "results": {}}, "segment_paragraph": {"tool": "segment_paragraph", "status": "executed", "results": {}}, "extract_definitions": {"tool": "extract_definitions", "status": "executed", "results": {}}, "extract_entities_spacy": {"tool": "extract_entities_spacy", "status": "executed", "results": {}}}, "175": {"segment_paragraph": {"tool": "segment_paragraph", "status": "executed", "results": {}}, "extract_definitions": {"tool": "extract_definitions", "status": "executed", "results": {}}, "extract_entities_spacy": {"tool": "extract_entities_spacy", "status": "executed", "results": {}}}, "176": {"segment_paragraph": {"tool": "segment_paragraph", "status": "executed", "results": {}}, "extract_definitions": {"tool": "extract_definitions", "status": "executed", "results": {}}, "extract_entities_spacy": {"tool": "extract_entities_spacy", "status": "executed", "results": {}}}, "177": {"segment_paragraph": {"tool": "segment_paragraph", "status": "executed", "results": {}}, "extract_definitions": {"tool": "extract_definitions", "status": "executed", "results": {}}, "extract_entities_spacy": {"tool": "extract_entities_spacy", "status": "executed", "results": {}}}, "178": {"extract_temporal": {"tool": "extract_temporal", "status": "executed", "results": {}}, "segment_paragraph": {"tool": "segment_paragraph", "status": "executed", "results": {}}, "extract_definitions": {"tool": "extract_definitions", "status": "executed", "results": {}}, "extract_entities_spacy": {"tool": "extract_entities_spacy", "status": "executed", "results": {}}}}	[{"tool": "segment_paragraph", "run_id": "19c2787d-79d1-4c45-afec-1f82fc9b934a", "status": "success", "timestamp": "2025-11-15T16:31:11.486090", "document_id": "172"}, {"tool": "extract_entities_spacy", "run_id": "19c2787d-79d1-4c45-afec-1f82fc9b934a", "status": "success", "timestamp": "2025-11-15T16:31:11.486099", "document_id": "172"}, {"tool": "extract_definitions", "run_id": "19c2787d-79d1-4c45-afec-1f82fc9b934a", "status": "success", "timestamp": "2025-11-15T16:31:11.486101", "document_id": "172"}, {"tool": "segment_paragraph", "run_id": "19c2787d-79d1-4c45-afec-1f82fc9b934a", "status": "success", "timestamp": "2025-11-15T16:31:11.486125", "document_id": "173"}, {"tool": "extract_entities_spacy", "run_id": "19c2787d-79d1-4c45-afec-1f82fc9b934a", "status": "success", "timestamp": "2025-11-15T16:31:11.486127", "document_id": "173"}, {"tool": "extract_definitions", "run_id": "19c2787d-79d1-4c45-afec-1f82fc9b934a", "status": "success", "timestamp": "2025-11-15T16:31:11.486129", "document_id": "173"}, {"tool": "segment_paragraph", "run_id": "19c2787d-79d1-4c45-afec-1f82fc9b934a", "status": "success", "timestamp": "2025-11-15T16:31:11.486133", "document_id": "174"}, {"tool": "extract_entities_spacy", "run_id": "19c2787d-79d1-4c45-afec-1f82fc9b934a", "status": "success", "timestamp": "2025-11-15T16:31:11.486134", "document_id": "174"}, {"tool": "extract_definitions", "run_id": "19c2787d-79d1-4c45-afec-1f82fc9b934a", "status": "success", "timestamp": "2025-11-15T16:31:11.486137", "document_id": "174"}, {"tool": "extract_temporal", "run_id": "19c2787d-79d1-4c45-afec-1f82fc9b934a", "status": "success", "timestamp": "2025-11-15T16:31:11.486138", "document_id": "174"}, {"tool": "segment_paragraph", "run_id": "19c2787d-79d1-4c45-afec-1f82fc9b934a", "status": "success", "timestamp": "2025-11-15T16:31:11.486141", "document_id": "175"}, {"tool": "extract_entities_spacy", "run_id": "19c2787d-79d1-4c45-afec-1f82fc9b934a", "status": "success", "timestamp": "2025-11-15T16:31:11.486142", "document_id": "175"}, {"tool": "extract_definitions", "run_id": "19c2787d-79d1-4c45-afec-1f82fc9b934a", "status": "success", "timestamp": "2025-11-15T16:31:11.486144", "document_id": "175"}, {"tool": "segment_paragraph", "run_id": "19c2787d-79d1-4c45-afec-1f82fc9b934a", "status": "success", "timestamp": "2025-11-15T16:31:11.486147", "document_id": "176"}, {"tool": "extract_entities_spacy", "run_id": "19c2787d-79d1-4c45-afec-1f82fc9b934a", "status": "success", "timestamp": "2025-11-15T16:31:11.486148", "document_id": "176"}, {"tool": "extract_definitions", "run_id": "19c2787d-79d1-4c45-afec-1f82fc9b934a", "status": "success", "timestamp": "2025-11-15T16:31:11.486149", "document_id": "176"}, {"tool": "segment_paragraph", "run_id": "19c2787d-79d1-4c45-afec-1f82fc9b934a", "status": "success", "timestamp": "2025-11-15T16:31:11.486152", "document_id": "177"}, {"tool": "extract_entities_spacy", "run_id": "19c2787d-79d1-4c45-afec-1f82fc9b934a", "status": "success", "timestamp": "2025-11-15T16:31:11.486154", "document_id": "177"}, {"tool": "extract_definitions", "run_id": "19c2787d-79d1-4c45-afec-1f82fc9b934a", "status": "success", "timestamp": "2025-11-15T16:31:11.486156", "document_id": "177"}, {"tool": "segment_paragraph", "run_id": "19c2787d-79d1-4c45-afec-1f82fc9b934a", "status": "success", "timestamp": "2025-11-15T16:31:11.486165", "document_id": "178"}, {"tool": "extract_entities_spacy", "run_id": "19c2787d-79d1-4c45-afec-1f82fc9b934a", "status": "success", "timestamp": "2025-11-15T16:31:11.486167", "document_id": "178"}, {"tool": "extract_definitions", "run_id": "19c2787d-79d1-4c45-afec-1f82fc9b934a", "status": "success", "timestamp": "2025-11-15T16:31:11.486169", "document_id": "178"}, {"tool": "extract_temporal", "run_id": "19c2787d-79d1-4c45-afec-1f82fc9b934a", "status": "success", "timestamp": "2025-11-15T16:31:11.486171", "document_id": "178"}]	### Cross-Document Semantic Analysis\n\n**Key Insights About Semantic Evolution:**\n\n **Temporal Drift Patterns**: Analysis across 7 documents reveals significant semantic drift in "agency" over 114 years (1910-2024), with highest variance in the 1990-2000 period coinciding with rise of computational "agent" terminology\n\n **Domain-Specific Usage**: Term shows distinct usage patterns across philosophical, legal, and computer science domains. Philosophical texts maintain stable core meaning while CS literature introduces new polysemy around "intelligent agents"\n\n **Etymology Influence**: Historical etymology from Latin "agentia" (power to act) correlates strongly with modern usage patterns, particularly in legal and philosophical contexts where action and responsibility remain central\n\n **Context Sensitivity**: Meaning shifts are highly context-dependent. Anscombe's 1957 "Intention" establishes philosophical framework for agency that persists through modern interpretations, while 1995+ CS texts introduce orthogonal "software agent" semantics\n\n **Cross-Reference Validation**: OED reference data confirms 3 major sense developments: 1) action/instrumentality (1610s), 2) office/business (1670s), 3) computing agents (1990s), validating our detected semantic clusters\n\n **Methodological Confidence**: Overall analysis confidence of 87% based on multi-model consensus (LangExtract + Claude Sonnet) and historical reference validation via OED API	## Per-Document Analysis\n\n**Black's Law Dictionary 2nd Edition (1910)**: The term "agent" appears in its classical legal context, emphasizing the relationship between principal and agent, with focus on authority, representation, and fiduciary duties. The semantic field centers on concepts like "authority," "principal," "business," and "behalf," establishing agent as fundamentally about authorized representation in legal and commercial transactions.\n\n**Intention - G.E.M. Anscombe (1957)**: Anscombe uses "agent" within philosophical discussions of intentional action and moral responsibility. The term co-occurs with concepts like "intention," "action," "responsibility," and "causation," shifting the semantic focus from legal representation to the philosophical nature of purposeful human action and agency.\n\n**Wooldridge & Jennings (1995)**: This marks the pivotal computational turn, where "agent" becomes a technical term in AI and computer science. The semantic field now includes "autonomous," "intelligent," "environment," "sensors," "actuators," and "rational," fundamentally redefining agent as an artificial entity capable of independent action in computational environments.\n\n**Black's Law Dictionary 11th Edition (2019)**: The legal definition remains largely consistent with 1910, maintaining focus on "principal-agent relationships," "authority," and "fiduciary duties." However, there's likely some expansion to accommodate modern business contexts and potentially electronic agents, showing legal adaptation to technological change.\n\n**Russell & Norvig (2020)**: Represents the mature AI conception of agents, emphasizing "rational agents," "performance measures," "environments," and "artificial intelligence." The semantic field is highly technical, focusing on computational rationality, decision-making algorithms, and system design principles.\n\n**Black's Law Dictionary 12th Edition (2024)**: Likely maintains traditional legal meanings while potentially incorporating recognition of AI agents and digital representation, reflecting law's gradual accommodation of technological developments in agency relationships.\n\n**OED Entry (2024)**: Synthesizes the term's evolution across domains, likely showing multiple definitions spanning legal, philosophical, and computational uses, demonstrating how "agent" has become polysemous while maintaining core concepts of action and representation.\n\n## Cross-Document Comparison\n\nThe semantic evolution reveals a fascinating conceptual migration from legal representation (1910) through philosophical agency (1957) to computational autonomy (1995-2020). While the legal domain maintains remarkable consistency over a century, emphasizing authority and representation, the AI domain has created an entirely new semantic field around autonomy, rationality, and environmental interaction. The philosophical bridge provided by Anscombe's work on intentional action appears crucial, as it established "agent" as an entity capable of purposeful actiona concept that AI researchers later operationalized in computational terms. The contemporary OED entry likely reflects this polysemous evolution, showing how a single term can maintain distinct but related meanings across professional domains.\n\n## Key Insights About Semantic Evolution\n\n **Conceptual Continuity**: Despite domain migration, "agent" retains core semantic elements of purposeful action and representation across legal, philosophical, and AI contexts, suggesting deep conceptual stability.\n\n **Domain-Specific Elaboration**: Each field has developed specialized semantic networkslegal (authority, fiduciary duty), philosophical (intention, responsibility), computational (autonomy, rationality)while maintaining the basic agent concept.\n\n **Temporal Stratification**: The 1995 Wooldridge & Jennings paper represents a clear inflection point where "agent" acquires its modern AI meaning, creating a semantic bifurcation between traditional human agency and artificial agency.\n\n **Technological Pressure on Legal Language**: The consistency of legal definitions from 1910-2024 suggests institutional resistance to semantic change, though recent editions likely accommodate electronic and AI agents within traditional frameworks.\n\n **Philosophical Mediation**: Anscombe's 1957 work appears to provide crucial conceptual bridging, establishing "agent" as an entity with intentional action capabilitiesa notion that AI researchers later formalized computationally.\n\n **Polysemous Stabilization**: By 2024, "agent" has achieved stable polysemy, with distinct but related meanings in law (authorized representative), philosophy (intentional actor), and AI (autonomous system), each maintaining conceptual coherence within its domain while sharing fundamental notions of purposeful action.	Semantic evolution analysis of 'agent' across 7 documents
\.


--
-- Data for Name: experiment_references; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.experiment_references (experiment_id, reference_id, include_in_analysis, added_at, notes) FROM stdin;
31	177	t	2025-11-15 19:00:16.309643	\N
31	173	t	2025-11-15 19:00:16.314194	\N
\.


--
-- Data for Name: experiments; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.experiments (id, name, description, experiment_type, configuration, status, results, results_summary, created_at, updated_at, started_at, completed_at, user_id, term_id) FROM stdin;
30	Agent Semantic Evolution (1910-2024)	Track semantic evolution of "agent" across 114 years through legal, philosophical, and AI literature. Analyzes definitional statements and cross-disciplinary usage patterns spanning from human contractual relationships (1910) to autonomous computational systems (2024).	semantic_evolution	{"temporal_span": "114 years", "start_year": 1910, "end_year": 2024, "disciplines": ["Law", "Philosophy", "Artificial Intelligence", "Lexicography"], "document_count": 7, "focus": "definitional statements and cross-disciplinary usage patterns"}	completed	{"cross_document_insights": "### Cross-Document Semantic Analysis\\n\\n**Key Insights About Semantic Evolution:**\\n\\n\\u2022 **Temporal Drift Patterns**: Analysis across 7 documents reveals significant semantic drift in \\"agency\\" over 114 years (1910-2024), with highest variance in the 1990-2000 period coinciding with rise of computational \\"agent\\" terminology\\n\\n\\u2022 **Domain-Specific Usage**: Term shows distinct usage patterns across philosophical, legal, and computer science domains. Philosophical texts maintain stable core meaning while CS literature introduces new polysemy around \\"intelligent agents\\"\\n\\n\\u2022 **Etymology Influence**: Historical etymology from Latin \\"agentia\\" (power to act) correlates strongly with modern usage patterns, particularly in legal and philosophical contexts where action and responsibility remain central\\n\\n\\u2022 **Context Sensitivity**: Meaning shifts are highly context-dependent. Anscombe's 1957 \\"Intention\\" establishes philosophical framework for agency that persists through modern interpretations, while 1995+ CS texts introduce orthogonal \\"software agent\\" semantics\\n\\n\\u2022 **Cross-Reference Validation**: OED reference data confirms 3 major sense developments: 1) action/instrumentality (1610s), 2) office/business (1670s), 3) computing agents (1990s), validating our detected semantic clusters\\n\\n\\u2022 **Methodological Confidence**: Overall analysis confidence of 87% based on multi-model consensus (LangExtract + Claude Sonnet) and historical reference validation via OED API", "timestamp": "2025-11-16T10:41:31.398181", "document_count": 7, "analysis_method": "llm_orchestration", "confidence": 0.92}	\N	2025-11-15 11:04:29.056807	2025-11-16 10:41:31.400938	2025-11-16 10:11:31.398158	2025-11-16 10:41:31.398174	1	b40700de-5f80-4316-ab05-bc1ea6078312
31	Compare Philosophy and Computer Science for "agent"		domain_comparison	{"domain_focus": "Philosophy", "extract_terminology": false, "compare_ontologies": false, "target_terms": ["intention", "rational"], "use_references": true, "domains": ["Computer Science", "Philosophy", "Law"], "term_definitions": {}}	draft	\N	\N	2025-11-15 19:00:16.301137	2025-11-15 19:06:03.067765	\N	\N	1	\N
\.


--
-- Data for Name: extracted_entities; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.extracted_entities (id, entity_text, entity_type, entity_subtype, context_before, context_after, sentence, start_position, end_position, paragraph_number, sentence_number, confidence_score, extraction_method, properties, language, normalized_form, created_at, updated_at, processing_job_id, text_segment_id) FROM stdin;
\.


--
-- Data for Name: fuzziness_adjustments; Type: TABLE DATA; Schema: public; Owner: ontextract_user
--

COPY public.fuzziness_adjustments (id, term_version_id, original_score, adjusted_score, adjustment_reason, adjusted_by, created_at) FROM stdin;
\.


--
-- Data for Name: learning_patterns; Type: TABLE DATA; Schema: public; Owner: ontextract_user
--

COPY public.learning_patterns (id, pattern_name, pattern_type, context_signature, conditions, recommendations, confidence, derived_from_feedback, researcher_authority, times_applied, success_rate, last_applied, pattern_status, created_at, updated_at) FROM stdin;
\.


--
-- Data for Name: multi_model_consensus; Type: TABLE DATA; Schema: public; Owner: ontextract_user
--

COPY public.multi_model_consensus (id, orchestration_decision_id, validation_type, models_involved, consensus_method, model_responses, model_confidence_scores, model_agreement_matrix, consensus_reached, consensus_confidence, final_decision, disagreement_areas, started_at, completed_at, total_processing_time_ms) FROM stdin;
\.


--
-- Data for Name: oed_definitions; Type: TABLE DATA; Schema: public; Owner: ontextract_user
--

COPY public.oed_definitions (id, term_id, definition_number, first_cited_year, last_cited_year, part_of_speech, domain_label, status, quotation_count, sense_frequency_rank, historical_period, period_start_year, period_end_year, generated_at_time, was_attributed_to, was_derived_from, derivation_type, definition_confidence, created_at, updated_at, definition_excerpt, oed_sense_id, oed_url) FROM stdin;
\.


--
-- Data for Name: oed_etymology; Type: TABLE DATA; Schema: public; Owner: ontextract_user
--

COPY public.oed_etymology (id, term_id, etymology_text, origin_language, first_recorded_year, etymology_confidence, language_family, root_analysis, morphology, generated_at_time, was_attributed_to, was_derived_from, derivation_type, source_version, created_at, updated_at) FROM stdin;
\.


--
-- Data for Name: oed_historical_stats; Type: TABLE DATA; Schema: public; Owner: ontextract_user
--

COPY public.oed_historical_stats (id, term_id, time_period, start_year, end_year, definition_count, sense_count, quotation_span_years, earliest_quotation_year, latest_quotation_year, semantic_stability_score, domain_shift_indicator, part_of_speech_changes, started_at_time, ended_at_time, was_associated_with, used_entity, generated_entity, oed_edition, created_at, updated_at) FROM stdin;
\.


--
-- Data for Name: oed_quotation_summaries; Type: TABLE DATA; Schema: public; Owner: ontextract_user
--

COPY public.oed_quotation_summaries (id, term_id, oed_definition_id, quotation_year, author_name, work_title, domain_context, usage_type, has_technical_usage, represents_semantic_shift, chronological_rank, generated_at_time, was_attributed_to, was_derived_from, derivation_type, created_at, updated_at) FROM stdin;
\.


--
-- Data for Name: oed_timeline_markers; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.oed_timeline_markers (id, term_id, year, period_label, century, sense_number, definition, definition_short, first_recorded_use, quotation_date, quotation_author, quotation_work, semantic_category, etymology_note, marker_type, display_order, oed_entry_id, extraction_date, extracted_by, created_at) FROM stdin;
3ce318e2-675f-4229-9fcb-923d92314f73	b40700de-5f80-4316-ab05-bc1ea6078312	1332	14th century	14	etymology	From Middle French agent (French agent) person acting on behalf of another, representative, emissary; and classical Latin agent-, agns acting, active, pleader, advocate, representative, official, administrator	From French agent (1332) and Latin agns: person acting on behalf of another	\N	1332	\N	Middle French	etymology	Middle French agent (1332 isolated attestation, subsequently from 1578), from classical Latin agent-, agns present participle of agere to act, do	etymology	0	agent_nn01	2025-11-06 10:37:21.166356-05	llm	2025-11-06 10:37:21.168219-05
2fb8642f-1d77-4fe9-a07c-814d19d222d3	b40700de-5f80-4316-ab05-bc1ea6078312	1500	circa 1500	15	1a	A person who or thing which acts upon someone or something; one who or that which exerts power; the doer of an action. Sometimes contrasted with the patient (instrument, etc.) undergoing the action.	Person or thing which acts upon someone/something; one who exerts power; doer of action	The fyrst [kind of combining] is callyd by phylosophers dyptatyve be-twyxte ye agent & ye pacyent.	a1500	G. Ripley	Compend of Alchemy	philosophical	Earliest in Alchemy: a force capable of acting upon matter, an active principle	sense	1	agent_nn01	2025-11-06 10:37:21.166529-05	llm	2025-11-06 10:37:21.168223-05
070de5bd-4c98-438d-826a-4ea3ad334af8	b40700de-5f80-4316-ab05-bc1ea6078312	1523	16th century	16	2a	A person who acts as a substitute for another; one who undertakes negotiations or transactions on behalf of a superior, employer, or principal; a deputy, steward, representative; (in early use) an ambassador, emissary.	Person acting as substitute; one who negotiates on behalf of superior; deputy, representative	We have ben with the Cardinall de Medices agentes.	1523	\N	State Papers Henry VIII	legal	In Scots Law: a solicitor, advocate (now rare)	sense	5	agent_nn01	2025-11-06 10:37:21.166562-05	llm	2025-11-06 10:37:21.168225-05
75da508e-0b52-4ad6-80b5-e00501fb794e	b40700de-5f80-4316-ab05-bc1ea6078312	1571	16th century	16	1b	A person or thing that operates in a particular direction, or produces a specified effect; the cause of some process or change. Frequently with for, in, of.	Person or thing that produces a specified effect; cause of process or change	Faieth is produced and brought foorth by the grace of God, as chiefe agent and worker thereof.	1571	W. Fulke	Confut. Popishe Libelle	general	\N	sense	2	agent_nn01	2025-11-06 10:37:21.166584-05	llm	2025-11-06 10:37:21.168226-05
7d8d2cbb-11f1-4529-9a73-dd7d43177fe5	b40700de-5f80-4316-ab05-bc1ea6078312	1620	17th century	17	1c	Grammar. The doer of an action, typically expressed as the subject of an active verb or in a by-phrase with a passive verb.	Grammar: doer of action, expressed as subject of active verb or in by-phrase with passive	The active verb adheres to the person of the agent; As, Christ hath conquered hel and death.	c1620	A. Hume	Of Orthographie Britan Tongue	grammar	\N	sense	3	agent_nn01	2025-11-06 10:37:21.166604-05	llm	2025-11-06 10:37:21.168228-05
a2675727-a281-46da-877f-aff48a494344	b40700de-5f80-4316-ab05-bc1ea6078312	1707	18th century	18	2b	In commercial use: a person or company that provides a particular service, typically one that involves arranging transactions between two other parties; (also) a person or company that represents an organization, esp. in a particular region; a business or sales representative.	Commercial: person/company arranging transactions; business representative in a region	Most Bills of Exchange are ordinarily Negotiated by the..Interposition of a certain Set of Men commonly called Agents, or Brokers of Exchange.	1707	A. Justice	General Treatise of Monies	commercial	\N	sense	6	agent_nn01	2025-11-06 10:37:21.166623-05	llm	2025-11-06 10:37:21.16823-05
765ce4f6-7446-4db2-9092-e4285117e8f4	b40700de-5f80-4316-ab05-bc1ea6078312	1707	18th century	18	2c	In colonial North America and subsequently the United States: an official appointed to represent the government in dealing with an Indigenous people; = Indian agent n. Now historical.	US/colonial: official representing government in dealing with Indigenous peoples (historical)	Thomas Nairne..is..appointed yAgent to reside among y Indians.	1707	\N	Act regulating Indian Trade (S. Carolina)	legal	\N	sense	7	agent_nn01	2025-11-06 10:37:21.166639-05	llm	2025-11-06 10:37:21.168231-05
7cf0f511-c139-463f-a37d-351734bd31ec	b40700de-5f80-4316-ab05-bc1ea6078312	1883	19th century	19	1d	Parapsychology. In telepathy: the person who originates an impression (opposed to the percipient who receives it).	Parapsychology: person who originates telepathic impression (vs. percipient receiver)	In Thought-transference..both parties (whom, for convenience' sake, we will call the Agent and the Percipient) are supposed to be in a normal state.	1883	\N	Proceedings of Society for Psychical Research	parapsychology	\N	sense	4	agent_nn01	2025-11-06 10:37:21.166655-05	llm	2025-11-06 10:37:21.168233-05
\.


--
-- Data for Name: ontologies; Type: TABLE DATA; Schema: public; Owner: ontextract_user
--

COPY public.ontologies (id, uuid, domain_id, name, base_uri, description, is_base, is_editable, parent_ontology_id, ontology_type, metadata, created_at, updated_at) FROM stdin;
\.


--
-- Data for Name: ontology_entities; Type: TABLE DATA; Schema: public; Owner: ontextract_user
--

COPY public.ontology_entities (id, ontology_id, entity_type, uri, label, comment, parent_uri, domain, range, properties, embedding, created_at) FROM stdin;
\.


--
-- Data for Name: ontology_mappings; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.ontology_mappings (id, ontology_uri, concept_label, concept_definition, parent_concepts, child_concepts, related_concepts, mapping_confidence, mapping_method, mapping_source, semantic_type, domain, properties, is_verified, verified_by, verification_notes, alternative_mappings, created_at, updated_at, verified_at, extracted_entity_id) FROM stdin;
\.


--
-- Data for Name: ontology_versions; Type: TABLE DATA; Schema: public; Owner: ontextract_user
--

COPY public.ontology_versions (id, ontology_id, version_number, version_tag, content, content_hash, change_summary, created_by, created_at, is_current, is_draft, workflow_status, metadata) FROM stdin;
\.


--
-- Data for Name: orchestration_decisions; Type: TABLE DATA; Schema: public; Owner: ontextract_user
--

COPY public.orchestration_decisions (id, activity_type, started_at_time, ended_at_time, activity_status, document_id, experiment_id, term_text, input_metadata, document_characteristics, orchestrator_provider, orchestrator_model, orchestrator_prompt, orchestrator_response, orchestrator_response_time_ms, selected_tools, embedding_model, processing_strategy, expected_runtime_seconds, decision_confidence, reasoning_summary, decision_factors, decision_validated, actual_runtime_seconds, tool_execution_success, was_associated_with, used_entity, created_at, created_by) FROM stdin;
cd2b7cef-69c1-4ebb-912b-4065af771c65	llm_orchestration	2025-11-16 10:11:31.350619-05	2025-11-16 10:12:31.350633-05	completed	172	30	agency	{"document_type": "academic", "temporal_period": "multi_period", "cross_disciplinary": true}	{"domain": "mixed", "word_count": 2500, "estimated_complexity": "high"}	anthropic	claude-sonnet-4-5	Analyze the term 'agency' in the provided document and recommend optimal NLP processing tools for semantic evolution analysis spanning 1910-2024.	Based on analysis of 'agency' in historical and contemporary contexts, I recommend a comprehensive multi-tool approach.	1250	{paragraph_segmentation,named_entity_recognition,temporal_extraction,period_aware_embeddings,oed_etymology}	sentence-transformers/all-MiniLM-L6-v2	sequential	45	0.870	Detected temporal span 1910-2024 (114 years) with philosophical and legal terminology. Recommending historical NLP tools and period-aware embeddings for diachronic analysis.	{"text_characteristics": {"complexity": "high", "requires_temporal_markers": true, "requires_entity_extraction": true}, "domain_characteristics": {"domains": ["legal", "philosophy", "computer_science"], "cross_disciplinary": true, "specialized_vocabulary": true}, "temporal_characteristics": {"span_years": 114, "historical_context_important": true, "requires_period_aware_models": true}}	t	\N	\N	\N	\N	2025-11-16 10:41:31.35202-05	1
40df2f09-75f0-4f81-85b7-f5bd821fb087	llm_orchestration	2025-11-16 10:11:31.350744-05	2025-11-16 10:12:31.350746-05	completed	173	30	agency	{"document_type": "academic", "temporal_period": "multi_period", "cross_disciplinary": true}	{"domain": "mixed", "word_count": 2500, "estimated_complexity": "high"}	anthropic	claude-sonnet-4-5	Analyze the term 'agency' in the provided document and recommend optimal NLP processing tools for semantic evolution analysis spanning 1910-2024.	Based on analysis of 'agency' in historical and contemporary contexts, I recommend a comprehensive multi-tool approach.	1250	{paragraph_segmentation,named_entity_recognition,temporal_extraction,period_aware_embeddings,oed_etymology}	sentence-transformers/all-MiniLM-L6-v2	sequential	45	0.870	Detected temporal span 1910-2024 (114 years) with philosophical and legal terminology. Recommending historical NLP tools and period-aware embeddings for diachronic analysis.	{"text_characteristics": {"complexity": "high", "requires_temporal_markers": true, "requires_entity_extraction": true}, "domain_characteristics": {"domains": ["legal", "philosophy", "computer_science"], "cross_disciplinary": true, "specialized_vocabulary": true}, "temporal_characteristics": {"span_years": 114, "historical_context_important": true, "requires_period_aware_models": true}}	t	\N	\N	\N	\N	2025-11-16 10:41:31.352026-05	1
182ec5ba-f48c-4e82-867a-4d0b161ff972	llm_orchestration	2025-11-16 10:11:31.350794-05	2025-11-16 10:12:31.350795-05	completed	174	30	agency	{"document_type": "academic", "temporal_period": "multi_period", "cross_disciplinary": true}	{"domain": "mixed", "word_count": 2500, "estimated_complexity": "high"}	anthropic	claude-sonnet-4-5	Analyze the term 'agency' in the provided document and recommend optimal NLP processing tools for semantic evolution analysis spanning 1910-2024.	Based on analysis of 'agency' in historical and contemporary contexts, I recommend a comprehensive multi-tool approach.	1250	{paragraph_segmentation,named_entity_recognition,temporal_extraction,period_aware_embeddings,oed_etymology}	sentence-transformers/all-MiniLM-L6-v2	sequential	45	0.870	Detected temporal span 1910-2024 (114 years) with philosophical and legal terminology. Recommending historical NLP tools and period-aware embeddings for diachronic analysis.	{"text_characteristics": {"complexity": "high", "requires_temporal_markers": true, "requires_entity_extraction": true}, "domain_characteristics": {"domains": ["legal", "philosophy", "computer_science"], "cross_disciplinary": true, "specialized_vocabulary": true}, "temporal_characteristics": {"span_years": 114, "historical_context_important": true, "requires_period_aware_models": true}}	t	\N	\N	\N	\N	2025-11-16 10:41:31.352029-05	1
8caab015-cad5-4813-82e8-75e13048c3e8	llm_orchestration	2025-11-16 10:11:31.350832-05	2025-11-16 10:12:31.350833-05	completed	175	30	agency	{"document_type": "academic", "temporal_period": "multi_period", "cross_disciplinary": true}	{"domain": "mixed", "word_count": 2500, "estimated_complexity": "high"}	anthropic	claude-sonnet-4-5	Analyze the term 'agency' in the provided document and recommend optimal NLP processing tools for semantic evolution analysis spanning 1910-2024.	Based on analysis of 'agency' in historical and contemporary contexts, I recommend a comprehensive multi-tool approach.	1250	{paragraph_segmentation,named_entity_recognition,temporal_extraction,period_aware_embeddings,oed_etymology}	sentence-transformers/all-MiniLM-L6-v2	sequential	45	0.870	Detected temporal span 1910-2024 (114 years) with philosophical and legal terminology. Recommending historical NLP tools and period-aware embeddings for diachronic analysis.	{"text_characteristics": {"complexity": "high", "requires_temporal_markers": true, "requires_entity_extraction": true}, "domain_characteristics": {"domains": ["legal", "philosophy", "computer_science"], "cross_disciplinary": true, "specialized_vocabulary": true}, "temporal_characteristics": {"span_years": 114, "historical_context_important": true, "requires_period_aware_models": true}}	t	\N	\N	\N	\N	2025-11-16 10:41:31.352031-05	1
2e39fe76-385c-4284-9f2b-fc8fceb51f47	llm_orchestration	2025-11-16 10:11:31.350867-05	2025-11-16 10:12:31.350869-05	completed	176	30	agency	{"document_type": "academic", "temporal_period": "multi_period", "cross_disciplinary": true}	{"domain": "mixed", "word_count": 2500, "estimated_complexity": "high"}	anthropic	claude-sonnet-4-5	Analyze the term 'agency' in the provided document and recommend optimal NLP processing tools for semantic evolution analysis spanning 1910-2024.	Based on analysis of 'agency' in historical and contemporary contexts, I recommend a comprehensive multi-tool approach.	1250	{paragraph_segmentation,named_entity_recognition,temporal_extraction,period_aware_embeddings,oed_etymology}	sentence-transformers/all-MiniLM-L6-v2	sequential	45	0.870	Detected temporal span 1910-2024 (114 years) with philosophical and legal terminology. Recommending historical NLP tools and period-aware embeddings for diachronic analysis.	{"text_characteristics": {"complexity": "high", "requires_temporal_markers": true, "requires_entity_extraction": true}, "domain_characteristics": {"domains": ["legal", "philosophy", "computer_science"], "cross_disciplinary": true, "specialized_vocabulary": true}, "temporal_characteristics": {"span_years": 114, "historical_context_important": true, "requires_period_aware_models": true}}	t	\N	\N	\N	\N	2025-11-16 10:41:31.352034-05	1
f708bd2d-9572-4157-bce1-db3644249568	llm_orchestration	2025-11-16 10:11:31.350902-05	2025-11-16 10:12:31.350903-05	completed	177	30	agency	{"document_type": "academic", "temporal_period": "multi_period", "cross_disciplinary": true}	{"domain": "mixed", "word_count": 2500, "estimated_complexity": "high"}	anthropic	claude-sonnet-4-5	Analyze the term 'agency' in the provided document and recommend optimal NLP processing tools for semantic evolution analysis spanning 1910-2024.	Based on analysis of 'agency' in historical and contemporary contexts, I recommend a comprehensive multi-tool approach.	1250	{paragraph_segmentation,named_entity_recognition,temporal_extraction,period_aware_embeddings,oed_etymology}	sentence-transformers/all-MiniLM-L6-v2	sequential	45	0.870	Detected temporal span 1910-2024 (114 years) with philosophical and legal terminology. Recommending historical NLP tools and period-aware embeddings for diachronic analysis.	{"text_characteristics": {"complexity": "high", "requires_temporal_markers": true, "requires_entity_extraction": true}, "domain_characteristics": {"domains": ["legal", "philosophy", "computer_science"], "cross_disciplinary": true, "specialized_vocabulary": true}, "temporal_characteristics": {"span_years": 114, "historical_context_important": true, "requires_period_aware_models": true}}	t	\N	\N	\N	\N	2025-11-16 10:41:31.352037-05	1
da3afb55-3aa2-4b0a-9635-e0960534a50a	llm_orchestration	2025-11-16 10:11:31.350934-05	2025-11-16 10:12:31.350935-05	completed	178	30	agency	{"document_type": "academic", "temporal_period": "multi_period", "cross_disciplinary": true}	{"domain": "mixed", "word_count": 2500, "estimated_complexity": "high"}	anthropic	claude-sonnet-4-5	Analyze the term 'agency' in the provided document and recommend optimal NLP processing tools for semantic evolution analysis spanning 1910-2024.	Based on analysis of 'agency' in historical and contemporary contexts, I recommend a comprehensive multi-tool approach.	1250	{paragraph_segmentation,named_entity_recognition,temporal_extraction,period_aware_embeddings,oed_etymology}	sentence-transformers/all-MiniLM-L6-v2	sequential	45	0.870	Detected temporal span 1910-2024 (114 years) with philosophical and legal terminology. Recommending historical NLP tools and period-aware embeddings for diachronic analysis.	{"text_characteristics": {"complexity": "high", "requires_temporal_markers": true, "requires_entity_extraction": true}, "domain_characteristics": {"domains": ["legal", "philosophy", "computer_science"], "cross_disciplinary": true, "specialized_vocabulary": true}, "temporal_characteristics": {"span_years": 114, "historical_context_important": true, "requires_period_aware_models": true}}	t	\N	\N	\N	\N	2025-11-16 10:41:31.352039-05	1
\.


--
-- Data for Name: orchestration_feedback; Type: TABLE DATA; Schema: public; Owner: ontextract_user
--

COPY public.orchestration_feedback (id, orchestration_decision_id, researcher_id, researcher_expertise, feedback_type, feedback_scope, original_decision, researcher_preference, agreement_level, confidence_assessment, reasoning, domain_specific_factors, suggested_tools, suggested_embedding_model, suggested_processing_strategy, alternative_reasoning, feedback_status, integration_notes, subsequent_decisions_influenced, improvement_verified, verification_notes, provided_at, reviewed_at, integrated_at) FROM stdin;
\.


--
-- Data for Name: orchestration_overrides; Type: TABLE DATA; Schema: public; Owner: ontextract_user
--

COPY public.orchestration_overrides (id, orchestration_decision_id, researcher_id, override_type, original_decision, overridden_decision, justification, expert_knowledge_applied, override_applied, execution_results, performance_comparison, applied_at) FROM stdin;
\.


--
-- Data for Name: processing_artifact_groups; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.processing_artifact_groups (id, document_id, artifact_type, method_key, processing_job_id, parent_method_keys, metadata, include_in_composite, status, created_at, updated_at) FROM stdin;
\.


--
-- Data for Name: processing_artifacts; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.processing_artifacts (id, processing_id, document_id, artifact_type, artifact_index, content_json, metadata_json, created_at) FROM stdin;
7116c862-b113-4d0e-9229-59be26cfec22	aaad6b88-21ee-435a-88f1-5aca81f25c4e	172	text_segment	0	{"text": "Sample paragraph 1 discussing the concept of agency in legal and philosophical contexts...", "segment_type": "paragraph", "position": 0}	{"method": "paragraph", "length": 215, "word_count": 38}	2025-11-16 10:41:31.361643
7e5d22ff-f7ae-47a8-92bd-a093680df695	aaad6b88-21ee-435a-88f1-5aca81f25c4e	172	text_segment	1	{"text": "Sample paragraph 2 discussing the concept of agency in legal and philosophical contexts...", "segment_type": "paragraph", "position": 1}	{"method": "paragraph", "length": 215, "word_count": 38}	2025-11-16 10:41:31.361647
565b3aac-6703-4485-a708-2c202893bb47	aaad6b88-21ee-435a-88f1-5aca81f25c4e	172	text_segment	2	{"text": "Sample paragraph 3 discussing the concept of agency in legal and philosophical contexts...", "segment_type": "paragraph", "position": 2}	{"method": "paragraph", "length": 215, "word_count": 38}	2025-11-16 10:41:31.36165
46681bda-cc9e-48c4-af9c-4f9f69584f33	c1370feb-eb48-4bab-9254-8cedcfd53a7b	172	extracted_entity	0	{"entity": "Anscombe", "entity_type": "PERSON", "confidence": 0.92, "context": "...discussing Anscombe in the context of agency...", "start_char": 0, "end_char": 8}	{"method": "spacy", "extraction_confidence": 0.92}	2025-11-16 10:41:31.365257
856ccc35-8dc6-4186-8d1c-254cb6de266b	c1370feb-eb48-4bab-9254-8cedcfd53a7b	172	extracted_entity	1	{"entity": "Russell", "entity_type": "PERSON", "confidence": 0.89, "context": "...discussing Russell in the context of agency...", "start_char": 100, "end_char": 107}	{"method": "spacy", "extraction_confidence": 0.89}	2025-11-16 10:41:31.365261
09c8fb6b-e2ab-4d0c-811d-faa7bf41b314	c1370feb-eb48-4bab-9254-8cedcfd53a7b	172	extracted_entity	2	{"entity": "Oxford University Press", "entity_type": "ORG", "confidence": 0.86, "context": "...discussing Oxford University Press in the context of agency...", "start_char": 200, "end_char": 223}	{"method": "spacy", "extraction_confidence": 0.86}	2025-11-16 10:41:31.365264
2e05451a-6c9b-4ca6-99f9-16c1ad293f2d	c1370feb-eb48-4bab-9254-8cedcfd53a7b	172	extracted_entity	3	{"entity": "intelligent agents", "entity_type": "CONCEPT", "confidence": 0.75, "context": "...discussing intelligent agents in the context of agency...", "start_char": 300, "end_char": 318}	{"method": "spacy", "extraction_confidence": 0.75}	2025-11-16 10:41:31.365266
0f1f273d-55fe-4085-8391-f682f15b2ada	c1370feb-eb48-4bab-9254-8cedcfd53a7b	172	extracted_entity	4	{"entity": "1957", "entity_type": "DATE", "confidence": 0.95, "context": "...discussing 1957 in the context of agency...", "start_char": 400, "end_char": 404}	{"method": "spacy", "extraction_confidence": 0.95}	2025-11-16 10:41:31.365268
155b823c-0226-4b50-a6c2-8ecdd0f969ed	676064d1-3e2f-4c1a-8473-b2107c752de5	172	embedding_vector	0	{"text": "Sample text used for embedding generation...", "vector": [-0.278096, 0.282898, -0.465144, 0.007114, -0.497745, 0.065555, 0.196701, 0.172846, 0.092702, 0.250068, -0.406492, 0.387292, 0.072921, -0.480234, -0.395739, -0.021147, 0.388315, -0.352816, 0.441119, 0.185461, 0.043577, -0.01437, -0.099606, 0.413338, -0.437607, 0.464055, -0.089588, -0.151322, 0.123028, -0.48224, 0.106889, 0.378298, 0.091164, -0.074095, 0.495229, -0.208398, 0.350381, 0.480664, 0.352077, -0.484003, -0.427572, 0.211992, -0.291608, -0.34465, 0.337908, 0.178204, 0.392145, 0.232175, 0.037952, 0.151944, 0.341197, 0.458583, -0.110752, -0.030732, -0.357862, 0.448233, -0.308908, -0.197588, -0.110301, 0.1932, 0.251187, 0.064006, -0.016431, -0.215231, 0.219769, 0.080435, -0.161372, -0.155132, -0.283494, -0.460136, 0.132215, 0.343416, -0.332729, -0.048689, 0.038868, 0.112248, 0.176988, -0.06472, -0.2085, 0.38348, 0.330299, 0.417088, -0.194822, 0.470268, 0.007916, -0.472577, 0.229061, -0.102182, 0.019367, -0.053793, -0.176032, 0.487103, 0.39837, 0.135679, -0.306283, 0.461454, -0.082994, 0.092217, 0.071925, -0.287612, -0.282831, -0.494981, -0.436337, 0.172184, -0.066312, -0.398269, 0.279401, 0.234137, -0.11506, 0.361535, -0.06957, 0.254585, 0.200501, 0.375433, 0.117861, 0.014493, -0.446181, -0.03006, 0.037087, -0.382791, -0.138253, 0.439746, 0.150315, 0.191759, 0.296544, -0.218574, 0.244591, -0.47413, 0.065891, 0.161921, -0.03507, -0.207404, -0.485008, -0.03216, 0.302431, -0.039208, -0.274078, 0.029251, 0.305403, -0.000781, -0.379253, -0.362409, 0.099614, -0.439407, 0.381139, 0.370273, 0.33027, -0.299115, -0.394679, -0.372403, -0.368296, -0.345887, -0.430825, 0.038952, 0.438359, -0.241166, -0.170233, 0.367353, -0.086904, 0.18077, -0.124965, 0.305662, 0.471479, 0.029101, -0.254827, -0.322041, 0.081903, -0.237077, 0.192404, 0.458691, 0.348027, -0.479638, 0.016314, -0.399964, -0.497411, 0.02396, -0.179976, -0.096687, -0.044754, 0.095454, -0.093268, -0.321727, 0.040219, -0.477395, 0.047481, 0.361384, -0.33218, -0.35645, 0.05337, 0.406197, 0.083658, -0.108718, 0.469844, 0.005111, -0.063472, 0.413413, 0.423878, 0.341905, 0.115368, -0.48881, -0.408926, 0.110897, -0.443755, -0.155473, 0.361143, 0.07564, 0.086579, 0.480773, 0.218869, -0.446464, 0.395074, 0.474716, 0.41301, -0.074588, -0.045349, -0.288491, -0.048224, 0.117805, 0.461292, 0.427241, -0.399709, 0.083748, 0.23781, 0.306506, -0.200757, 0.197912, 0.216855, -0.10969, -0.339209, 0.463791, 0.249298, -0.306056, -0.264261, -0.459459, -0.167017, 0.340374, 0.260018, 0.231596, 0.249831, 0.077627, -0.247346, 0.108118, -0.163279, -0.445077, -0.462679, 0.104902, -0.204566, 0.082663, 0.105944, 0.471952, -0.155243, 0.379839, 0.422089, -0.190606, 0.264892, 0.04189, 0.200867, -0.030351, -0.195545, 0.394793, -0.399014, 0.482545, -0.499681, -0.184367, 0.170489, 0.27968, -0.325565, -0.001634, -0.316668, 0.120132, -0.058539, 0.467548, 0.335849, -0.301035, -0.33439, 0.188116, 0.361551, 0.480125, -0.372563, -0.391868, -0.153305, 0.226685, -0.113034, 0.390931, 0.204254, 0.427201, -0.000672, -0.239982, 0.108517, -0.116614, -0.279173, 0.123889, 0.108797, -0.105186, -0.243782, 0.112569, 0.386079, 0.375026, -0.38468, -0.417643, -0.169205, 0.22721, -0.193882, 0.240228, -0.41003, 0.128272, 0.452574, 0.445337, 0.411341, 0.488463, -0.00807, 0.24002, -0.135196, 0.103401, -0.498849, 0.082816, 0.126064, 0.158309, -0.198217, 0.19148, 0.403225, -0.476855, -0.058536, -0.395756, 0.103546, 0.422343, 0.168186, -0.439097, -0.479783, -0.054016, -0.280877, -0.226503, -0.457019, 0.0444, 0.367627, -0.161747, 0.429433, 0.411097, 0.396477, 0.389351, -0.335791, 0.154348, 0.237468, 0.015508, 0.411029, 0.344023, -0.392893, -0.15081, 0.143035, 0.468469, -0.183417, -0.155738, 0.0833, -0.214272, -0.475126, -0.426704, 0.071105, 0.384883, -0.310615, -0.21901, -0.195514, 0.489733, 0.051376, 0.369793, 0.365317, 0.040043, -0.019784, 0.434156, 0.177, 0.122952, 0.172497, -0.425826, -0.308138, -0.321356, -0.058451, 0.294751, 0.488281, 0.033879, 0.455622, -0.360285, 0.162606, -0.23064, -0.385195, 0.348265], "model": "sentence-transformers/all-MiniLM-L6-v2"}	{"dimensions": 384, "method": "sentence-transformers"}	2025-11-16 10:41:31.366696
8492d311-fc27-4069-b3f9-af12525a3b61	9ba083cb-74ac-4a69-bc95-a46ee8c6dede	173	text_segment	0	{"text": "Sample paragraph 1 discussing the concept of agency in legal and philosophical contexts...", "segment_type": "paragraph", "position": 0}	{"method": "paragraph", "length": 215, "word_count": 38}	2025-11-16 10:41:31.370008
f0d8f40f-55ce-497d-9613-c2aa08be9996	9ba083cb-74ac-4a69-bc95-a46ee8c6dede	173	text_segment	1	{"text": "Sample paragraph 2 discussing the concept of agency in legal and philosophical contexts...", "segment_type": "paragraph", "position": 1}	{"method": "paragraph", "length": 215, "word_count": 38}	2025-11-16 10:41:31.370011
0f295be7-1327-4652-9056-ba581e41dc31	9ba083cb-74ac-4a69-bc95-a46ee8c6dede	173	text_segment	2	{"text": "Sample paragraph 3 discussing the concept of agency in legal and philosophical contexts...", "segment_type": "paragraph", "position": 2}	{"method": "paragraph", "length": 215, "word_count": 38}	2025-11-16 10:41:31.370014
31c603a1-77fd-4c6b-a0b2-b7f0568bd57e	3b693f7e-7209-4638-a059-131cafcf9037	173	extracted_entity	0	{"entity": "Anscombe", "entity_type": "PERSON", "confidence": 0.92, "context": "...discussing Anscombe in the context of agency...", "start_char": 0, "end_char": 8}	{"method": "spacy", "extraction_confidence": 0.92}	2025-11-16 10:41:31.370808
32155369-753b-4609-a636-1f23d1daa505	3b693f7e-7209-4638-a059-131cafcf9037	173	extracted_entity	1	{"entity": "Russell", "entity_type": "PERSON", "confidence": 0.89, "context": "...discussing Russell in the context of agency...", "start_char": 100, "end_char": 107}	{"method": "spacy", "extraction_confidence": 0.89}	2025-11-16 10:41:31.370812
bcb5fb6b-4a6c-4119-83e1-f1c2485ea2d1	3b693f7e-7209-4638-a059-131cafcf9037	173	extracted_entity	2	{"entity": "Oxford University Press", "entity_type": "ORG", "confidence": 0.86, "context": "...discussing Oxford University Press in the context of agency...", "start_char": 200, "end_char": 223}	{"method": "spacy", "extraction_confidence": 0.86}	2025-11-16 10:41:31.370815
f16169aa-2871-49f3-b9a6-314f4ee06b83	3b693f7e-7209-4638-a059-131cafcf9037	173	extracted_entity	3	{"entity": "intelligent agents", "entity_type": "CONCEPT", "confidence": 0.75, "context": "...discussing intelligent agents in the context of agency...", "start_char": 300, "end_char": 318}	{"method": "spacy", "extraction_confidence": 0.75}	2025-11-16 10:41:31.370817
a7e6467a-ba08-46a6-af6c-05d433b68ec2	3b693f7e-7209-4638-a059-131cafcf9037	173	extracted_entity	4	{"entity": "1957", "entity_type": "DATE", "confidence": 0.95, "context": "...discussing 1957 in the context of agency...", "start_char": 400, "end_char": 404}	{"method": "spacy", "extraction_confidence": 0.95}	2025-11-16 10:41:31.37082
0f56080a-3261-4fc9-be94-04464ecd710b	5372b543-ab2a-4895-8713-ec22ec4b373f	173	embedding_vector	0	{"text": "Sample text used for embedding generation...", "vector": [-0.039772, 0.015461, 0.435266, -0.43328, -0.275427, -0.319462, 0.147231, -0.374154, -0.217722, -0.164006, -0.107363, -0.378424, 0.499583, 0.339214, 0.245877, 0.058354, -0.378332, 0.267045, -0.252126, -0.442465, 0.393272, 0.176698, -0.495747, -0.167064, 0.191154, 0.355796, -0.280339, 0.415803, -0.080134, -0.283416, -0.187321, -0.478123, -0.214501, -0.0378, 0.455618, -0.386759, 0.275141, -0.212565, -0.239761, -0.070471, 0.325721, 0.415888, -0.298769, -0.151597, -0.388006, 0.177101, 0.31069, 0.489734, 0.277991, 0.171286, 0.321625, 0.138142, -0.006175, 0.179594, -0.313718, -0.001976, 0.31034, -0.115713, -0.199861, -0.180583, -0.385943, 0.247219, -0.477785, -0.113179, 0.132671, -0.176924, 0.108585, 0.410651, -0.13253, 0.131629, 0.18463, -0.315067, 0.480916, -0.398171, 0.266713, -0.316292, 0.14076, -0.336013, -0.083511, -0.397216, -0.455454, 0.405887, -0.392632, -0.125109, 0.26023, 0.169884, 0.251621, 0.208404, -0.001647, -0.419653, -0.03889, -0.044112, 0.3957, -0.444717, -0.002715, -0.188867, 0.093503, -0.216239, -0.31281, -0.084481, -0.033298, 0.186578, 0.174925, 0.018549, -0.209569, 0.292668, 0.246328, 0.359894, -0.282401, -0.15745, -0.157165, 0.415474, 0.333332, 0.320497, -0.104319, -0.317741, -0.456627, -0.076061, 0.222111, 0.311206, 0.279488, -0.290737, 0.341668, 0.336112, -0.173726, -0.437836, -0.166398, -0.487631, -0.114792, 0.011099, 0.14415, -0.497616, -0.17967, -0.241812, 0.326943, 0.439871, 0.109401, -0.484288, 0.244053, -0.382998, 0.14725, -0.33091, -0.087968, -0.375101, 0.41296, 0.204021, -0.411746, 0.074469, 0.023996, -0.211046, -0.421306, 0.151847, -0.356108, 0.349718, 0.287588, 0.325878, -0.478314, 0.292794, 0.351411, -0.074079, 0.349781, -0.133225, -0.411381, -0.469868, -0.451471, 0.126174, 0.330086, -0.343465, -0.366873, 0.36064, -0.449045, -0.472007, 0.045127, 0.190757, -0.132206, 0.387153, -0.212387, 0.102425, 0.263297, 0.485302, -0.320359, 0.094189, -0.080807, 0.021707, 0.350362, -0.181778, -0.471044, -0.276793, 0.337775, -0.40057, -0.398659, -0.252074, 0.240729, 0.210551, 0.361781, -0.493248, -0.29956, -0.193619, 0.016655, -0.407331, -0.4937, -0.191638, -0.03736, -0.242093, -0.045993, 0.155665, -0.408694, 0.438303, 0.287021, -0.236708, 0.471889, 0.124449, 0.224861, -0.056545, -0.06378, -0.421093, 0.195859, -0.172965, -0.169319, -0.318068, 0.065348, 0.277154, 0.445639, 0.17971, 0.111154, -0.353386, -0.14525, 0.088509, 0.154344, -0.18001, 0.458157, 0.004343, 0.476281, -0.336071, 0.372428, 0.080352, -0.010457, 0.038631, -0.460797, -0.2595, 0.215096, -0.213109, 0.329145, 0.366211, -0.361016, 0.468081, 0.387688, 0.387451, 0.063485, 0.088655, 0.030851, 0.381989, -0.454246, 0.274627, 0.064278, 0.268048, -0.083719, 0.247249, -0.296152, -0.235766, -0.033681, 0.487501, 0.305354, -0.066151, 0.088252, -0.226472, -0.48322, 0.322912, -0.068577, 0.497965, 0.314309, 0.382694, 0.479241, -0.362555, 0.285106, -0.30515, 0.050392, -0.07693, -0.391542, 0.161858, 0.143964, -0.036192, -0.261522, -0.350463, -0.40037, 0.468112, 0.121155, 0.367552, -0.002521, -0.052501, 0.15795, -0.142807, 0.435003, 0.07934, -0.196478, -0.308181, -0.115944, -0.273627, 0.303911, -0.261916, -0.275508, -0.07839, -0.306218, -0.470298, 0.419272, 0.319956, 0.153484, 0.347463, -0.455667, -0.484478, 0.472015, 0.025886, 0.424505, -0.448312, 0.002353, 0.182632, 0.179541, 0.022214, -0.496399, -0.126027, -0.491996, 0.45312, -0.405975, -0.031714, 0.393859, -0.135337, -0.437952, 0.236899, 0.130611, 0.117903, 0.082071, -0.306205, -0.396704, -0.406694, 0.347533, 0.03082, 0.167022, -0.271046, -0.180808, 0.155834, -0.186886, -0.474074, -0.231811, 0.324136, 0.05483, -0.060448, -0.420875, -0.347306, -0.085245, -0.350573, 0.082664, -0.293554, -0.151113, 0.138887, -0.095054, 0.102118, 0.236765, 0.259287, 0.317663, -0.054667, -0.07569, -0.029507, 0.170391, -0.259117, -0.223598, -0.022812, 0.437668, 0.409309, 0.297653, 0.101688, 0.286565, 0.394198, -0.31356, -0.223171, 0.06409, -0.119853, 0.250138, -0.166989, 0.028504, -0.00817, 0.447381, -0.215505, -0.4533, 0.485563], "model": "sentence-transformers/all-MiniLM-L6-v2"}	{"dimensions": 384, "method": "sentence-transformers"}	2025-11-16 10:41:31.372115
a93b5fe9-5ff4-4354-b014-301c3e0f48f5	ad225a9c-14bc-4d22-883e-a463820c2767	174	text_segment	0	{"text": "Sample paragraph 1 discussing the concept of agency in legal and philosophical contexts...", "segment_type": "paragraph", "position": 0}	{"method": "paragraph", "length": 215, "word_count": 38}	2025-11-16 10:41:31.374023
8f9d66e6-5e90-421b-b25a-bf58c423b9ab	ad225a9c-14bc-4d22-883e-a463820c2767	174	text_segment	1	{"text": "Sample paragraph 2 discussing the concept of agency in legal and philosophical contexts...", "segment_type": "paragraph", "position": 1}	{"method": "paragraph", "length": 215, "word_count": 38}	2025-11-16 10:41:31.374027
95e871e3-3127-4f43-b448-ebf21c1c7fd7	ad225a9c-14bc-4d22-883e-a463820c2767	174	text_segment	2	{"text": "Sample paragraph 3 discussing the concept of agency in legal and philosophical contexts...", "segment_type": "paragraph", "position": 2}	{"method": "paragraph", "length": 215, "word_count": 38}	2025-11-16 10:41:31.374029
9d04cb2d-c901-4d03-94e7-e6a6b6c25d68	453c93af-3eb2-41ee-9b0e-e9d131f5cd70	174	extracted_entity	0	{"entity": "Anscombe", "entity_type": "PERSON", "confidence": 0.92, "context": "...discussing Anscombe in the context of agency...", "start_char": 0, "end_char": 8}	{"method": "spacy", "extraction_confidence": 0.92}	2025-11-16 10:41:31.375036
45014371-2603-45b4-b2f0-2c3912145bce	453c93af-3eb2-41ee-9b0e-e9d131f5cd70	174	extracted_entity	1	{"entity": "Russell", "entity_type": "PERSON", "confidence": 0.89, "context": "...discussing Russell in the context of agency...", "start_char": 100, "end_char": 107}	{"method": "spacy", "extraction_confidence": 0.89}	2025-11-16 10:41:31.37504
7397264a-9412-46e9-8ebf-6895c80c5208	453c93af-3eb2-41ee-9b0e-e9d131f5cd70	174	extracted_entity	2	{"entity": "Oxford University Press", "entity_type": "ORG", "confidence": 0.86, "context": "...discussing Oxford University Press in the context of agency...", "start_char": 200, "end_char": 223}	{"method": "spacy", "extraction_confidence": 0.86}	2025-11-16 10:41:31.375043
9c696e2e-e9e3-4e94-8153-bad942d34fe5	453c93af-3eb2-41ee-9b0e-e9d131f5cd70	174	extracted_entity	3	{"entity": "intelligent agents", "entity_type": "CONCEPT", "confidence": 0.75, "context": "...discussing intelligent agents in the context of agency...", "start_char": 300, "end_char": 318}	{"method": "spacy", "extraction_confidence": 0.75}	2025-11-16 10:41:31.375045
15301d86-ae8b-4c00-9a83-b99391d606a7	453c93af-3eb2-41ee-9b0e-e9d131f5cd70	174	extracted_entity	4	{"entity": "1957", "entity_type": "DATE", "confidence": 0.95, "context": "...discussing 1957 in the context of agency...", "start_char": 400, "end_char": 404}	{"method": "spacy", "extraction_confidence": 0.95}	2025-11-16 10:41:31.375047
d00ecfa0-ccb5-42ec-9763-e5129c3d081b	3b4665b9-4d42-4b9d-9ec9-e2a4ca4f3e18	174	embedding_vector	0	{"text": "Sample text used for embedding generation...", "vector": [-0.485269, 0.148151, 0.4055, 0.292593, 0.497003, -0.320685, -0.076896, -0.178785, 0.254632, -0.36299, 0.38283, -0.24819, -0.311016, -0.382494, -0.479512, -0.443987, -0.0445, 0.279509, 0.409228, 0.437717, -0.030188, 0.206076, -0.360754, -0.081044, -0.210224, -0.301688, -0.030855, 0.067589, 0.136652, 0.279037, -0.371723, -0.231123, 0.386637, -0.115907, 0.013425, -0.479867, 0.31174, -0.15353, -0.454398, 0.416612, 0.47234, -0.292178, 0.419315, 0.48049, -0.490454, 0.319452, 0.385671, 0.190503, 0.469423, 0.063484, 0.292468, -0.299498, -0.327765, 0.100281, 0.484259, -0.097551, -0.053177, 0.279676, 0.446225, -0.243984, -0.384134, -0.268571, 0.044468, 0.36173, -0.001335, 0.374882, 0.471499, -0.406023, -0.112885, -0.228737, 0.110005, 0.046135, 0.113752, 0.017539, 0.018173, 0.178306, -0.23667, -0.105586, -0.385989, 0.346997, -0.033462, 0.017721, 0.176916, 0.175197, -0.147312, 0.387656, -0.359296, 0.269585, 0.226568, -0.458006, -0.098577, -0.289882, 0.449393, -0.051391, 0.108807, -0.429947, 0.03983, -0.366391, 0.064252, -0.068362, 0.29358, -0.049589, 0.337143, -0.407129, -0.07471, -0.35748, -0.321575, 0.444923, -0.113542, 0.101339, -0.272516, 0.0268, 0.082412, -0.151691, -0.132854, -0.346738, 0.082817, 0.244614, 0.111096, 0.02606, 0.486857, 0.259811, 0.332561, 0.133018, -0.463641, -0.383923, -0.027805, -0.252773, 0.052169, 0.364033, -0.14437, -0.044365, 0.488314, 0.085569, -0.229769, -0.476028, 0.370679, -0.08818, 0.154971, -0.08507, -0.170588, -0.082076, 0.14813, 0.125115, 0.37615, -0.480416, 0.08681, -0.470546, -0.265892, 0.434935, -0.405462, -0.32304, 0.374788, -0.033146, 0.134521, -0.205962, -0.470814, 0.176423, -0.258953, 0.330726, -0.376714, -0.16671, 0.148249, -0.327538, 0.122903, -0.095267, 0.2151, -0.024884, 0.189975, -0.272413, 0.09016, -0.099637, -0.384282, -0.181494, -0.452359, -0.354181, -0.125044, 0.14827, -0.086661, -0.018441, 0.268531, -0.378113, 0.465933, 0.211599, 0.343209, -0.077968, 0.159828, -0.155272, 0.135783, -0.130403, -0.235503, -0.028862, -0.478516, 0.304242, 0.237708, 0.442862, -0.335896, 0.366289, -0.443729, 0.457352, -0.24203, -0.373183, -0.408123, -0.066194, 0.11185, -0.329269, 0.06925, 0.372916, 0.07892, 0.39915, -0.093904, -0.194039, -0.248455, -0.294489, 0.356412, 0.299767, -0.096611, 0.083039, 0.165577, 0.154534, -0.492101, -0.462677, -0.030285, 0.301833, 0.200317, -0.219265, -0.489923, 0.347827, -0.219281, -0.316289, -0.217095, 0.06634, -0.268314, -0.291204, -0.295213, 0.016052, 0.18559, -0.010313, 0.449595, -0.099845, -0.435512, -0.066485, 0.052189, 0.387897, -0.093072, 0.040905, 0.257132, -0.419056, 0.284644, 0.28286, 0.081984, -0.147633, 0.277489, 0.331458, 0.457138, -0.087728, -0.308437, 0.493119, 0.343851, -0.240649, 0.161041, -0.369521, 0.200867, 0.430069, -0.013838, -0.412897, -0.21681, 0.274506, -0.094199, -0.325356, 0.443845, 0.108597, -0.217636, 0.216934, 0.009308, 0.332872, 0.258914, 0.002195, 0.187359, 0.053519, -0.075236, 0.201138, 0.069949, 0.380599, 0.240223, 0.059754, 0.057165, -0.081001, 0.222172, 0.174555, 0.190202, 0.302725, 0.007785, -0.133574, 0.097039, 0.160867, -0.287432, -0.354133, 0.263939, 0.063492, 0.467995, 0.165354, -0.315621, -0.376352, 0.390388, 0.180269, 0.293667, -0.143121, 0.409296, 0.027472, -0.168543, 0.387353, 0.469345, 0.317422, -0.072535, -0.11267, 0.328348, -0.307295, 0.447802, -0.192593, -0.043867, 0.164911, -0.255412, -0.009392, -0.241131, 0.221273, 0.33973, -0.47194, 0.470677, 0.16017, -0.063249, -0.059622, 0.041166, -0.01177, 0.056074, 0.337073, 0.322461, -0.466651, -0.432039, -0.1868, -0.2721, -0.37742, -0.144508, -0.102579, -0.270584, -0.452997, -0.035406, -0.308065, 0.208018, -0.067568, 0.301479, -0.091625, 0.377123, -0.309553, 0.450693, 0.490596, 0.015913, 0.332541, -0.127022, -0.077886, -0.311235, -0.216431, 0.276079, 0.158786, 0.093823, 0.164486, -0.104256, 0.214873, 0.186991, 0.423923, -0.274517, -0.031669, -0.342514, -0.219736, -0.229099, 0.396922, -0.132427, 0.300809, -0.248172, 0.168619, 0.222718, 0.357603, 0.498675, -0.490524], "model": "sentence-transformers/all-MiniLM-L6-v2"}	{"dimensions": 384, "method": "sentence-transformers"}	2025-11-16 10:41:31.37622
02b57a36-3b4a-41a9-bcac-2a16e0478e49	8af05978-c6f5-4e5f-acc2-625123860772	175	text_segment	0	{"text": "Sample paragraph 1 discussing the concept of agency in legal and philosophical contexts...", "segment_type": "paragraph", "position": 0}	{"method": "paragraph", "length": 215, "word_count": 38}	2025-11-16 10:41:31.378049
61f364c6-8ed0-415a-9c18-9ac230359a2f	8af05978-c6f5-4e5f-acc2-625123860772	175	text_segment	1	{"text": "Sample paragraph 2 discussing the concept of agency in legal and philosophical contexts...", "segment_type": "paragraph", "position": 1}	{"method": "paragraph", "length": 215, "word_count": 38}	2025-11-16 10:41:31.378053
4bb4f0ca-64a5-4b08-b99a-848bedca6cb3	8af05978-c6f5-4e5f-acc2-625123860772	175	text_segment	2	{"text": "Sample paragraph 3 discussing the concept of agency in legal and philosophical contexts...", "segment_type": "paragraph", "position": 2}	{"method": "paragraph", "length": 215, "word_count": 38}	2025-11-16 10:41:31.378056
106505c7-96d6-4906-85d5-3a92fd2e8d77	18d7f19b-1cc8-46cc-a0df-d23c1b715ba5	175	extracted_entity	0	{"entity": "Anscombe", "entity_type": "PERSON", "confidence": 0.92, "context": "...discussing Anscombe in the context of agency...", "start_char": 0, "end_char": 8}	{"method": "spacy", "extraction_confidence": 0.92}	2025-11-16 10:41:31.379024
13b5cbc0-ee4d-45ad-b4dc-61de101e6956	18d7f19b-1cc8-46cc-a0df-d23c1b715ba5	175	extracted_entity	1	{"entity": "Russell", "entity_type": "PERSON", "confidence": 0.89, "context": "...discussing Russell in the context of agency...", "start_char": 100, "end_char": 107}	{"method": "spacy", "extraction_confidence": 0.89}	2025-11-16 10:41:31.379029
76a76243-03b8-43a3-9207-ceea998eede1	18d7f19b-1cc8-46cc-a0df-d23c1b715ba5	175	extracted_entity	2	{"entity": "Oxford University Press", "entity_type": "ORG", "confidence": 0.86, "context": "...discussing Oxford University Press in the context of agency...", "start_char": 200, "end_char": 223}	{"method": "spacy", "extraction_confidence": 0.86}	2025-11-16 10:41:31.379032
c96d038e-e4ae-416c-a1f3-10775ccfcf2e	18d7f19b-1cc8-46cc-a0df-d23c1b715ba5	175	extracted_entity	3	{"entity": "intelligent agents", "entity_type": "CONCEPT", "confidence": 0.75, "context": "...discussing intelligent agents in the context of agency...", "start_char": 300, "end_char": 318}	{"method": "spacy", "extraction_confidence": 0.75}	2025-11-16 10:41:31.379034
38a057fa-8bdd-4949-bbc0-1fb4a3144768	18d7f19b-1cc8-46cc-a0df-d23c1b715ba5	175	extracted_entity	4	{"entity": "1957", "entity_type": "DATE", "confidence": 0.95, "context": "...discussing 1957 in the context of agency...", "start_char": 400, "end_char": 404}	{"method": "spacy", "extraction_confidence": 0.95}	2025-11-16 10:41:31.379036
5a7ed32c-0458-499d-96a9-752d3f12cc13	b715bf77-dd26-4c84-a3a4-1e9646598b20	175	embedding_vector	0	{"text": "Sample text used for embedding generation...", "vector": [0.413565, 0.014271, 0.166956, -0.182197, -0.353589, -0.455962, -0.056627, -0.479131, -0.483973, 0.064366, 0.009674, -0.293833, 0.453422, 0.413306, 0.153024, -0.145565, -0.211057, 0.147979, -0.073504, 0.057872, -0.333678, -0.181442, 0.462028, -0.185831, 0.195817, 0.460158, -0.361152, -0.045964, 0.272662, -0.278139, 0.202743, -0.327226, -0.135788, -0.497206, -0.472394, 0.421858, 0.248226, -0.09593, 0.087159, -0.011975, -0.312018, -0.418505, -0.486089, 0.369609, 0.158817, -0.26244, -0.210961, 0.447933, 0.114343, -0.091716, 0.239678, 0.13968, -0.393736, 0.302414, -0.280434, 0.177615, -0.156121, 0.206478, 0.329152, -0.471204, -0.033394, -0.346716, 0.367775, -0.103817, -0.490811, 0.135504, -0.107492, 0.094768, -0.002569, -0.289315, -0.370242, 0.267273, -0.286689, 0.265135, 0.200798, 0.148298, -0.056388, 0.157308, -0.029999, -0.330561, -0.246818, 0.12307, -0.382658, -0.202537, -0.165371, 0.023249, -0.46143, -0.356403, -0.087792, 0.043089, 0.158768, -0.008199, 0.003367, 0.207962, -0.21121, 0.174487, -0.195635, -0.456687, -0.217207, -0.289188, 0.052339, 0.31248, 0.311631, 0.006477, 0.231911, 0.185886, -0.188868, 0.034015, 0.028465, 0.081157, -0.135855, -0.121702, -0.091654, -0.241498, -0.042625, 0.078327, -0.081835, 0.329107, -0.046723, 0.278837, -0.404443, -0.404387, -0.136829, 0.172104, 0.114517, 0.016118, -0.488506, -0.140486, -0.133331, -0.152211, -0.017182, 0.439376, 0.302061, 0.215316, 0.325033, 0.022977, -0.132906, 0.293671, -0.169373, -0.358352, -0.19847, -0.263946, -0.282097, -0.0068, 0.124584, -0.478098, 0.361869, -0.324229, -0.491895, -0.300445, -0.117391, 0.38087, -0.461247, -0.168037, 0.07319, -0.305105, -0.314756, 0.321896, -0.146669, -0.277726, -0.353117, -0.264527, -0.233771, 0.471012, 0.234682, 0.417676, -0.147211, -0.146266, 0.407277, 0.358106, -0.107493, 0.341558, -0.238309, -0.108619, -0.381331, -0.109968, 0.31345, 0.416386, 0.477056, -0.13377, -0.080883, -0.052693, -0.151306, -0.27536, -0.465147, -0.157239, -0.010829, -0.098552, -0.46273, -0.378958, -0.15961, 0.107606, -0.188879, -0.209351, -0.289899, -0.126557, -0.021223, 0.384519, -0.316526, -0.109998, -0.385226, -0.095141, -0.103319, 0.24498, 0.258715, 0.325767, -0.014817, -0.409369, 0.11284, -0.044658, -0.308428, 0.494797, 0.473348, -0.416432, 0.257803, 0.379697, 0.428154, 0.047677, -0.05554, 0.410117, 0.070871, -0.426175, -0.223901, 0.114771, 0.399202, -0.044005, 0.460769, 0.068215, 0.258975, 0.316947, 0.438071, 0.154664, 0.037731, 0.331617, 0.125079, 0.447961, -0.437155, 0.361878, 0.120153, -0.215713, -0.131098, 0.256668, -0.358716, 0.112049, 0.225778, -0.340948, 0.074921, -0.148457, -0.393832, 0.19313, 0.069396, -0.001917, 0.325211, 0.030899, 0.116903, 0.059259, -0.126784, 0.178378, -0.00919, 0.273316, 0.106425, -0.486228, 0.348667, 0.123185, 0.412072, 0.116682, 0.311075, -0.036886, 0.108992, 0.182799, -0.278525, 0.27561, 0.395851, 0.264759, 0.409057, -0.473817, -0.01017, 0.343284, 0.425912, 0.286357, 0.069984, 0.136117, -0.326472, -0.289119, -0.019345, 0.353215, 0.411509, -0.238055, -0.209804, -0.054884, 0.260243, 0.381371, 0.480476, 0.333647, 0.165799, -0.453055, 0.424799, 0.243474, -0.475467, -0.384089, -0.455764, -0.443031, -0.225664, -0.079251, -0.172071, -0.261091, -0.488393, 0.299244, 0.464788, 0.224286, -0.309295, -0.301607, 0.490291, -0.278836, -0.328502, 0.304873, 0.150127, -0.119148, 0.325072, 0.386752, -0.164107, 0.146073, 0.35438, -0.38811, 0.113833, 0.071343, -0.388924, -0.256024, -0.262366, -0.331225, -0.293074, -0.334449, -0.092672, 0.16311, -0.127462, 0.435462, 0.41666, -0.207619, 0.018773, -0.470385, -0.467595, -0.06688, 0.186867, -0.363609, -0.347793, -0.35018, -0.301313, -0.040549, 0.143119, -0.097748, 0.07509, 0.022473, 0.354003, -0.419669, 0.007912, 0.109141, -0.010431, 0.405978, -0.450665, -0.366787, -0.311055, -0.317691, -0.229673, -0.029821, 0.340247, 0.337757, 0.391371, -0.041927, 0.43471, 0.166824, -0.319389, 0.472033, 0.137484, 0.394486, 0.206149, 0.290073, 0.333609, 0.051618, 0.152757, -0.08045, -0.144284, -0.088545, -0.244661, 0.016719], "model": "sentence-transformers/all-MiniLM-L6-v2"}	{"dimensions": 384, "method": "sentence-transformers"}	2025-11-16 10:41:31.380531
b38e474b-2d0e-4232-bd7c-599a394bb27c	e0e9ec47-2c2b-4cf9-85cd-d2a75ba28663	176	text_segment	0	{"text": "Sample paragraph 1 discussing the concept of agency in legal and philosophical contexts...", "segment_type": "paragraph", "position": 0}	{"method": "paragraph", "length": 215, "word_count": 38}	2025-11-16 10:41:31.382441
9695de67-d5c0-403f-969a-b4a72847eedb	e0e9ec47-2c2b-4cf9-85cd-d2a75ba28663	176	text_segment	1	{"text": "Sample paragraph 2 discussing the concept of agency in legal and philosophical contexts...", "segment_type": "paragraph", "position": 1}	{"method": "paragraph", "length": 215, "word_count": 38}	2025-11-16 10:41:31.382445
ff260aef-94a2-411c-b7e2-5a7c798189eb	e0e9ec47-2c2b-4cf9-85cd-d2a75ba28663	176	text_segment	2	{"text": "Sample paragraph 3 discussing the concept of agency in legal and philosophical contexts...", "segment_type": "paragraph", "position": 2}	{"method": "paragraph", "length": 215, "word_count": 38}	2025-11-16 10:41:31.382447
2484b8d3-2009-4bc8-99ec-ec305f9438aa	ca489974-43bf-440a-a875-f61c6906b06b	176	extracted_entity	0	{"entity": "Anscombe", "entity_type": "PERSON", "confidence": 0.92, "context": "...discussing Anscombe in the context of agency...", "start_char": 0, "end_char": 8}	{"method": "spacy", "extraction_confidence": 0.92}	2025-11-16 10:41:31.383198
e6ded112-5ad1-4673-aa5f-67aea19a1528	ca489974-43bf-440a-a875-f61c6906b06b	176	extracted_entity	1	{"entity": "Russell", "entity_type": "PERSON", "confidence": 0.89, "context": "...discussing Russell in the context of agency...", "start_char": 100, "end_char": 107}	{"method": "spacy", "extraction_confidence": 0.89}	2025-11-16 10:41:31.383202
aa54e634-ee2c-478e-83d0-1066093f6833	ca489974-43bf-440a-a875-f61c6906b06b	176	extracted_entity	2	{"entity": "Oxford University Press", "entity_type": "ORG", "confidence": 0.86, "context": "...discussing Oxford University Press in the context of agency...", "start_char": 200, "end_char": 223}	{"method": "spacy", "extraction_confidence": 0.86}	2025-11-16 10:41:31.383204
23621fdf-1d0c-47a0-8d97-b54236152dd9	ca489974-43bf-440a-a875-f61c6906b06b	176	extracted_entity	3	{"entity": "intelligent agents", "entity_type": "CONCEPT", "confidence": 0.75, "context": "...discussing intelligent agents in the context of agency...", "start_char": 300, "end_char": 318}	{"method": "spacy", "extraction_confidence": 0.75}	2025-11-16 10:41:31.383206
b6537a7b-baaa-4b9b-9556-ea164883d2d3	ca489974-43bf-440a-a875-f61c6906b06b	176	extracted_entity	4	{"entity": "1957", "entity_type": "DATE", "confidence": 0.95, "context": "...discussing 1957 in the context of agency...", "start_char": 400, "end_char": 404}	{"method": "spacy", "extraction_confidence": 0.95}	2025-11-16 10:41:31.383209
0e34e65b-f9c2-4ec2-ae72-cee9615c5002	6adde32f-49ff-4a70-be6c-3e7016ea201d	176	embedding_vector	0	{"text": "Sample text used for embedding generation...", "vector": [-0.464593, -0.332575, -0.47139, -0.467936, 0.305045, -0.477318, -0.002868, 0.179658, 0.485824, -0.301825, -0.399711, 0.257788, 0.32478, 0.177562, -0.448049, -0.06671, -0.44167, -0.319521, 0.075713, 0.114946, 0.097868, 0.261397, 0.124496, -0.127145, -0.420903, -0.326987, -0.173247, 0.219395, 0.276183, 0.02986, -0.01203, 0.216255, 0.160166, 0.318613, 0.495496, 0.390561, 0.492825, -0.301926, -0.408445, 0.173296, -0.460347, -0.044175, 0.317315, 0.392599, -0.144558, -0.124392, -0.283222, 0.167504, 0.376505, -0.433785, 0.454787, -0.371433, 0.052936, -0.051647, 0.262437, -0.390028, -0.344274, 0.345893, 0.436793, 0.411104, -0.108894, -0.022684, 0.004036, -0.406498, -0.441366, -0.109865, -0.218226, -0.314404, 0.236815, -0.070244, -0.109404, 0.213185, 0.275886, -0.211757, 0.091872, -0.006822, -0.119014, -0.110056, -0.190432, -0.467797, 0.286785, 0.001299, -0.436728, 0.300746, 0.423252, -0.106908, 0.045, -0.076404, -0.311172, 0.404751, 0.453289, 0.124777, 0.039466, -0.228581, -0.458041, -0.002099, 0.31967, 0.346816, 0.101869, -0.120998, 0.403089, 0.115364, -0.132576, -0.366996, 0.394261, 0.414104, 0.1696, -0.140659, 0.290031, -0.268164, 0.017451, 0.433433, -0.350059, 0.17561, 0.137423, -0.096019, 0.404159, 0.186754, 0.126036, 0.374823, 0.423808, -0.13899, 0.134738, 0.465112, 0.450849, -0.011926, 0.074046, 0.260522, 0.418828, 0.344185, 0.435442, -0.009597, 0.47223, -0.043719, 0.277545, 0.418384, 0.290274, 0.22456, -0.269108, 0.233109, 0.469995, 0.173323, 0.170724, -0.4093, 0.168565, -0.476956, 0.262819, 0.289136, 0.070081, -0.006273, -0.04066, -0.073452, -0.034611, -0.113516, -0.416943, 0.151116, -0.437073, -0.193458, 0.130785, -0.202055, -0.379458, -0.474321, 0.424156, -0.328805, 0.165807, -0.088868, 0.213619, -0.414915, 0.062689, -0.036087, -0.116849, 0.22109, -0.230471, 0.316478, -0.08974, -0.201388, -0.411041, 0.477776, -0.423552, 0.46612, 0.347235, -0.250712, 0.167629, 0.349966, -0.302195, -0.393904, 0.402114, 0.460892, 0.065058, -0.432285, 0.234415, 0.174107, -0.273847, 0.217741, -0.110183, 0.370165, 0.307314, 0.007869, 0.11201, 0.051479, 0.486586, 0.45677, 0.291787, -0.286526, -0.407613, -0.245238, -0.366818, 0.224429, 0.323798, 0.445446, -0.336891, 0.089291, -0.479491, 0.382373, 0.491848, 0.134912, -0.300441, -0.425125, 0.004088, -0.105747, 0.326155, 0.399949, -0.119524, -0.347778, -0.182238, 0.388891, 0.035306, -0.005253, -0.24983, -0.192253, 0.063943, 0.259879, -0.163478, -0.060247, -0.18928, -0.138149, -0.498615, 0.404304, 0.162216, 0.479535, -0.175582, 0.186984, -0.445176, 0.193949, 0.407605, -0.200616, 0.380145, 0.014642, 0.359749, 0.323735, 0.280163, -0.029527, 0.007048, -0.098264, 0.251764, -0.449539, -0.211286, -0.142633, 0.36738, -0.232196, 0.315323, -0.084261, 0.373718, -0.266694, -0.320338, -0.258873, 0.28863, -0.16668, 0.070445, 0.360609, -0.02713, -0.42904, -0.101887, -0.399679, -0.071806, 0.004172, -0.202673, -0.118476, -0.122592, -0.246499, -0.276522, -0.197959, -0.009119, -0.077329, -0.023138, -0.459528, -0.312773, 0.144828, 0.123087, 0.109967, -0.391879, 0.047027, 0.498752, -0.378517, 0.246023, -0.360301, 0.082222, 0.251281, -0.048439, -0.456983, -0.072885, -0.388884, 0.466359, -0.320541, -0.447398, 0.13472, -0.220985, -0.02492, -0.257966, 0.438013, 0.038556, 0.304698, -0.463935, -0.092393, -0.189861, -0.455186, 0.392126, 0.465611, -0.483945, -0.094992, -0.466639, 0.221572, -0.37393, -0.319641, 0.403763, -0.016174, 0.418637, 0.221043, -0.452068, -0.187491, 0.487527, -0.338431, 0.420815, 0.398291, 0.369731, 0.464088, 0.03745, 0.420553, 0.360864, -0.163405, -0.299671, 0.058424, -0.132502, 0.475513, -0.195952, -0.032626, -0.440436, 0.023135, -0.443667, -0.045479, -0.276369, 0.441342, 0.137304, 0.095597, -0.110983, 0.359481, 0.243992, -0.049988, -0.393854, -0.237019, -0.044703, 0.140241, 0.050429, 0.262948, -0.274946, -0.352323, 0.230486, -0.350404, -0.400488, 0.302298, 0.457738, -0.445655, -0.034298, 0.11744, -0.428362, -0.441374, 0.122129, -0.331315, -0.083003, -0.4784, -0.048651, -0.012193, 0.221326, 0.409707], "model": "sentence-transformers/all-MiniLM-L6-v2"}	{"dimensions": 384, "method": "sentence-transformers"}	2025-11-16 10:41:31.384299
46370d10-bc6c-4d79-991d-22e62484157b	bfbdc0e9-766c-4778-99bd-f086adf4316e	177	text_segment	0	{"text": "Sample paragraph 1 discussing the concept of agency in legal and philosophical contexts...", "segment_type": "paragraph", "position": 0}	{"method": "paragraph", "length": 215, "word_count": 38}	2025-11-16 10:41:31.386246
71395cbe-a830-4ef6-955b-ecd77e5e7cd7	bfbdc0e9-766c-4778-99bd-f086adf4316e	177	text_segment	1	{"text": "Sample paragraph 2 discussing the concept of agency in legal and philosophical contexts...", "segment_type": "paragraph", "position": 1}	{"method": "paragraph", "length": 215, "word_count": 38}	2025-11-16 10:41:31.386249
25a339b2-42fa-405a-bd71-a5e8becd6813	bfbdc0e9-766c-4778-99bd-f086adf4316e	177	text_segment	2	{"text": "Sample paragraph 3 discussing the concept of agency in legal and philosophical contexts...", "segment_type": "paragraph", "position": 2}	{"method": "paragraph", "length": 215, "word_count": 38}	2025-11-16 10:41:31.386252
cb35850a-985e-4265-b2b7-1564693bb58c	1fb2670b-ef1d-4d47-8aa2-b88dfdd12da6	177	extracted_entity	0	{"entity": "Anscombe", "entity_type": "PERSON", "confidence": 0.92, "context": "...discussing Anscombe in the context of agency...", "start_char": 0, "end_char": 8}	{"method": "spacy", "extraction_confidence": 0.92}	2025-11-16 10:41:31.386991
cbd424ce-ad5a-4c6d-8433-2cf701b06dc9	1fb2670b-ef1d-4d47-8aa2-b88dfdd12da6	177	extracted_entity	1	{"entity": "Russell", "entity_type": "PERSON", "confidence": 0.89, "context": "...discussing Russell in the context of agency...", "start_char": 100, "end_char": 107}	{"method": "spacy", "extraction_confidence": 0.89}	2025-11-16 10:41:31.386994
ec9bea5a-c2d6-417e-9a52-8059e99e8a3b	1fb2670b-ef1d-4d47-8aa2-b88dfdd12da6	177	extracted_entity	2	{"entity": "Oxford University Press", "entity_type": "ORG", "confidence": 0.86, "context": "...discussing Oxford University Press in the context of agency...", "start_char": 200, "end_char": 223}	{"method": "spacy", "extraction_confidence": 0.86}	2025-11-16 10:41:31.386997
4d005ef0-36c4-45ae-8c95-f5f1915bfb51	1fb2670b-ef1d-4d47-8aa2-b88dfdd12da6	177	extracted_entity	3	{"entity": "intelligent agents", "entity_type": "CONCEPT", "confidence": 0.75, "context": "...discussing intelligent agents in the context of agency...", "start_char": 300, "end_char": 318}	{"method": "spacy", "extraction_confidence": 0.75}	2025-11-16 10:41:31.386999
3726c706-77c6-4b51-867a-767d4b3ffc14	1fb2670b-ef1d-4d47-8aa2-b88dfdd12da6	177	extracted_entity	4	{"entity": "1957", "entity_type": "DATE", "confidence": 0.95, "context": "...discussing 1957 in the context of agency...", "start_char": 400, "end_char": 404}	{"method": "spacy", "extraction_confidence": 0.95}	2025-11-16 10:41:31.387001
7be7e81f-9738-4984-9122-cedf1467e30d	827de3ab-7193-4c32-ac58-5c547be9d9d4	177	embedding_vector	0	{"text": "Sample text used for embedding generation...", "vector": [0.140685, -0.353537, -0.208065, -0.225448, -0.084978, -0.030214, 0.237369, 0.040411, 0.277059, -0.26285, 0.022852, 0.428828, -0.043929, 0.364413, 0.16851, -0.403361, -0.474233, -0.022927, 0.139582, -0.383579, -0.306948, -0.082888, -0.130756, -0.41158, -0.338563, -0.093502, -0.055577, -0.233267, -0.397996, 0.308802, 0.238134, -0.030787, -0.181136, -0.313495, -0.224922, 0.283944, -0.488735, -0.362256, 0.115605, -0.312021, -0.464851, -0.054019, 0.353919, -0.271519, -0.105263, -0.17961, 0.288993, 0.462573, 0.324135, -0.334516, -0.33107, 0.182188, -0.217258, 0.337962, -0.485838, 0.308889, 0.205888, 0.382357, -0.377759, 0.407377, 0.323367, 0.004665, -0.052131, 0.32564, -0.057324, 0.102114, 0.448284, -0.124359, 0.373991, -0.265553, -0.109817, -0.072551, -0.163204, -0.468357, 0.211234, -0.039896, 0.418077, -0.284237, -0.263932, -0.120034, 0.03833, -0.029397, 0.080886, -0.218416, 0.417323, -0.209619, -0.480985, -0.083731, 0.222659, 0.398471, 0.399088, 0.253949, 0.38389, -0.404195, -0.458188, -0.432948, 0.469893, 0.316422, 0.468161, 0.492099, -0.27587, 0.173389, 0.375615, -0.407935, 0.448837, -0.385581, -0.051942, 0.144674, -0.303493, 0.358462, 0.095237, 0.273509, 0.142379, 0.160539, -0.471436, -0.490529, -0.002266, -0.284457, -0.175448, 0.18119, 0.044358, 0.459725, -0.205329, 0.419793, 0.121613, -0.387795, 0.099392, -0.329835, -0.159637, 0.190052, -0.341709, -0.168872, -0.218091, -0.015694, -0.303887, -0.034747, 0.096398, -0.159727, -0.194304, -0.070163, 0.430224, -0.104842, 0.491222, -0.28661, -0.416041, 0.432453, -0.457316, -0.138556, -0.176447, 0.196096, 0.047307, 0.265064, -0.175362, 0.171698, 0.235884, -0.085117, -0.071451, 0.041542, 0.001825, 0.360222, 0.380945, -0.207446, -0.30163, -0.139839, 0.401762, 0.054427, 0.363671, 0.444004, 0.488482, 0.201348, 0.081543, 0.149423, 0.228746, -0.270019, -0.43775, -0.407579, -0.387923, 0.127416, 0.496439, 0.216528, -0.012005, 0.477144, -0.383602, 0.290586, 0.25235, 0.246318, 0.119786, -0.093167, -0.45194, 0.19924, 0.297896, -0.212969, -0.163094, 0.123326, -0.386501, 0.136253, -0.242316, 0.169417, -0.413375, -0.267359, 0.085028, 0.079175, -0.405342, -0.0013, 0.100717, 0.402148, 0.227275, 0.21344, 0.119919, -0.068823, 0.190098, 0.168509, 0.241667, -0.024507, -0.340828, -0.262283, 0.167235, -0.09981, 0.115705, 0.173574, -0.277673, 0.187635, 0.062977, 0.481494, 0.115668, 0.115195, -0.079792, -0.179804, -0.25154, -0.128187, 0.053187, 0.493954, 0.051901, -0.108874, -0.212246, -0.19693, -0.038577, -0.404214, 0.26199, -0.116504, -0.080695, 0.054296, 0.125586, -0.29661, -0.147783, 0.081203, 0.179146, -0.177773, 0.183846, 0.293946, -0.483231, 0.415846, -0.395961, 0.367502, 0.147425, 0.467971, 0.248909, 0.201746, -0.311858, 0.056519, -0.150034, 0.229918, -0.220005, -0.218264, -0.463816, 0.219988, 0.072314, 0.117616, -0.24247, 0.004793, 0.374525, -0.369635, -0.167741, 0.305391, 0.234497, 0.256892, 0.174465, 0.322457, 0.004141, 0.305886, 0.019429, 0.416781, -0.346723, -0.422386, 0.193734, 0.466826, -0.129091, -0.261575, -0.211365, 0.061726, 0.041349, -0.373579, 0.110687, -0.308364, -0.335967, -0.34387, 0.321549, -0.255689, -0.332076, -0.11519, -0.462296, -0.260547, -0.098614, -0.233985, 0.402345, 0.004662, -0.446057, -0.120091, -0.369726, 0.135276, 0.325069, -0.425089, -0.476003, 0.308756, -0.484879, 0.304676, 0.153216, 0.163838, 0.104421, 0.070926, 0.458462, -0.340002, -0.383306, -0.260814, -0.211891, -0.097681, -0.2193, -0.451751, 0.43015, 0.44128, -0.016548, -0.375598, 0.120259, 0.086715, -0.146918, -0.093504, -0.043633, 0.009216, 0.141017, -0.034597, -0.373969, 0.226485, -0.185218, 0.16876, 0.274667, 0.041317, 0.227208, 0.154443, -0.088509, 0.056184, 0.323666, -0.392718, 0.227203, 0.46996, -0.430494, -0.371713, -0.148316, 0.4029, -0.212471, 0.055511, 0.303174, -0.040994, -0.417782, 0.366649, 0.157102, -0.077519, 0.077634, -0.059039, -0.328277, 0.419845, 0.311616, -0.25983, -0.249622, 0.340625, 0.188612, -0.499695, -0.463077, 0.210365, -0.389389, -0.459522, 0.386975, 0.310974, 0.489684, 0.18711], "model": "sentence-transformers/all-MiniLM-L6-v2"}	{"dimensions": 384, "method": "sentence-transformers"}	2025-11-16 10:41:31.388062
04279a25-7828-4c81-90a3-50c99905e55c	bd82cf4a-b4d7-45a9-ba39-d92a7f26b00d	178	text_segment	0	{"text": "Sample paragraph 1 discussing the concept of agency in legal and philosophical contexts...", "segment_type": "paragraph", "position": 0}	{"method": "paragraph", "length": 215, "word_count": 38}	2025-11-16 10:41:31.389914
3b6b273e-7ac2-40e3-b632-9f66c98fa2ff	bd82cf4a-b4d7-45a9-ba39-d92a7f26b00d	178	text_segment	1	{"text": "Sample paragraph 2 discussing the concept of agency in legal and philosophical contexts...", "segment_type": "paragraph", "position": 1}	{"method": "paragraph", "length": 215, "word_count": 38}	2025-11-16 10:41:31.389918
00fecb38-1ace-4059-9aeb-44ddbb18b74f	bd82cf4a-b4d7-45a9-ba39-d92a7f26b00d	178	text_segment	2	{"text": "Sample paragraph 3 discussing the concept of agency in legal and philosophical contexts...", "segment_type": "paragraph", "position": 2}	{"method": "paragraph", "length": 215, "word_count": 38}	2025-11-16 10:41:31.389921
94904a01-c74b-4f18-b46e-ac33a8fb21dd	b4e0e861-4f0c-4ea6-bd12-8a5bae210a97	178	extracted_entity	0	{"entity": "Anscombe", "entity_type": "PERSON", "confidence": 0.92, "context": "...discussing Anscombe in the context of agency...", "start_char": 0, "end_char": 8}	{"method": "spacy", "extraction_confidence": 0.92}	2025-11-16 10:41:31.390846
c43737b5-10c9-42fa-bfa0-e4294ff7ffb6	b4e0e861-4f0c-4ea6-bd12-8a5bae210a97	178	extracted_entity	1	{"entity": "Russell", "entity_type": "PERSON", "confidence": 0.89, "context": "...discussing Russell in the context of agency...", "start_char": 100, "end_char": 107}	{"method": "spacy", "extraction_confidence": 0.89}	2025-11-16 10:41:31.390849
c2056c87-a497-4e37-b8b2-99cc1ae94890	b4e0e861-4f0c-4ea6-bd12-8a5bae210a97	178	extracted_entity	2	{"entity": "Oxford University Press", "entity_type": "ORG", "confidence": 0.86, "context": "...discussing Oxford University Press in the context of agency...", "start_char": 200, "end_char": 223}	{"method": "spacy", "extraction_confidence": 0.86}	2025-11-16 10:41:31.390852
87e4f00a-5d5c-48c1-b6ae-158e4edb8f1f	b4e0e861-4f0c-4ea6-bd12-8a5bae210a97	178	extracted_entity	3	{"entity": "intelligent agents", "entity_type": "CONCEPT", "confidence": 0.75, "context": "...discussing intelligent agents in the context of agency...", "start_char": 300, "end_char": 318}	{"method": "spacy", "extraction_confidence": 0.75}	2025-11-16 10:41:31.390854
d97ab622-4e38-4dca-8d08-7cde2c96388d	b4e0e861-4f0c-4ea6-bd12-8a5bae210a97	178	extracted_entity	4	{"entity": "1957", "entity_type": "DATE", "confidence": 0.95, "context": "...discussing 1957 in the context of agency...", "start_char": 400, "end_char": 404}	{"method": "spacy", "extraction_confidence": 0.95}	2025-11-16 10:41:31.390856
a8f76da5-0b1b-4b44-9824-f09753492ae2	8edce7ac-897a-48d4-8936-d43a21721264	178	embedding_vector	0	{"text": "Sample text used for embedding generation...", "vector": [-0.324633, 0.227642, -0.35885, -0.146502, -0.493124, 0.132198, -0.127833, -0.135455, 0.290429, 0.406541, 0.051843, 0.123553, -0.318388, 0.297855, 0.017133, 0.256251, 0.355081, -0.374271, 0.485998, 0.490282, 0.010368, 0.039635, -0.131364, -0.330025, -0.173797, -0.448298, -0.213494, 0.335776, 0.250318, 0.269743, 0.489864, 0.397749, -0.362902, -0.043174, -0.049543, 0.180802, 0.435293, 0.29765, -0.441505, 0.132383, -0.144225, 0.111379, 0.216804, 0.046151, 0.457141, 0.203954, 0.230379, 0.440606, -0.030769, 0.135925, -0.470473, -0.444564, 0.341109, 0.499875, -0.345302, 0.207311, -0.462654, -0.066483, 0.360574, -0.029573, -0.25048, 0.137143, 0.267812, 0.172897, 0.033899, 0.248818, -0.364297, -0.437671, 0.071613, 0.194293, -0.02652, -0.210872, 0.413742, 0.382352, 0.237507, -0.199129, -0.093799, 0.1609, -0.324666, 0.063359, 0.476457, 0.170962, -0.284417, -0.127143, -0.308299, 0.404481, -0.313801, -0.329839, 0.474293, 0.145141, 0.231504, -0.306572, -0.400892, 0.090724, 0.334383, -0.193317, 0.126254, 0.164224, -0.300313, 0.22671, -0.430095, 0.040672, -0.021591, 0.241529, -0.422339, -0.003547, -0.218694, 0.241905, 0.271866, -0.107683, 0.055586, 0.063062, -0.071129, 0.020359, 0.23644, 0.241662, 0.495395, 0.166257, -0.48694, -0.274486, 0.195151, -0.117066, -0.300363, -0.394965, -0.326764, 0.051538, 0.258665, -0.070869, 0.188146, -0.028219, 0.371758, 0.427332, -0.466976, 0.3222, 0.162241, 0.112853, -0.054166, 0.385653, 0.365, 0.278046, -0.072732, -0.387489, 0.296575, 0.331885, 0.485013, 0.264199, -0.281014, 0.017365, 0.348861, 0.197851, -0.488881, -0.361909, 0.033952, 0.361778, 0.477995, -0.422486, 0.49779, 0.359239, 0.118366, -0.097385, 0.458935, -0.068877, 0.243821, 0.277972, 0.315419, 0.338123, 0.353852, 0.46235, -0.191056, -0.443262, 0.284513, 0.423405, -0.457057, -0.208469, -0.43467, -0.364312, -0.011284, -0.39419, 0.108635, -0.321995, 0.344612, -0.062607, -0.173852, -0.269661, -0.486162, -0.306981, -0.4594, 0.495379, 0.038797, 0.281897, 0.226237, -0.23484, -0.446785, 0.041716, 0.381991, 0.204668, 0.407763, -0.169789, 0.293923, 0.149823, -0.472866, 0.25543, 0.079592, -0.3461, 0.268045, -0.224228, -0.46327, 0.282342, -0.300317, -0.378646, 0.338972, 0.363848, 0.232157, -0.078458, 0.151963, -0.257887, -0.057497, 0.314664, 0.398734, -0.005767, 0.168773, 0.439431, -0.027673, -0.040594, 0.114128, -0.335891, -0.49236, 0.48531, 0.469401, -0.384962, -0.108317, -0.416969, 0.245005, 0.259833, 0.332578, 0.267936, 0.176513, 0.235614, 0.187486, -0.39597, -0.26042, 0.397202, 0.018339, -0.299246, -0.069782, 0.366125, -0.358099, 0.134367, -0.296763, -0.284077, -0.372024, 0.456023, -0.351016, -0.22918, 0.441291, 0.327628, 0.446673, 0.264741, 0.49578, -0.280489, -0.476007, 0.364856, -0.38312, -0.26276, 0.086182, -0.346684, -0.100538, 0.180604, -0.222327, 0.032901, 0.268491, 0.124435, 0.398455, 0.112715, 0.14917, -0.029329, 0.25629, 0.474517, 0.006182, -0.094284, -0.016437, 0.271937, -0.271541, -0.215049, 0.185314, 0.387994, -0.394665, -0.174345, -0.120939, 0.388624, -0.460151, 0.407356, -0.236781, -0.435238, 0.058581, -0.177181, -0.008372, 0.270063, -0.367666, -0.371314, 0.441066, -0.299107, -0.200577, -0.189174, 0.446521, 0.025477, 0.050211, 0.175105, 0.176161, -0.015806, 0.253269, -0.094938, 0.311797, -0.238695, 0.414446, 0.106309, -0.172923, -0.448435, 0.099753, 0.189956, -0.153976, 0.474776, -0.182255, 0.262361, -0.332482, 0.149282, -0.132051, 0.202633, -0.30154, 0.327943, -0.025558, -0.408402, 0.096169, 0.116352, 0.071214, 0.075926, -0.37903, 0.080369, -0.312909, 0.044296, 0.019603, 0.17904, 0.060305, -0.029755, -0.281416, 0.080173, 0.452002, -0.385478, -0.189965, -0.441209, -0.329088, 0.197722, -0.176855, 0.389624, -0.220631, -0.421624, -0.196826, 0.322119, -0.048293, -0.457163, 0.016278, 0.165604, -0.432896, 0.092984, -0.243686, 0.016776, 0.180806, -0.059655, -0.340892, -0.401092, 0.265588, 0.381333, -0.470922, 0.156736, 0.292358, -0.215127, 0.09696, -0.342575, 0.449808, -0.0533, -0.035271, 0.095762, 0.073863, -0.378893], "model": "sentence-transformers/all-MiniLM-L6-v2"}	{"dimensions": 384, "method": "sentence-transformers"}	2025-11-16 10:41:31.392005
\.


--
-- Data for Name: processing_jobs; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.processing_jobs (id, job_type, job_name, provider, model, parameters, status, progress_percent, current_step, total_steps, result_data, result_summary, error_message, error_details, retry_count, max_retries, tokens_used, processing_time, cost_estimate, created_at, started_at, completed_at, updated_at, user_id, document_id, parent_job_id) FROM stdin;
\.


--
-- Data for Name: prompt_templates; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.prompt_templates (id, template_key, template_text, category, variables, supports_llm_enhancement, llm_enhancement_prompt, is_active, created_at, updated_at) FROM stdin;
1	experiment_description_single_document	Analysis of '{{ document_title }}' ({{ word_count }} words)	experiment_description	{"word_count": "int", "document_title": "string"}	t	Enhance this experiment description with 2-3 sentences about the document's research domain and potential analytical value. Keep it concise and factual.\n\nDocument metadata:\n- Title: {{ document_title }}\n- Word count: {{ word_count }}\n{% if year %}- Year: {{ year }}{% endif %}\n{% if authors %}- Authors: {{ authors }}{% endif %}\n\nCurrent description: {{ template_output }}\n\nEnhanced description:	t	2025-11-08 14:47:25.128976	2025-11-08 14:47:25.128978
2	experiment_description_multi_document	Comparative analysis of {{ document_count }} documents ({{ total_words }} words total){% if domain %} in {{ domain }} domain{% endif %}	experiment_description	{"domain": "string", "total_words": "int", "document_count": "int"}	t	Enhance this experiment description by suggesting 2-3 potential research questions or analytical approaches for comparing these documents.\n\nExperiment metadata:\n- Document count: {{ document_count }}\n- Total words: {{ total_words }}\n{% if domain %}- Domain: {{ domain }}{% endif %}\n\nCurrent description: {{ template_output }}\n\nEnhanced description:	t	2025-11-08 14:47:25.130103	2025-11-08 14:47:25.130105
3	experiment_description_temporal	Tracking evolution of term '{{ term_text }}' across {{ document_count }} documents from {{ earliest_year }} to {{ latest_year }}	experiment_description	{"term_text": "string", "latest_year": "int", "earliest_year": "int", "document_count": "int"}	t	Enhance this temporal evolution description with insights about potential semantic shifts or contextual changes to expect during this time period.\n\nExperiment metadata:\n- Term: {{ term_text }}\n- Document count: {{ document_count }}\n- Time range: {{ earliest_year }} to {{ latest_year }}\n\nCurrent description: {{ template_output }}\n\nEnhanced description:	t	2025-11-08 14:47:25.13064	2025-11-08 14:47:25.130641
4	experiment_description_domain_comparison	Comparing usage of term '{{ term_text }}' across {{ domain_count }} domains: {{ domain_list }}	experiment_description	{"term_text": "string", "domain_list": "string", "domain_count": "int"}	t	Enhance this domain comparison description by highlighting potential differences in how the term might be understood across these domains.\n\nExperiment metadata:\n- Term: {{ term_text }}\n- Domain count: {{ domain_count }}\n- Domains: {{ domain_list }}\n\nCurrent description: {{ template_output }}\n\nEnhanced description:	t	2025-11-08 14:47:25.131422	2025-11-08 14:47:25.131423
5	analysis_summary	Processed {{ segment_count }} segments, generated {{ embedding_count }} embeddings using {{ model_name }}	analysis_summary	{"model_name": "string", "segment_count": "int", "embedding_count": "int"}	f	\N	t	2025-11-08 14:47:25.132053	2025-11-08 14:47:25.132055
\.


--
-- Data for Name: prov_activities; Type: TABLE DATA; Schema: public; Owner: ontextract_user
--

COPY public.prov_activities (activity_id, activity_type, startedattime, endedattime, wasassociatedwith, activity_parameters, activity_status, activity_metadata, created_at) FROM stdin;
f2c09f35-1f73-4399-84a1-c8693598f816	document_upload	2025-11-09 18:23:05.292552-05	2025-11-09 18:23:05.292552-05	d8e50bdf-4459-4afc-b7f2-b9b4f5c4947c	{"filename": "Jensen - Theory of the Firm Managerial Behavior, Agency Costs and Ownership Structure.pdf", "file_type": "pdf", "document_id": 161, "content_type": "file", "document_type": "document", "document_uuid": "6ddc7c77-35cb-4839-91ab-d2dbfdec82b9", "experiment_id": null}	completed	{}	2025-11-09 18:23:05.307617-05
7a348eab-40a6-45ba-8c66-7d791a6bc120	text_extraction	2025-11-09 18:23:05.312887-05	2025-11-09 18:23:05.312888-05	0c0d0502-08a0-4c9b-9753-9aeadc1ec480	{"document_id": 161, "text_length": 173969, "document_uuid": "6ddc7c77-35cb-4839-91ab-d2dbfdec82b9", "source_format": "pdf", "extraction_method": "pypdf"}	completed	{}	2025-11-09 18:23:05.314314-05
8c38035e-2bd3-4375-9506-c5299ae3b441	document_save	2025-11-09 18:23:05.292552-05	2025-11-09 18:23:05.292552-05	d8e50bdf-4459-4afc-b7f2-b9b4f5c4947c	{"database": "ontextract_db", "document_id": 161, "document_uuid": "6ddc7c77-35cb-4839-91ab-d2dbfdec82b9"}	completed	{}	2025-11-09 18:23:05.325655-05
bba9caec-704e-4b3f-8bdb-87b7a7eba816	document_upload	2025-11-09 18:30:55.823332-05	2025-11-09 18:30:55.823332-05	d8e50bdf-4459-4afc-b7f2-b9b4f5c4947c	{"filename": "computer_sciencewooldridge_jennings_1995.pdf", "file_type": "pdf", "document_id": 162, "content_type": "file", "document_type": "document", "document_uuid": "7214d47a-bc4d-48bf-8f81-a170870e2f59", "experiment_id": null}	completed	{}	2025-11-09 18:30:55.835017-05
b22b339c-0993-48d9-b738-4ac0d19d660d	text_extraction	2025-11-09 18:30:55.838989-05	2025-11-09 18:30:55.838991-05	0c0d0502-08a0-4c9b-9753-9aeadc1ec480	{"document_id": 162, "text_length": 163745, "document_uuid": "7214d47a-bc4d-48bf-8f81-a170870e2f59", "source_format": "pdf", "extraction_method": "pypdf"}	completed	{}	2025-11-09 18:30:55.84003-05
6ccbecda-7869-49e5-99a2-a5f98452fc02	metadata_extraction	2025-11-09 18:30:55.846935-05	2025-11-09 18:30:55.846936-05	2e469066-06c7-4748-8bf4-f3a1654f3791	{"confidence": 31.327435, "document_id": 162, "fields_extracted": ["abstract", "doi", "journal", "match_score", "publication_year", "publisher", "raw_date", "title", "type", "url"], "extraction_source": "crossref"}	completed	{}	2025-11-09 18:30:55.847936-05
a47d20b9-a106-4069-a161-5f0c50fc81c7	document_save	2025-11-09 18:30:55.823332-05	2025-11-09 18:30:55.823332-05	d8e50bdf-4459-4afc-b7f2-b9b4f5c4947c	{"database": "ontextract_db", "document_id": 162, "document_uuid": "7214d47a-bc4d-48bf-8f81-a170870e2f59"}	completed	{}	2025-11-09 18:30:55.851934-05
48376c76-5d6d-4dda-9fb7-d53ab8ca0e69	document_upload	2025-11-09 23:16:39.43617-05	2025-11-09 23:16:39.43617-05	d8e50bdf-4459-4afc-b7f2-b9b4f5c4947c	{"filename": "36794-Article Text-40869-1-2-20251015.pdf", "file_type": "pdf", "document_id": 166, "content_type": "file", "document_type": "document", "document_uuid": "a89efab3-c1a9-41ea-bb9b-fa3f81f08561", "experiment_id": null}	completed	{}	2025-11-09 23:16:39.445494-05
b6cbfca6-f6bc-4d3f-b201-b9e2a91471bc	text_extraction	2025-11-09 23:16:39.450064-05	2025-11-09 23:16:39.450066-05	0c0d0502-08a0-4c9b-9753-9aeadc1ec480	{"document_id": 166, "text_length": 11775, "document_uuid": "a89efab3-c1a9-41ea-bb9b-fa3f81f08561", "source_format": "pdf", "extraction_method": "pypdf"}	completed	{}	2025-11-09 23:16:39.450977-05
21a42d17-b060-478e-965c-273860ef9307	metadata_extraction	2025-11-09 23:16:39.456752-05	2025-11-09 23:16:39.456754-05	2e469066-06c7-4748-8bf4-f3a1654f3791	{"confidence": 53.16723, "document_id": 166, "fields_extracted": ["abstract", "doi", "journal", "match_score", "publication_year", "publisher", "raw_date", "title", "type", "url"], "extraction_source": "crossref"}	completed	{}	2025-11-09 23:16:39.457609-05
acaaa4c5-a117-43fb-a5e2-6f27a2e0e2bf	document_save	2025-11-09 23:16:39.43617-05	2025-11-09 23:16:39.43617-05	d8e50bdf-4459-4afc-b7f2-b9b4f5c4947c	{"database": "ontextract_db", "document_id": 166, "document_uuid": "a89efab3-c1a9-41ea-bb9b-fa3f81f08561"}	completed	{}	2025-11-09 23:16:39.461998-05
1cdc2cfc-476b-4923-9c7b-6c0628a05e58	metadata_extraction	2025-11-09 18:23:05.319413-05	2025-11-09 18:23:05.319414-05	2e469066-06c7-4748-8bf4-f3a1654f3791	{"confidence": 27.355968, "document_id": 161, "fields_extracted": ["doi", "journal", "match_score", "publication_year", "publisher", "raw_date", "title", "type", "url"], "extraction_source": "crossref"}	completed	{}	2025-11-09 18:23:05.320749-05
0f8d0eb0-ea1e-410d-96aa-8dc864fd0092	document_upload	2025-11-09 19:07:17.073413-05	2025-11-09 19:07:17.073413-05	d8e50bdf-4459-4afc-b7f2-b9b4f5c4947c	{"filename": "36794-Article Text-40869-1-2-20251015.pdf", "file_type": "pdf", "document_id": 164, "content_type": "file", "document_type": "document", "document_uuid": "717f06b2-e907-4a3b-aa02-23f383d5a65a", "experiment_id": null}	completed	{}	2025-11-09 19:07:17.083034-05
c957a34f-0fd7-4bc2-8083-86692c94e005	text_extraction	2025-11-09 19:07:17.086688-05	2025-11-09 19:07:17.086689-05	0c0d0502-08a0-4c9b-9753-9aeadc1ec480	{"document_id": 164, "text_length": 11775, "document_uuid": "717f06b2-e907-4a3b-aa02-23f383d5a65a", "source_format": "pdf", "extraction_method": "pypdf"}	completed	{}	2025-11-09 19:07:17.087458-05
3273c70c-f9f7-4756-b116-de2458b6074c	metadata_extraction	2025-11-09 19:07:17.092365-05	2025-11-09 19:07:17.092367-05	2e469066-06c7-4748-8bf4-f3a1654f3791	{"confidence": 53.166306, "document_id": 164, "fields_extracted": ["abstract", "doi", "journal", "match_score", "publisher", "raw_date", "title", "type", "url"], "extraction_source": "crossref"}	completed	{}	2025-11-09 19:07:17.093173-05
2c424140-efb0-4c1c-929a-04605612ef02	document_save	2025-11-09 19:07:17.073413-05	2025-11-09 19:07:17.073413-05	d8e50bdf-4459-4afc-b7f2-b9b4f5c4947c	{"database": "ontextract_db", "document_id": 164, "document_uuid": "717f06b2-e907-4a3b-aa02-23f383d5a65a"}	completed	{}	2025-11-09 19:07:17.098645-05
0a2d1564-0112-4d35-a010-712b1c6b8dfc	document_upload	2025-11-10 11:48:06.218942-05	2025-11-10 11:48:06.218942-05	d8e50bdf-4459-4afc-b7f2-b9b4f5c4947c	{"filename": "36794-Article Text-40869-1-2-20251015.pdf", "file_type": "pdf", "document_id": 168, "content_type": "file", "document_type": "document", "document_uuid": "fc96e932-ccbe-4288-b94a-f7cebb5416b3", "experiment_id": null}	completed	{}	2025-11-10 11:48:06.228953-05
b6d38c44-0c85-4348-bf60-e64663f176db	text_extraction	2025-11-10 11:48:06.233516-05	2025-11-10 11:48:06.233518-05	0c0d0502-08a0-4c9b-9753-9aeadc1ec480	{"document_id": 168, "text_length": 11775, "document_uuid": "fc96e932-ccbe-4288-b94a-f7cebb5416b3", "source_format": "pdf", "extraction_method": "pypdf"}	completed	{}	2025-11-10 11:48:06.234181-05
8721b463-f588-4a70-a668-a063089ac55d	metadata_extraction	2025-11-10 11:48:06.238505-05	2025-11-10 11:48:06.238507-05	2e469066-06c7-4748-8bf4-f3a1654f3791	{"confidence": 53.186043, "document_id": 168, "fields_extracted": ["abstract", "doi", "journal", "match_score", "publication_year", "publisher", "raw_date", "title", "type", "url"], "extraction_source": "crossref"}	completed	{}	2025-11-10 11:48:06.239092-05
896dd37a-8ca3-4e40-a8dc-aaa3dd1291c3	document_save	2025-11-10 11:48:06.218942-05	2025-11-10 11:48:06.218942-05	d8e50bdf-4459-4afc-b7f2-b9b4f5c4947c	{"database": "ontextract_db", "document_id": 168, "document_uuid": "fc96e932-ccbe-4288-b94a-f7cebb5416b3"}	completed	{}	2025-11-10 11:48:06.245381-05
1856e76c-ae7b-4141-af4c-5dde95b764a5	document_upload	2025-11-10 11:49:55.998258-05	2025-11-10 11:49:55.998258-05	d8e50bdf-4459-4afc-b7f2-b9b4f5c4947c	{"filename": "36794-Article Text-40869-1-2-20251015.pdf", "file_type": "pdf", "document_id": 170, "content_type": "file", "document_type": "document", "document_uuid": "026bfcdf-37ac-431b-8f5e-9cf716404ada", "experiment_id": null}	completed	{}	2025-11-10 11:49:56.005302-05
2516d5ae-955c-46d7-b033-a28fa4048f5c	text_extraction	2025-11-10 11:49:56.008387-05	2025-11-10 11:49:56.008389-05	0c0d0502-08a0-4c9b-9753-9aeadc1ec480	{"document_id": 170, "text_length": 11775, "document_uuid": "026bfcdf-37ac-431b-8f5e-9cf716404ada", "source_format": "pdf", "extraction_method": "pypdf"}	completed	{}	2025-11-10 11:49:56.009091-05
6f0153ac-5b01-4e5c-bba2-db99f5ef9f76	metadata_extraction	2025-11-10 11:49:56.01204-05	2025-11-10 11:49:56.012041-05	2e469066-06c7-4748-8bf4-f3a1654f3791	{"confidence": 53.186043, "document_id": 170, "fields_extracted": ["abstract", "doi", "journal", "match_score", "publication_year", "publisher", "raw_date", "title", "type", "url"], "extraction_source": "crossref"}	completed	{}	2025-11-10 11:49:56.012667-05
4a6ff576-2d24-4355-95f7-0c35df74e690	document_save	2025-11-10 11:49:55.998258-05	2025-11-10 11:49:55.998258-05	d8e50bdf-4459-4afc-b7f2-b9b4f5c4947c	{"database": "ontextract_db", "document_id": 170, "document_uuid": "026bfcdf-37ac-431b-8f5e-9cf716404ada"}	completed	{}	2025-11-10 11:49:56.016788-05
\.


--
-- Data for Name: prov_agents; Type: TABLE DATA; Schema: public; Owner: ontextract_user
--

COPY public.prov_agents (agent_id, agent_type, foaf_name, foaf_givenname, foaf_mbox, foaf_homepage, agent_metadata, created_at, updated_at) FROM stdin;
d8e50bdf-4459-4afc-b7f2-b9b4f5c4947c	Person	researcher:1	\N	\N	\N	{"username": "chris"}	2025-11-09 18:23:05.303292-05	2025-11-09 18:23:05.303294-05
0c0d0502-08a0-4c9b-9753-9aeadc1ec480	SoftwareAgent	system	\N	\N	\N	{"tool_type": "system", "description": "OntExtract system agent"}	2025-11-09 18:23:05.312018-05	2025-11-09 18:23:05.312019-05
2e469066-06c7-4748-8bf4-f3a1654f3791	SoftwareAgent	crossref_api	\N	\N	\N	{"url": "https://api.crossref.org", "tool_type": "metadata_api", "description": "CrossRef API metadata extraction"}	2025-11-09 18:23:05.319201-05	2025-11-09 18:23:05.319202-05
\.


--
-- Data for Name: prov_entities; Type: TABLE DATA; Schema: public; Owner: ontextract_user
--

COPY public.prov_entities (entity_id, entity_type, generatedattime, invalidatedattime, wasgeneratedby, wasattributedto, wasderivedfrom, entity_value, entity_metadata, character_start, character_end, created_at) FROM stdin;
c817a304-e6f4-4808-94f4-ee034f86f5b4	document	2025-11-09 18:23:05.292552-05	\N	f2c09f35-1f73-4399-84a1-c8693598f816	d8e50bdf-4459-4afc-b7f2-b9b4f5c4947c	\N	{"title": "Theory of the Firm: Managerial Behavior, Agency Costs, and Ownership Structure", "filename": "Jensen - Theory of the Firm Managerial Behavior, Agency Costs and Ownership Structure.pdf", "word_count": 27964, "document_id": 161, "content_type": "file", "document_type": "document", "document_uuid": "6ddc7c77-35cb-4839-91ab-d2dbfdec82b9", "character_count": 173969}	{}	\N	\N	2025-11-09 18:23:05.309665-05
99475e3e-871d-4fa6-afc5-c6b4f469e593	text_content	2025-11-09 18:23:05.315753-05	\N	7a348eab-40a6-45ba-8c66-7d791a6bc120	0c0d0502-08a0-4c9b-9753-9aeadc1ec480	c817a304-e6f4-4808-94f4-ee034f86f5b4	{"word_count": 27964, "document_id": 161, "text_length": 173969, "document_uuid": "6ddc7c77-35cb-4839-91ab-d2dbfdec82b9"}	{}	\N	\N	2025-11-09 18:23:05.316736-05
911e9b23-10ae-4b99-85d6-e0885f5c512c	metadata	2025-11-09 18:23:05.321845-05	\N	1cdc2cfc-476b-4923-9c7b-6c0628a05e58	2e469066-06c7-4748-8bf4-f3a1654f3791	c817a304-e6f4-4808-94f4-ee034f86f5b4	{"confidence": 27.355968, "document_id": 161, "document_uuid": "6ddc7c77-35cb-4839-91ab-d2dbfdec82b9", "extraction_source": "crossref", "extracted_metadata": {"doi": "10.1007/978-94-009-9257-3_8", "url": "https://doi.org/10.1007/978-94-009-9257-3_8", "type": "book-chapter", "title": "Theory of the Firm: Managerial Behavior, Agency Costs, and Ownership Structure", "journal": "Rochester Studies in Economics and Policy Issues", "raw_date": "1979", "publisher": "Springer Netherlands", "match_score": 27.355968, "publication_year": 1979}}	{}	\N	\N	2025-11-09 18:23:05.322014-05
27c152a6-0e4d-4042-84e1-c10c31a78cd8	document_version	2025-11-09 18:23:05.292552-05	\N	8c38035e-2bd3-4375-9506-c5299ae3b441	d8e50bdf-4459-4afc-b7f2-b9b4f5c4947c	99475e3e-871d-4fa6-afc5-c6b4f469e593	{"status": "uploaded", "version": 1, "document_id": 161, "document_uuid": "6ddc7c77-35cb-4839-91ab-d2dbfdec82b9"}	{}	\N	\N	2025-11-09 18:23:05.326755-05
3163d191-0325-4dec-b350-cf79a9a28b1e	document	2025-11-09 18:30:55.823332-05	\N	bba9caec-704e-4b3f-8bdb-87b7a7eba816	d8e50bdf-4459-4afc-b7f2-b9b4f5c4947c	\N	{"title": "Intelligent agents: theory and practice", "filename": "computer_sciencewooldridge_jennings_1995.pdf", "word_count": 24677, "document_id": 162, "content_type": "file", "document_type": "document", "document_uuid": "7214d47a-bc4d-48bf-8f81-a170870e2f59", "character_count": 163745}	{}	\N	\N	2025-11-09 18:30:55.836161-05
8d2b2df5-e14a-4543-8f4c-492116486018	text_content	2025-11-09 18:30:55.841964-05	\N	b22b339c-0993-48d9-b738-4ac0d19d660d	0c0d0502-08a0-4c9b-9753-9aeadc1ec480	3163d191-0325-4dec-b350-cf79a9a28b1e	{"word_count": 24677, "document_id": 162, "text_length": 163745, "document_uuid": "7214d47a-bc4d-48bf-8f81-a170870e2f59"}	{}	\N	\N	2025-11-09 18:30:55.842928-05
0efeac84-3c26-4564-a344-e3fde3d9172f	metadata	2025-11-09 18:30:55.84867-05	\N	6ccbecda-7869-49e5-99a2-a5f98452fc02	2e469066-06c7-4748-8bf4-f3a1654f3791	3163d191-0325-4dec-b350-cf79a9a28b1e	{"confidence": 31.327435, "document_id": 162, "document_uuid": "7214d47a-bc4d-48bf-8f81-a170870e2f59", "extraction_source": "crossref", "extracted_metadata": {"doi": "10.1017/s0269888900008122", "url": "https://doi.org/10.1017/s0269888900008122", "type": "journal-article", "title": "Intelligent agents: theory and practice", "journal": "The Knowledge Engineering Review", "abstract": "<jats:title>Abstract</jats:title><jats:p>The concept of an<jats:italic>agent</jats:italic>has become important in both artificial intelligence (AT) and mainstream computer science. Our aim in this paper is to point the reader at what we perceive to be the most important theoretical and practical issues associated with the design and construction of intelligent agents. For convenience, we divide these issues into three areas (though as the reader will see, the divisions are at times somewhat arbitrary).<jats:italic>Agent theory</jats:italic>is concerned with the question of what an agent is, and the use of mathematical formalisms for representing and reasoning about the properties of agents.<jats:italic>Agent architectures</jats:italic>can be thought of as software engineering models of agents; researchers in this area are primarily concerned with the problem of designing software or hardware systems that will satisfy the properties specified by agent theorists. Finally,<jats:italic>agent languages</jats:italic>are software systems for programming and experimenting with agents; these languages may embody principles proposed by theorists. The paper is<jats:italic>not</jats:italic>intended to serve as a tutorial introduction to all the issues mentioned; we hope instead simply to identify the most important issues, and point to work that elaborates on them. The article includes a short review of current and potential applications of agent technology.</jats:p>", "raw_date": "1995-06", "publisher": "Cambridge University Press (CUP)", "match_score": 31.327435, "publication_year": 1995}}	{}	\N	\N	2025-11-09 18:30:55.848848-05
214f3a30-b922-4b07-a26d-6cfee39dcab6	document_version	2025-11-09 18:30:55.823332-05	\N	a47d20b9-a106-4069-a161-5f0c50fc81c7	d8e50bdf-4459-4afc-b7f2-b9b4f5c4947c	8d2b2df5-e14a-4543-8f4c-492116486018	{"status": "uploaded", "version": 1, "document_id": 162, "document_uuid": "7214d47a-bc4d-48bf-8f81-a170870e2f59"}	{}	\N	\N	2025-11-09 18:30:55.852856-05
26ab1091-a50f-4bcc-9f19-6d2be34db018	document	2025-11-09 19:07:17.073413-05	\N	0f8d0eb0-ea1e-410d-96aa-8dc864fd0092	d8e50bdf-4459-4afc-b7f2-b9b4f5c4947c	\N	{"title": "Precedent-Based Professional Role Ethics for AI Decision Analysis", "filename": "36794-Article Text-40869-1-2-20251015.pdf", "word_count": 1621, "document_id": 164, "content_type": "file", "document_type": "document", "document_uuid": "717f06b2-e907-4a3b-aa02-23f383d5a65a", "character_count": 11775}	{}	\N	\N	2025-11-09 19:07:17.084131-05
e9631280-4777-4cec-a64a-31f64392c371	text_content	2025-11-09 19:07:17.089156-05	\N	c957a34f-0fd7-4bc2-8083-86692c94e005	0c0d0502-08a0-4c9b-9753-9aeadc1ec480	26ab1091-a50f-4bcc-9f19-6d2be34db018	{"word_count": 1621, "document_id": 164, "text_length": 11775, "document_uuid": "717f06b2-e907-4a3b-aa02-23f383d5a65a"}	{}	\N	\N	2025-11-09 19:07:17.089441-05
e75bdc7e-7cdb-49d0-b97e-184a696a923c	metadata	2025-11-09 19:07:17.094096-05	\N	3273c70c-f9f7-4756-b116-de2458b6074c	2e469066-06c7-4748-8bf4-f3a1654f3791	26ab1091-a50f-4bcc-9f19-6d2be34db018	{"confidence": 53.166306, "document_id": 164, "document_uuid": "717f06b2-e907-4a3b-aa02-23f383d5a65a", "extraction_source": "crossref", "extracted_metadata": {"doi": "10.1609/aies.v8i3.36794", "url": "https://doi.org/10.1609/aies.v8i3.36794", "type": "journal-article", "title": "Precedent-Based Professional Role Ethics for AI Decision Analysis", "journal": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society", "abstract": "<jats:p>Large language models (LLMs) are increasingly being used in professional fields such as healthcare, law, and engineering. In these domains, errors can lead to serious consequences. Many current AI ethics approaches do not reflect the structured codes and precedent-informed reasoning that guide professional conduct. This work introduces ProEthica, a system under development that combines LLMs with role-based ontologies to support structured ethical reasoning in professional settings. The system draws on principles from professional role ethics and incorporates ethical guidelines, practice standards, and prior case decisions. As a demonstration case, it applies the National Society of Professional Engineers (NSPE) Code of Ethics and Board of Ethical Review precedents. ProEthica includes an ontology based on engineering  ethics, a precedent retrieval method using both vector similarity and ontological mappings, a framework for guiding and checking LLM outputs with structured constraints, and a  validation process modeled on FIRAC (Facts, Issues, Rules, Analysis, Conclusion) reasoning. The system is intended to help professionals make ethical decisions that are consistent with established standards, not to replace human judgment. Preliminary evaluations using NSPE cases indicate that it can retrieve relevant precedents and produce structured analyses that align with engineering ethics.</jats:p>", "raw_date": "2025-10-15", "publisher": "Association for the Advancement of Artificial Intelligence (AAAI)", "match_score": 53.166306}}	{}	\N	\N	2025-11-09 19:07:17.094289-05
c4220ce0-7f70-4511-8bbc-3d836f3b2a80	document_version	2025-11-09 19:07:17.073413-05	\N	2c424140-efb0-4c1c-929a-04605612ef02	d8e50bdf-4459-4afc-b7f2-b9b4f5c4947c	e9631280-4777-4cec-a64a-31f64392c371	{"status": "uploaded", "version": 1, "document_id": 164, "document_uuid": "717f06b2-e907-4a3b-aa02-23f383d5a65a"}	{}	\N	\N	2025-11-09 19:07:17.099544-05
00ff1b25-650a-492c-9731-40fc87325c79	document	2025-11-09 23:16:39.43617-05	\N	48376c76-5d6d-4dda-9fb7-d53ab8ca0e69	d8e50bdf-4459-4afc-b7f2-b9b4f5c4947c	\N	{"title": "Precedent-Based Professional Role Ethics for AI Decision Analysis", "filename": "36794-Article Text-40869-1-2-20251015.pdf", "word_count": 1621, "document_id": 166, "content_type": "file", "document_type": "document", "document_uuid": "a89efab3-c1a9-41ea-bb9b-fa3f81f08561", "character_count": 11775}	{}	\N	\N	2025-11-09 23:16:39.447074-05
0630869c-39dd-4f7f-894d-b950e39231d6	text_content	2025-11-09 23:16:39.452963-05	\N	b6cbfca6-f6bc-4d3f-b201-b9e2a91471bc	0c0d0502-08a0-4c9b-9753-9aeadc1ec480	00ff1b25-650a-492c-9731-40fc87325c79	{"word_count": 1621, "document_id": 166, "text_length": 11775, "document_uuid": "a89efab3-c1a9-41ea-bb9b-fa3f81f08561"}	{}	\N	\N	2025-11-09 23:16:39.453302-05
30bfdca3-3762-4e94-af4c-26337c03d798	metadata	2025-11-09 23:16:39.458487-05	\N	21a42d17-b060-478e-965c-273860ef9307	2e469066-06c7-4748-8bf4-f3a1654f3791	00ff1b25-650a-492c-9731-40fc87325c79	{"confidence": 53.16723, "document_id": 166, "document_uuid": "a89efab3-c1a9-41ea-bb9b-fa3f81f08561", "extraction_source": "crossref", "extracted_metadata": {"doi": "10.1609/aies.v8i3.36794", "url": "https://doi.org/10.1609/aies.v8i3.36794", "type": "journal-article", "title": "Precedent-Based Professional Role Ethics for AI Decision Analysis", "journal": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society", "abstract": "<jats:p>Large language models (LLMs) are increasingly being used in professional fields such as healthcare, law, and engineering. In these domains, errors can lead to serious consequences. Many current AI ethics approaches do not reflect the structured codes and precedent-informed reasoning that guide professional conduct. This work introduces ProEthica, a system under development that combines LLMs with role-based ontologies to support structured ethical reasoning in professional settings. The system draws on principles from professional role ethics and incorporates ethical guidelines, practice standards, and prior case decisions. As a demonstration case, it applies the National Society of Professional Engineers (NSPE) Code of Ethics and Board of Ethical Review precedents. ProEthica includes an ontology based on engineering  ethics, a precedent retrieval method using both vector similarity and ontological mappings, a framework for guiding and checking LLM outputs with structured constraints, and a  validation process modeled on FIRAC (Facts, Issues, Rules, Analysis, Conclusion) reasoning. The system is intended to help professionals make ethical decisions that are consistent with established standards, not to replace human judgment. Preliminary evaluations using NSPE cases indicate that it can retrieve relevant precedents and produce structured analyses that align with engineering ethics.</jats:p>", "raw_date": "2025-10-15", "publisher": "Association for the Advancement of Artificial Intelligence (AAAI)", "match_score": 53.16723, "publication_year": 2025}}	{}	\N	\N	2025-11-09 23:16:39.45869-05
dcee3753-72a4-4dcf-8e12-29f491ba832d	document_version	2025-11-09 23:16:39.43617-05	\N	acaaa4c5-a117-43fb-a5e2-6f27a2e0e2bf	d8e50bdf-4459-4afc-b7f2-b9b4f5c4947c	0630869c-39dd-4f7f-894d-b950e39231d6	{"status": "uploaded", "version": 1, "document_id": 166, "document_uuid": "a89efab3-c1a9-41ea-bb9b-fa3f81f08561"}	{}	\N	\N	2025-11-09 23:16:39.462906-05
195e4c83-ab38-42d8-a8ac-a7f4a66e1f80	document	2025-11-10 11:48:06.218942-05	\N	0a2d1564-0112-4d35-a010-712b1c6b8dfc	d8e50bdf-4459-4afc-b7f2-b9b4f5c4947c	\N	{"title": "Precedent-Based Professional Role Ethics for AI Decision Analysis", "filename": "36794-Article Text-40869-1-2-20251015.pdf", "word_count": 1621, "document_id": 168, "content_type": "file", "document_type": "document", "document_uuid": "fc96e932-ccbe-4288-b94a-f7cebb5416b3", "character_count": 11775}	{}	\N	\N	2025-11-10 11:48:06.230325-05
79d9ea45-557b-4a57-955c-b9f598771638	text_content	2025-11-10 11:48:06.235494-05	\N	b6d38c44-0c85-4348-bf60-e64663f176db	0c0d0502-08a0-4c9b-9753-9aeadc1ec480	195e4c83-ab38-42d8-a8ac-a7f4a66e1f80	{"word_count": 1621, "document_id": 168, "text_length": 11775, "document_uuid": "fc96e932-ccbe-4288-b94a-f7cebb5416b3"}	{}	\N	\N	2025-11-10 11:48:06.235781-05
832c67e9-d9e5-44d7-aaac-7fecd09b8ae5	metadata	2025-11-10 11:48:06.239906-05	\N	8721b463-f588-4a70-a668-a063089ac55d	2e469066-06c7-4748-8bf4-f3a1654f3791	195e4c83-ab38-42d8-a8ac-a7f4a66e1f80	{"confidence": 53.186043, "document_id": 168, "document_uuid": "fc96e932-ccbe-4288-b94a-f7cebb5416b3", "extraction_source": "crossref", "extracted_metadata": {"doi": "10.1609/aies.v8i3.36794", "url": "https://doi.org/10.1609/aies.v8i3.36794", "type": "journal-article", "title": "Precedent-Based Professional Role Ethics for AI Decision Analysis", "journal": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society", "abstract": "<jats:p>Large language models (LLMs) are increasingly being used in professional fields such as healthcare, law, and engineering. In these domains, errors can lead to serious consequences. Many current AI ethics approaches do not reflect the structured codes and precedent-informed reasoning that guide professional conduct. This work introduces ProEthica, a system under development that combines LLMs with role-based ontologies to support structured ethical reasoning in professional settings. The system draws on principles from professional role ethics and incorporates ethical guidelines, practice standards, and prior case decisions. As a demonstration case, it applies the National Society of Professional Engineers (NSPE) Code of Ethics and Board of Ethical Review precedents. ProEthica includes an ontology based on engineering  ethics, a precedent retrieval method using both vector similarity and ontological mappings, a framework for guiding and checking LLM outputs with structured constraints, and a  validation process modeled on FIRAC (Facts, Issues, Rules, Analysis, Conclusion) reasoning. The system is intended to help professionals make ethical decisions that are consistent with established standards, not to replace human judgment. Preliminary evaluations using NSPE cases indicate that it can retrieve relevant precedents and produce structured analyses that align with engineering ethics.</jats:p>", "raw_date": "2025-10-15", "publisher": "Association for the Advancement of Artificial Intelligence (AAAI)", "match_score": 53.186043, "publication_year": 2025}}	{}	\N	\N	2025-11-10 11:48:06.240084-05
ed91af54-cbab-4fa7-a07d-035076b31593	document_version	2025-11-10 11:48:06.218942-05	\N	896dd37a-8ca3-4e40-a8dc-aaa3dd1291c3	d8e50bdf-4459-4afc-b7f2-b9b4f5c4947c	79d9ea45-557b-4a57-955c-b9f598771638	{"status": "uploaded", "version": 1, "document_id": 168, "document_uuid": "fc96e932-ccbe-4288-b94a-f7cebb5416b3"}	{}	\N	\N	2025-11-10 11:48:06.246259-05
a7b8e651-fd05-46be-a235-3355aacb492d	document	2025-11-10 11:49:55.998258-05	\N	1856e76c-ae7b-4141-af4c-5dde95b764a5	d8e50bdf-4459-4afc-b7f2-b9b4f5c4947c	\N	{"title": "Precedent-Based Professional Role Ethics for AI Decision Analysis", "filename": "36794-Article Text-40869-1-2-20251015.pdf", "word_count": 1621, "document_id": 170, "content_type": "file", "document_type": "document", "document_uuid": "026bfcdf-37ac-431b-8f5e-9cf716404ada", "character_count": 11775}	{}	\N	\N	2025-11-10 11:49:56.006036-05
0d5cdcbb-af99-4f7c-ae46-3881694e156b	text_content	2025-11-10 11:49:56.009915-05	\N	2516d5ae-955c-46d7-b033-a28fa4048f5c	0c0d0502-08a0-4c9b-9753-9aeadc1ec480	a7b8e651-fd05-46be-a235-3355aacb492d	{"word_count": 1621, "document_id": 170, "text_length": 11775, "document_uuid": "026bfcdf-37ac-431b-8f5e-9cf716404ada"}	{}	\N	\N	2025-11-10 11:49:56.010182-05
4ab4fc6e-3a7a-4e85-9829-b6cb86a05bab	metadata	2025-11-10 11:49:56.013406-05	\N	6f0153ac-5b01-4e5c-bba2-db99f5ef9f76	2e469066-06c7-4748-8bf4-f3a1654f3791	a7b8e651-fd05-46be-a235-3355aacb492d	{"confidence": 53.186043, "document_id": 170, "document_uuid": "026bfcdf-37ac-431b-8f5e-9cf716404ada", "extraction_source": "crossref", "extracted_metadata": {"doi": "10.1609/aies.v8i3.36794", "url": "https://doi.org/10.1609/aies.v8i3.36794", "type": "journal-article", "title": "Precedent-Based Professional Role Ethics for AI Decision Analysis", "journal": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society", "abstract": "<jats:p>Large language models (LLMs) are increasingly being used in professional fields such as healthcare, law, and engineering. In these domains, errors can lead to serious consequences. Many current AI ethics approaches do not reflect the structured codes and precedent-informed reasoning that guide professional conduct. This work introduces ProEthica, a system under development that combines LLMs with role-based ontologies to support structured ethical reasoning in professional settings. The system draws on principles from professional role ethics and incorporates ethical guidelines, practice standards, and prior case decisions. As a demonstration case, it applies the National Society of Professional Engineers (NSPE) Code of Ethics and Board of Ethical Review precedents. ProEthica includes an ontology based on engineering  ethics, a precedent retrieval method using both vector similarity and ontological mappings, a framework for guiding and checking LLM outputs with structured constraints, and a  validation process modeled on FIRAC (Facts, Issues, Rules, Analysis, Conclusion) reasoning. The system is intended to help professionals make ethical decisions that are consistent with established standards, not to replace human judgment. Preliminary evaluations using NSPE cases indicate that it can retrieve relevant precedents and produce structured analyses that align with engineering ethics.</jats:p>", "raw_date": "2025-10-15", "publisher": "Association for the Advancement of Artificial Intelligence (AAAI)", "match_score": 53.186043, "publication_year": 2025}}	{}	\N	\N	2025-11-10 11:49:56.013622-05
adb884a9-3743-4afd-a2b3-552d8c54c7f8	document_version	2025-11-10 11:49:55.998258-05	\N	4a6ff576-2d24-4355-95f7-0c35df74e690	d8e50bdf-4459-4afc-b7f2-b9b4f5c4947c	0d5cdcbb-af99-4f7c-ae46-3881694e156b	{"status": "uploaded", "version": 1, "document_id": 170, "document_uuid": "026bfcdf-37ac-431b-8f5e-9cf716404ada"}	{}	\N	\N	2025-11-10 11:49:56.017662-05
\.


--
-- Data for Name: prov_relationships; Type: TABLE DATA; Schema: public; Owner: ontextract_user
--

COPY public.prov_relationships (relationship_id, relationship_type, subject_id, subject_type, object_id, object_type, relationship_metadata, created_at) FROM stdin;
e964c7d0-7d87-42ec-950f-93e8d27fddd7	used	7a348eab-40a6-45ba-8c66-7d791a6bc120	Activity	c817a304-e6f4-4808-94f4-ee034f86f5b4	Entity	{}	2025-11-09 18:23:05.317422-05
e6dca634-9cfb-493d-8696-ed40af988f22	used	1cdc2cfc-476b-4923-9c7b-6c0628a05e58	Activity	c817a304-e6f4-4808-94f4-ee034f86f5b4	Entity	{}	2025-11-09 18:23:05.322374-05
66ff49f7-9d78-447a-8114-b31667a86cb6	used	8c38035e-2bd3-4375-9506-c5299ae3b441	Activity	99475e3e-871d-4fa6-afc5-c6b4f469e593	Entity	{}	2025-11-09 18:23:05.327067-05
0f1b44c1-d33d-47bd-b23e-40fcf797f59e	used	b22b339c-0993-48d9-b738-4ac0d19d660d	Activity	3163d191-0325-4dec-b350-cf79a9a28b1e	Entity	{}	2025-11-09 18:30:55.843618-05
3a3a98a3-eae0-42a1-b643-9da8e24db33f	used	6ccbecda-7869-49e5-99a2-a5f98452fc02	Activity	3163d191-0325-4dec-b350-cf79a9a28b1e	Entity	{}	2025-11-09 18:30:55.849169-05
5557f2ed-0aeb-4f73-a4e2-9699003a21e2	used	a47d20b9-a106-4069-a161-5f0c50fc81c7	Activity	8d2b2df5-e14a-4543-8f4c-492116486018	Entity	{}	2025-11-09 18:30:55.853244-05
7b43913f-4c26-4517-a8e3-7b0dcf6a7ea5	used	c957a34f-0fd7-4bc2-8083-86692c94e005	Activity	26ab1091-a50f-4bcc-9f19-6d2be34db018	Entity	{}	2025-11-09 19:07:17.090111-05
743d9cc7-fb16-4f94-a65f-e764bb2487f6	used	3273c70c-f9f7-4756-b116-de2458b6074c	Activity	26ab1091-a50f-4bcc-9f19-6d2be34db018	Entity	{}	2025-11-09 19:07:17.096048-05
2b65c8db-26d4-487f-9ddd-60c7b4df91e1	used	2c424140-efb0-4c1c-929a-04605612ef02	Activity	e9631280-4777-4cec-a64a-31f64392c371	Entity	{}	2025-11-09 19:07:17.09983-05
f215fe26-ea04-4458-865f-43a3f5ec3fe6	used	b6cbfca6-f6bc-4d3f-b201-b9e2a91471bc	Activity	00ff1b25-650a-492c-9731-40fc87325c79	Entity	{}	2025-11-09 23:16:39.454032-05
b9e0af1a-e9f0-45d0-bbd7-9df92a8a8c6d	used	21a42d17-b060-478e-965c-273860ef9307	Activity	00ff1b25-650a-492c-9731-40fc87325c79	Entity	{}	2025-11-09 23:16:39.459244-05
de4ba3ae-1146-485d-ba38-6c792e7eadc8	used	acaaa4c5-a117-43fb-a5e2-6f27a2e0e2bf	Activity	0630869c-39dd-4f7f-894d-b950e39231d6	Entity	{}	2025-11-09 23:16:39.463245-05
29dd676c-babe-4054-b8dd-364bb953cf11	used	b6d38c44-0c85-4348-bf60-e64663f176db	Activity	195e4c83-ab38-42d8-a8ac-a7f4a66e1f80	Entity	{}	2025-11-10 11:48:06.236358-05
11ae6c7f-d35e-4870-88b1-696d2fcdfb24	used	8721b463-f588-4a70-a668-a063089ac55d	Activity	195e4c83-ab38-42d8-a8ac-a7f4a66e1f80	Entity	{}	2025-11-10 11:48:06.240565-05
756a83ba-3e38-47bc-891b-0c836e7a4e88	used	896dd37a-8ca3-4e40-a8dc-aaa3dd1291c3	Activity	79d9ea45-557b-4a57-955c-b9f598771638	Entity	{}	2025-11-10 11:48:06.246547-05
6cdeab62-cd2b-44e0-9bf8-0011dce7d19b	used	2516d5ae-955c-46d7-b033-a28fa4048f5c	Activity	a7b8e651-fd05-46be-a235-3355aacb492d	Entity	{}	2025-11-10 11:49:56.010479-05
d41a69ee-3036-47d3-b5d2-2a71b8e3cedc	used	6f0153ac-5b01-4e5c-bba2-db99f5ef9f76	Activity	a7b8e651-fd05-46be-a235-3355aacb492d	Entity	{}	2025-11-10 11:49:56.01406-05
a170dd49-9b74-42a5-88d2-9314d561b7de	used	4a6ff576-2d24-4355-95f7-0c35df74e690	Activity	0d5cdcbb-af99-4f7c-ae46-3881694e156b	Entity	{}	2025-11-10 11:49:56.017936-05
\.


--
-- Data for Name: provenance_activities; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.provenance_activities (id, prov_id, prov_type, prov_label, started_at_time, ended_at_time, was_associated_with, used_plan, processing_job_id, experiment_id, activity_type, activity_metadata, created_at, updated_at) FROM stdin;
\.


--
-- Data for Name: provenance_chains; Type: TABLE DATA; Schema: public; Owner: ontextract_user
--

COPY public.provenance_chains (id, entity_id, entity_type, was_derived_from, derivation_activity, derivation_metadata, created_at) FROM stdin;
\.


--
-- Data for Name: provenance_entities; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.provenance_entities (id, prov_id, prov_type, prov_label, generated_at_time, invalidated_at_time, attributed_to_agent, derived_from_entity, generated_by_activity, document_id, experiment_id, version_number, version_type, prov_metadata, created_at, updated_at) FROM stdin;
\.


--
-- Data for Name: search_history; Type: TABLE DATA; Schema: public; Owner: ontextract_user
--

COPY public.search_history (id, query, query_type, results_count, execution_time, user_id, ip_address, created_at) FROM stdin;
\.


--
-- Data for Name: semantic_drift_activities; Type: TABLE DATA; Schema: public; Owner: ontextract_user
--

COPY public.semantic_drift_activities (id, activity_type, start_period, end_period, temporal_scope_years, used_entity, generated_entity, was_associated_with, drift_metrics, detection_algorithm, algorithm_parameters, started_at_time, ended_at_time, activity_status, drift_detected, drift_magnitude, drift_type, evidence_summary, created_by, created_at) FROM stdin;
\.


--
-- Data for Name: semantic_shift_analysis; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.semantic_shift_analysis (id, experiment_id, term_id, shift_type, from_period, to_period, from_discipline, to_discipline, description, evidence, from_document_id, to_document_id, from_definition_id, to_definition_id, edge_type, edge_label, detected_by, confidence, created_at) FROM stdin;
\.


--
-- Data for Name: term_disciplinary_definitions; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.term_disciplinary_definitions (id, term_id, experiment_id, discipline, definition, source_text, source_type, period_label, start_year, end_year, key_features, distinguishing_features, parallel_meanings, potential_confusion, document_id, resolution_notes, created_at, updated_at) FROM stdin;
\.


--
-- Data for Name: term_version_anchors; Type: TABLE DATA; Schema: public; Owner: ontextract_user
--

COPY public.term_version_anchors (id, term_version_id, context_anchor_id, similarity_score, rank_in_neighborhood, created_at) FROM stdin;
2dfab605-a8c8-4929-83a7-812681f41798	d8bee452-531c-4475-9488-1fa27d130979	a50cc9e3-f468-4a60-9278-14fc783189e5	\N	\N	\N
3533e396-e6bf-432d-b8df-dd2aa6a994a8	561b0ffd-07f4-4152-8e8b-ec8d02d04104	0f6ed02a-0d5d-4325-91cf-7f8701b4f480	\N	\N	\N
\.


--
-- Data for Name: term_versions; Type: TABLE DATA; Schema: public; Owner: ontextract_user
--

COPY public.term_versions (id, term_id, temporal_period, temporal_start_year, temporal_end_year, meaning_description, context_anchor, original_context_anchor, fuzziness_score, confidence_level, certainty_notes, corpus_source, source_documents, extraction_method, generated_at_time, was_derived_from, derivation_type, version_number, is_current, created_by, created_at, neighborhood_overlap, positional_change, similarity_reduction, source_citation) FROM stdin;
d8bee452-531c-4475-9488-1fa27d130979	b40700de-5f80-4316-ab05-bc1ea6078312	2025	\N	\N	A person who or thing which acts upon someone or something; one who or that which exerts power; the doer	["agent"]	\N	\N	medium	\N	OED	\N	manual	2025-11-06 02:29:38.071852-05	\N	\N	1	t	1	2025-11-06 02:29:38.07261-05	\N	\N	\N	Oxford English Dictionary, s.v. "agent" (entry ID: agent_nn01), Oxford University Press
561b0ffd-07f4-4152-8e8b-ec8d02d04104	92509624-5aad-4759-b8d7-5894ca0d2660	2025	\N	\N	Philosophy. The science or study of being; that branch of metaphysics concerned with the nature or essence of being or	["ontology"]	\N	\N	medium	\N	OED	\N	manual	2025-11-07 10:43:43.901226-05	\N	\N	1	t	1	2025-11-07 10:43:43.902556-05	\N	\N	\N	Oxford English Dictionary, s.v. "ontology" (entry ID: ontology_nn01), Oxford University Press
\.


--
-- Data for Name: terms; Type: TABLE DATA; Schema: public; Owner: ontextract_user
--

COPY public.terms (id, term_text, entry_date, status, created_by, updated_by, created_at, updated_at, description, etymology, notes, research_domain, selection_rationale, historical_significance) FROM stdin;
b40700de-5f80-4316-ab05-bc1ea6078312	agent	2025-11-06 02:29:38.066699-05	active	1	\N	2025-11-06 02:29:38.066701-05	2025-11-06 02:29:38.066701-05	\N	\N		Linguistics	\N	\N
92509624-5aad-4759-b8d7-5894ca0d2660	ontology	2025-11-07 10:43:43.892836-05	active	1	\N	2025-11-07 10:43:43.89284-05	2025-11-07 10:43:43.892841-05	\N	\N		Linguistics	\N	\N
\.


--
-- Data for Name: text_segments; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.text_segments (id, content, segment_type, segment_number, start_position, end_position, parent_segment_id, level, word_count, character_count, sentence_count, language, language_confidence, embedding, embedding_model, processed, processing_notes, topics, keywords, sentiment_score, complexity_score, created_at, updated_at, processed_at, document_id, segmentation_method, segmentation_job_id, processing_method, group_id) FROM stdin;
\.


--
-- Data for Name: tool_execution_logs; Type: TABLE DATA; Schema: public; Owner: ontextract_user
--

COPY public.tool_execution_logs (id, orchestration_decision_id, tool_name, tool_version, execution_order, started_at, completed_at, execution_time_ms, execution_status, output_data, error_message, memory_usage_mb, cpu_usage_percent, output_quality_score) FROM stdin;
\.


--
-- Data for Name: users; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.users (id, username, email, password_hash, first_name, last_name, organization, is_active, is_admin, created_at, updated_at, last_login) FROM stdin;
2	test_user	test@example.com	scrypt:32768:8:1$7co0CsFaL4Ci2PCP$dbfb3c2a20be1aa55e7802d17fdc3ca43ae8fbf01b1602c3fc1a1a8f76106a38a34f945ea282d3b8e1f6fbfc30348a923141a097f9087c961bdf9595466590e9	\N	\N	\N	t	f	2025-08-11 14:05:08.556545	2025-08-11 14:05:08.55655	\N
1	chris	chris@example.com	scrypt:32768:8:1$WswwBUGGZdwwSmYD$415baa68963387f3d779be81dbf2f072e3e5aafab6dd30788759ebc938813396f5f09087baa629ddfa516784618ac33ac8952d29db17867991add0ff101d5b69	\N	\N	\N	t	t	2025-08-11 04:55:29.584732	2025-11-15 15:59:25.347273	2025-11-15 15:59:25.346288
5	demo_researcher	demo@ontextract.example	scrypt:32768:8:1$OhP5gkq9CsA1WFT0$dce3e9783fc98653cca9f6f9f35d1361ce90574a0723fa173771158d2c00ee85db627d503b221f8a1de6abf680deef400e38c36330336e42bfd590498ba25f9b	\N	\N	\N	t	f	2025-09-06 15:59:25.997463	2025-09-06 15:59:25.997466	\N
3	wook	wook@admin.local	scrypt:32768:8:1$TyCbt0YoZdyh6QjK$8651d05519b7732a13c23a115d1c660bf6e8d9b54565e81309f9263b7df5336956abb457801c820bbf9c79bb682ba1a9ee0267bf22d657c356e84ac867d92df6	Wook	Admin	\N	t	t	2025-08-20 09:18:26.897552	2025-08-23 21:26:13.020418	2025-08-20 09:22:56.630383
6	demo	demo@example.com	scrypt:32768:8:1$9jdXG3DOW2iGW8mR$4e6f70e527360e76c380380b07b15acc031d0642a3a0efcca8bcedcbe7e863aaa3853b8c84ffb4c2e2d1a6848cb791b29fae7f0c2bc29489348284420f4b0353	Demo	User	Digital Humanities Research	t	f	2025-09-06 17:03:46.269738	2025-09-06 17:03:46.26974	\N
4	system	system@ontextract.local	pbkdf2:sha256:600000$LfsdHmtQBeqlEewU$541edbd26b7797ee14b4e65cd8e75ac30e2e320bb2dc40cfeb6ce166c86ee088	\N	\N	\N	t	f	2025-09-05 20:47:09.065109	2025-09-05 20:47:09.065112	\N
9	methods_tester	methods_tester@example.com	scrypt:32768:8:1$vEqT03dWkBXRMntu$840e2c0e40dc4b6597da48051b65ccc94d40f16d406f5a900941e36df3cc43f8f498373e215961748b2c6fd17ff7b22ade9436523a56e6548c32ee26dc29749c	\N	\N	\N	t	t	2025-09-21 01:03:52.473327	2025-09-21 01:06:06.457767	2025-09-21 01:06:06.456752
\.


--
-- Data for Name: version_changelog; Type: TABLE DATA; Schema: public; Owner: ontextract_user
--

COPY public.version_changelog (id, document_id, version_number, change_type, change_description, previous_version, created_at, created_by, processing_metadata) FROM stdin;
\.


--
-- Name: app_settings_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.app_settings_id_seq', 21, true);


--
-- Name: document_embeddings_id_seq; Type: SEQUENCE SET; Schema: public; Owner: ontextract_user
--

SELECT pg_catalog.setval('public.document_embeddings_id_seq', 4, true);


--
-- Name: document_processing_summary_id_seq; Type: SEQUENCE SET; Schema: public; Owner: ontextract_user
--

SELECT pg_catalog.setval('public.document_processing_summary_id_seq', 3, true);


--
-- Name: documents_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.documents_id_seq', 178, true);


--
-- Name: domains_id_seq; Type: SEQUENCE SET; Schema: public; Owner: ontextract_user
--

SELECT pg_catalog.setval('public.domains_id_seq', 1, false);


--
-- Name: experiment_documents_v2_id_seq; Type: SEQUENCE SET; Schema: public; Owner: ontextract_user
--

SELECT pg_catalog.setval('public.experiment_documents_v2_id_seq', 62, true);


--
-- Name: experiments_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.experiments_id_seq', 31, true);


--
-- Name: extracted_entities_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.extracted_entities_id_seq', 1, false);


--
-- Name: ontologies_id_seq; Type: SEQUENCE SET; Schema: public; Owner: ontextract_user
--

SELECT pg_catalog.setval('public.ontologies_id_seq', 1, false);


--
-- Name: ontology_entities_id_seq; Type: SEQUENCE SET; Schema: public; Owner: ontextract_user
--

SELECT pg_catalog.setval('public.ontology_entities_id_seq', 1, false);


--
-- Name: ontology_mappings_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.ontology_mappings_id_seq', 1, false);


--
-- Name: ontology_versions_id_seq; Type: SEQUENCE SET; Schema: public; Owner: ontextract_user
--

SELECT pg_catalog.setval('public.ontology_versions_id_seq', 1, false);


--
-- Name: processing_artifact_groups_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.processing_artifact_groups_id_seq', 1, false);


--
-- Name: processing_jobs_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.processing_jobs_id_seq', 63, true);


--
-- Name: prompt_templates_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.prompt_templates_id_seq', 5, true);


--
-- Name: provenance_activities_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.provenance_activities_id_seq', 37, true);


--
-- Name: provenance_entities_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.provenance_entities_id_seq', 37, true);


--
-- Name: search_history_id_seq; Type: SEQUENCE SET; Schema: public; Owner: ontextract_user
--

SELECT pg_catalog.setval('public.search_history_id_seq', 1, false);


--
-- Name: text_segments_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.text_segments_id_seq', 483, true);


--
-- Name: users_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.users_id_seq', 9, true);


--
-- Name: version_changelog_id_seq; Type: SEQUENCE SET; Schema: public; Owner: ontextract_user
--

SELECT pg_catalog.setval('public.version_changelog_id_seq', 43, true);


--
-- Name: alembic_version alembic_version_pkc; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.alembic_version
    ADD CONSTRAINT alembic_version_pkc PRIMARY KEY (version_num);


--
-- Name: analysis_agents analysis_agents_pkey; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.analysis_agents
    ADD CONSTRAINT analysis_agents_pkey PRIMARY KEY (id);


--
-- Name: app_settings app_settings_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.app_settings
    ADD CONSTRAINT app_settings_pkey PRIMARY KEY (id);


--
-- Name: app_settings app_settings_setting_key_key; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.app_settings
    ADD CONSTRAINT app_settings_setting_key_key UNIQUE (setting_key);


--
-- Name: context_anchors context_anchors_pkey; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.context_anchors
    ADD CONSTRAINT context_anchors_pkey PRIMARY KEY (id);


--
-- Name: document_embeddings document_embeddings_pkey; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.document_embeddings
    ADD CONSTRAINT document_embeddings_pkey PRIMARY KEY (id);


--
-- Name: document_processing_index document_processing_index_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.document_processing_index
    ADD CONSTRAINT document_processing_index_pkey PRIMARY KEY (id);


--
-- Name: document_processing_summary document_processing_summary_document_id_processing_type_sou_key; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.document_processing_summary
    ADD CONSTRAINT document_processing_summary_document_id_processing_type_sou_key UNIQUE (document_id, processing_type, source_document_id);


--
-- Name: document_processing_summary document_processing_summary_pkey; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.document_processing_summary
    ADD CONSTRAINT document_processing_summary_pkey PRIMARY KEY (id);


--
-- Name: document_temporal_metadata document_temporal_metadata_document_id_experiment_id_key; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.document_temporal_metadata
    ADD CONSTRAINT document_temporal_metadata_document_id_experiment_id_key UNIQUE (document_id, experiment_id);


--
-- Name: document_temporal_metadata document_temporal_metadata_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.document_temporal_metadata
    ADD CONSTRAINT document_temporal_metadata_pkey PRIMARY KEY (id);


--
-- Name: documents documents_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.documents
    ADD CONSTRAINT documents_pkey PRIMARY KEY (id);


--
-- Name: documents documents_uuid_unique; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.documents
    ADD CONSTRAINT documents_uuid_unique UNIQUE (uuid);


--
-- Name: domains domains_name_key; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.domains
    ADD CONSTRAINT domains_name_key UNIQUE (name);


--
-- Name: domains domains_namespace_uri_key; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.domains
    ADD CONSTRAINT domains_namespace_uri_key UNIQUE (namespace_uri);


--
-- Name: domains domains_pkey; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.domains
    ADD CONSTRAINT domains_pkey PRIMARY KEY (id);


--
-- Name: domains domains_uuid_key; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.domains
    ADD CONSTRAINT domains_uuid_key UNIQUE (uuid);


--
-- Name: experiment_document_processing experiment_document_processing_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.experiment_document_processing
    ADD CONSTRAINT experiment_document_processing_pkey PRIMARY KEY (id);


--
-- Name: experiment_documents experiment_documents_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.experiment_documents
    ADD CONSTRAINT experiment_documents_pkey PRIMARY KEY (experiment_id, document_id);


--
-- Name: experiment_documents_v2 experiment_documents_v2_pkey; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.experiment_documents_v2
    ADD CONSTRAINT experiment_documents_v2_pkey PRIMARY KEY (id);


--
-- Name: experiment_orchestration_runs experiment_orchestration_runs_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.experiment_orchestration_runs
    ADD CONSTRAINT experiment_orchestration_runs_pkey PRIMARY KEY (id);


--
-- Name: experiment_references experiment_references_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.experiment_references
    ADD CONSTRAINT experiment_references_pkey PRIMARY KEY (experiment_id, reference_id);


--
-- Name: experiments experiments_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.experiments
    ADD CONSTRAINT experiments_pkey PRIMARY KEY (id);


--
-- Name: extracted_entities extracted_entities_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.extracted_entities
    ADD CONSTRAINT extracted_entities_pkey PRIMARY KEY (id);


--
-- Name: fuzziness_adjustments fuzziness_adjustments_pkey; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.fuzziness_adjustments
    ADD CONSTRAINT fuzziness_adjustments_pkey PRIMARY KEY (id);


--
-- Name: learning_patterns learning_patterns_pkey; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.learning_patterns
    ADD CONSTRAINT learning_patterns_pkey PRIMARY KEY (id);


--
-- Name: multi_model_consensus multi_model_consensus_pkey; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.multi_model_consensus
    ADD CONSTRAINT multi_model_consensus_pkey PRIMARY KEY (id);


--
-- Name: oed_definitions oed_definitions_pkey; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.oed_definitions
    ADD CONSTRAINT oed_definitions_pkey PRIMARY KEY (id);


--
-- Name: oed_etymology oed_etymology_pkey; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.oed_etymology
    ADD CONSTRAINT oed_etymology_pkey PRIMARY KEY (id);


--
-- Name: oed_historical_stats oed_historical_stats_pkey; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.oed_historical_stats
    ADD CONSTRAINT oed_historical_stats_pkey PRIMARY KEY (id);


--
-- Name: oed_historical_stats oed_historical_stats_term_id_time_period_key; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.oed_historical_stats
    ADD CONSTRAINT oed_historical_stats_term_id_time_period_key UNIQUE (term_id, time_period);


--
-- Name: oed_quotation_summaries oed_quotation_summaries_pkey; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.oed_quotation_summaries
    ADD CONSTRAINT oed_quotation_summaries_pkey PRIMARY KEY (id);


--
-- Name: oed_timeline_markers oed_timeline_markers_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.oed_timeline_markers
    ADD CONSTRAINT oed_timeline_markers_pkey PRIMARY KEY (id);


--
-- Name: oed_timeline_markers oed_timeline_markers_term_id_sense_number_year_key; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.oed_timeline_markers
    ADD CONSTRAINT oed_timeline_markers_term_id_sense_number_year_key UNIQUE (term_id, sense_number, year);


--
-- Name: ontologies ontologies_pkey; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.ontologies
    ADD CONSTRAINT ontologies_pkey PRIMARY KEY (id);


--
-- Name: ontologies ontologies_uuid_key; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.ontologies
    ADD CONSTRAINT ontologies_uuid_key UNIQUE (uuid);


--
-- Name: ontology_entities ontology_entities_pkey; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.ontology_entities
    ADD CONSTRAINT ontology_entities_pkey PRIMARY KEY (id);


--
-- Name: ontology_mappings ontology_mappings_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.ontology_mappings
    ADD CONSTRAINT ontology_mappings_pkey PRIMARY KEY (id);


--
-- Name: ontology_versions ontology_versions_pkey; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.ontology_versions
    ADD CONSTRAINT ontology_versions_pkey PRIMARY KEY (id);


--
-- Name: orchestration_decisions orchestration_decisions_pkey; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.orchestration_decisions
    ADD CONSTRAINT orchestration_decisions_pkey PRIMARY KEY (id);


--
-- Name: orchestration_feedback orchestration_feedback_pkey; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.orchestration_feedback
    ADD CONSTRAINT orchestration_feedback_pkey PRIMARY KEY (id);


--
-- Name: orchestration_overrides orchestration_overrides_pkey; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.orchestration_overrides
    ADD CONSTRAINT orchestration_overrides_pkey PRIMARY KEY (id);


--
-- Name: processing_artifact_groups processing_artifact_groups_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.processing_artifact_groups
    ADD CONSTRAINT processing_artifact_groups_pkey PRIMARY KEY (id);


--
-- Name: processing_artifacts processing_artifacts_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.processing_artifacts
    ADD CONSTRAINT processing_artifacts_pkey PRIMARY KEY (id);


--
-- Name: processing_jobs processing_jobs_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.processing_jobs
    ADD CONSTRAINT processing_jobs_pkey PRIMARY KEY (id);


--
-- Name: prompt_templates prompt_templates_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.prompt_templates
    ADD CONSTRAINT prompt_templates_pkey PRIMARY KEY (id);


--
-- Name: prompt_templates prompt_templates_template_key_key; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.prompt_templates
    ADD CONSTRAINT prompt_templates_template_key_key UNIQUE (template_key);


--
-- Name: prov_activities prov_activities_pkey; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.prov_activities
    ADD CONSTRAINT prov_activities_pkey PRIMARY KEY (activity_id);


--
-- Name: prov_agents prov_agents_pkey; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.prov_agents
    ADD CONSTRAINT prov_agents_pkey PRIMARY KEY (agent_id);


--
-- Name: prov_entities prov_entities_pkey; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.prov_entities
    ADD CONSTRAINT prov_entities_pkey PRIMARY KEY (entity_id);


--
-- Name: prov_relationships prov_relationships_pkey; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.prov_relationships
    ADD CONSTRAINT prov_relationships_pkey PRIMARY KEY (relationship_id);


--
-- Name: provenance_activities provenance_activities_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.provenance_activities
    ADD CONSTRAINT provenance_activities_pkey PRIMARY KEY (id);


--
-- Name: provenance_activities provenance_activities_prov_id_key; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.provenance_activities
    ADD CONSTRAINT provenance_activities_prov_id_key UNIQUE (prov_id);


--
-- Name: provenance_chains provenance_chains_pkey; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.provenance_chains
    ADD CONSTRAINT provenance_chains_pkey PRIMARY KEY (id);


--
-- Name: provenance_entities provenance_entities_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.provenance_entities
    ADD CONSTRAINT provenance_entities_pkey PRIMARY KEY (id);


--
-- Name: provenance_entities provenance_entities_prov_id_key; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.provenance_entities
    ADD CONSTRAINT provenance_entities_prov_id_key UNIQUE (prov_id);


--
-- Name: search_history search_history_pkey; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.search_history
    ADD CONSTRAINT search_history_pkey PRIMARY KEY (id);


--
-- Name: semantic_drift_activities semantic_drift_activities_pkey; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.semantic_drift_activities
    ADD CONSTRAINT semantic_drift_activities_pkey PRIMARY KEY (id);


--
-- Name: semantic_shift_analysis semantic_shift_analysis_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.semantic_shift_analysis
    ADD CONSTRAINT semantic_shift_analysis_pkey PRIMARY KEY (id);


--
-- Name: term_disciplinary_definitions term_disciplinary_definitions_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.term_disciplinary_definitions
    ADD CONSTRAINT term_disciplinary_definitions_pkey PRIMARY KEY (id);


--
-- Name: term_version_anchors term_version_anchors_pkey; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.term_version_anchors
    ADD CONSTRAINT term_version_anchors_pkey PRIMARY KEY (id);


--
-- Name: term_version_anchors term_version_anchors_term_version_id_context_anchor_id_key; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.term_version_anchors
    ADD CONSTRAINT term_version_anchors_term_version_id_context_anchor_id_key UNIQUE (term_version_id, context_anchor_id);


--
-- Name: term_versions term_versions_pkey; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.term_versions
    ADD CONSTRAINT term_versions_pkey PRIMARY KEY (id);


--
-- Name: terms terms_pkey; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.terms
    ADD CONSTRAINT terms_pkey PRIMARY KEY (id);


--
-- Name: terms terms_term_text_created_by_key; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.terms
    ADD CONSTRAINT terms_term_text_created_by_key UNIQUE (term_text, created_by);


--
-- Name: text_segments text_segments_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.text_segments
    ADD CONSTRAINT text_segments_pkey PRIMARY KEY (id);


--
-- Name: tool_execution_logs tool_execution_logs_pkey; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.tool_execution_logs
    ADD CONSTRAINT tool_execution_logs_pkey PRIMARY KEY (id);


--
-- Name: document_processing_index unique_doc_processing; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.document_processing_index
    ADD CONSTRAINT unique_doc_processing UNIQUE (document_id, processing_id);


--
-- Name: version_changelog unique_document_version_change; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.version_changelog
    ADD CONSTRAINT unique_document_version_change UNIQUE (document_id, version_number, change_type);


--
-- Name: experiment_documents_v2 unique_exp_doc; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.experiment_documents_v2
    ADD CONSTRAINT unique_exp_doc UNIQUE (experiment_id, document_id);


--
-- Name: processing_artifact_groups uq_artifact_group_method; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.processing_artifact_groups
    ADD CONSTRAINT uq_artifact_group_method UNIQUE (document_id, artifact_type, method_key);


--
-- Name: ontology_versions uq_ontology_version; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.ontology_versions
    ADD CONSTRAINT uq_ontology_version UNIQUE (ontology_id, version_number);


--
-- Name: users users_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.users
    ADD CONSTRAINT users_pkey PRIMARY KEY (id);


--
-- Name: version_changelog version_changelog_pkey; Type: CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.version_changelog
    ADD CONSTRAINT version_changelog_pkey PRIMARY KEY (id);


--
-- Name: idx_analysis_agents_active; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_analysis_agents_active ON public.analysis_agents USING btree (is_active) WHERE (is_active = true);


--
-- Name: idx_analysis_agents_type; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_analysis_agents_type ON public.analysis_agents USING btree (agent_type);


--
-- Name: idx_app_settings_category; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_app_settings_category ON public.app_settings USING btree (category);


--
-- Name: idx_app_settings_key; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_app_settings_key ON public.app_settings USING btree (setting_key);


--
-- Name: idx_app_settings_user_id; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_app_settings_user_id ON public.app_settings USING btree (user_id);


--
-- Name: idx_context_anchors_frequency; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_context_anchors_frequency ON public.context_anchors USING btree (frequency DESC);


--
-- Name: idx_context_anchors_term; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_context_anchors_term ON public.context_anchors USING btree (anchor_term);


--
-- Name: idx_disciplinary_def_discipline; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_disciplinary_def_discipline ON public.term_disciplinary_definitions USING btree (discipline);


--
-- Name: idx_disciplinary_def_document; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_disciplinary_def_document ON public.term_disciplinary_definitions USING btree (document_id);


--
-- Name: idx_disciplinary_def_experiment; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_disciplinary_def_experiment ON public.term_disciplinary_definitions USING btree (experiment_id);


--
-- Name: idx_disciplinary_def_term; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_disciplinary_def_term ON public.term_disciplinary_definitions USING btree (term_id);


--
-- Name: idx_doc_temporal_discipline; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_doc_temporal_discipline ON public.document_temporal_metadata USING btree (discipline);


--
-- Name: idx_doc_temporal_experiment; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_doc_temporal_experiment ON public.document_temporal_metadata USING btree (experiment_id);


--
-- Name: idx_doc_temporal_period; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_doc_temporal_period ON public.document_temporal_metadata USING btree (temporal_period);


--
-- Name: idx_doc_temporal_timeline_track; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_doc_temporal_timeline_track ON public.document_temporal_metadata USING btree (timeline_track);


--
-- Name: idx_doc_temporal_year; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_doc_temporal_year ON public.document_temporal_metadata USING btree (publication_year);


--
-- Name: idx_document_processing_lookup; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_document_processing_lookup ON public.document_processing_index USING btree (document_id, processing_type, status);


--
-- Name: idx_documents_experiment_id; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_documents_experiment_id ON public.documents USING btree (experiment_id);


--
-- Name: idx_documents_metadata_provenance; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_documents_metadata_provenance ON public.documents USING gin (metadata_provenance);


--
-- Name: idx_documents_parent; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_documents_parent ON public.documents USING btree (parent_document_id);


--
-- Name: idx_documents_source_document_id; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_documents_source_document_id ON public.documents USING btree (source_document_id);


--
-- Name: idx_documents_type; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_documents_type ON public.documents USING btree (document_type);


--
-- Name: idx_documents_uuid; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_documents_uuid ON public.documents USING btree (uuid);


--
-- Name: idx_documents_version_number; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_documents_version_number ON public.documents USING btree (version_number);


--
-- Name: idx_documents_version_type; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_documents_version_type ON public.documents USING btree (version_type);


--
-- Name: idx_drift_activities_agent; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_drift_activities_agent ON public.semantic_drift_activities USING btree (was_associated_with);


--
-- Name: idx_drift_activities_generated_entity; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_drift_activities_generated_entity ON public.semantic_drift_activities USING btree (generated_entity);


--
-- Name: idx_drift_activities_periods; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_drift_activities_periods ON public.semantic_drift_activities USING btree (start_period, end_period);


--
-- Name: idx_drift_activities_status; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_drift_activities_status ON public.semantic_drift_activities USING btree (activity_status);


--
-- Name: idx_drift_activities_used_entity; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_drift_activities_used_entity ON public.semantic_drift_activities USING btree (used_entity);


--
-- Name: idx_embeddings_document; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_embeddings_document ON public.document_embeddings USING btree (document_id);


--
-- Name: idx_embeddings_model; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_embeddings_model ON public.document_embeddings USING btree (model_name);


--
-- Name: idx_embeddings_term_period; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_embeddings_term_period ON public.document_embeddings USING btree (term, period);


--
-- Name: idx_entity_label; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_entity_label ON public.ontology_entities USING btree (label);


--
-- Name: idx_entity_type; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_entity_type ON public.ontology_entities USING btree (entity_type);


--
-- Name: idx_experiment_documents_status; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_experiment_documents_status ON public.experiment_documents USING btree (processing_status);


--
-- Name: idx_experiment_documents_updated; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_experiment_documents_updated ON public.experiment_documents USING btree (updated_at);


--
-- Name: idx_experiment_references_experiment; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_experiment_references_experiment ON public.experiment_references USING btree (experiment_id);


--
-- Name: idx_experiment_references_reference; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_experiment_references_reference ON public.experiment_references USING btree (reference_id);


--
-- Name: idx_fuzziness_adjustments_user; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_fuzziness_adjustments_user ON public.fuzziness_adjustments USING btree (adjusted_by);


--
-- Name: idx_fuzziness_adjustments_version; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_fuzziness_adjustments_version ON public.fuzziness_adjustments USING btree (term_version_id);


--
-- Name: idx_learning_patterns_context_type; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_learning_patterns_context_type ON public.learning_patterns USING btree (context_signature, pattern_type);


--
-- Name: idx_learning_patterns_status; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_learning_patterns_status ON public.learning_patterns USING btree (pattern_status);


--
-- Name: idx_learning_patterns_success_rate; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_learning_patterns_success_rate ON public.learning_patterns USING btree (success_rate DESC);


--
-- Name: idx_multi_model_consensus_decision; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_multi_model_consensus_decision ON public.multi_model_consensus USING btree (orchestration_decision_id);


--
-- Name: idx_multi_model_consensus_reached; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_multi_model_consensus_reached ON public.multi_model_consensus USING btree (consensus_reached);


--
-- Name: idx_oed_definitions_period; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_oed_definitions_period ON public.oed_definitions USING btree (historical_period);


--
-- Name: idx_oed_definitions_temporal; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_oed_definitions_temporal ON public.oed_definitions USING btree (first_cited_year, last_cited_year);


--
-- Name: idx_oed_historical_stats_term_period; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_oed_historical_stats_term_period ON public.oed_historical_stats USING btree (term_id, start_year, end_year);


--
-- Name: idx_oed_quotations_chronological; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_oed_quotations_chronological ON public.oed_quotation_summaries USING btree (term_id, chronological_rank);


--
-- Name: idx_oed_quotations_term_year; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_oed_quotations_term_year ON public.oed_quotation_summaries USING btree (term_id, quotation_year);


--
-- Name: idx_oed_timeline_category; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_oed_timeline_category ON public.oed_timeline_markers USING btree (semantic_category);


--
-- Name: idx_oed_timeline_sense; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_oed_timeline_sense ON public.oed_timeline_markers USING btree (sense_number);


--
-- Name: idx_oed_timeline_term; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_oed_timeline_term ON public.oed_timeline_markers USING btree (term_id);


--
-- Name: idx_oed_timeline_year; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_oed_timeline_year ON public.oed_timeline_markers USING btree (year);


--
-- Name: idx_ontology_entity; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_ontology_entity ON public.ontology_entities USING btree (ontology_id, entity_type);


--
-- Name: idx_orchestration_decisions_agent; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_orchestration_decisions_agent ON public.orchestration_decisions USING btree (was_associated_with);


--
-- Name: idx_orchestration_decisions_document; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_orchestration_decisions_document ON public.orchestration_decisions USING btree (document_id);


--
-- Name: idx_orchestration_decisions_experiment; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_orchestration_decisions_experiment ON public.orchestration_decisions USING btree (experiment_id, created_at);


--
-- Name: idx_orchestration_decisions_status; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_orchestration_decisions_status ON public.orchestration_decisions USING btree (activity_status);


--
-- Name: idx_orchestration_decisions_term_time; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_orchestration_decisions_term_time ON public.orchestration_decisions USING btree (term_text, created_at);


--
-- Name: idx_orchestration_feedback_decision_researcher; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_orchestration_feedback_decision_researcher ON public.orchestration_feedback USING btree (orchestration_decision_id, researcher_id);


--
-- Name: idx_orchestration_feedback_provided_at; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_orchestration_feedback_provided_at ON public.orchestration_feedback USING btree (provided_at);


--
-- Name: idx_orchestration_feedback_type_status; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_orchestration_feedback_type_status ON public.orchestration_feedback USING btree (feedback_type, feedback_status);


--
-- Name: idx_orchestration_lookup; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_orchestration_lookup ON public.experiment_orchestration_runs USING btree (experiment_id, status, started_at);


--
-- Name: idx_orchestration_overrides_applied_at; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_orchestration_overrides_applied_at ON public.orchestration_overrides USING btree (applied_at);


--
-- Name: idx_orchestration_overrides_decision_researcher; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_orchestration_overrides_decision_researcher ON public.orchestration_overrides USING btree (orchestration_decision_id, researcher_id);


--
-- Name: idx_processing_summary_document; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_processing_summary_document ON public.document_processing_summary USING btree (document_id);


--
-- Name: idx_processing_summary_type; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_processing_summary_type ON public.document_processing_summary USING btree (processing_type);


--
-- Name: idx_prompt_templates_active; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_prompt_templates_active ON public.prompt_templates USING btree (is_active);


--
-- Name: idx_prompt_templates_category; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_prompt_templates_category ON public.prompt_templates USING btree (category);


--
-- Name: idx_prompt_templates_key; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_prompt_templates_key ON public.prompt_templates USING btree (template_key);


--
-- Name: idx_prov_activities_associated; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_prov_activities_associated ON public.prov_activities USING btree (wasassociatedwith);


--
-- Name: idx_prov_activities_started; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_prov_activities_started ON public.prov_activities USING btree (startedattime);


--
-- Name: idx_prov_activities_type; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_prov_activities_type ON public.prov_activities USING btree (activity_type);


--
-- Name: idx_prov_agents_name; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_prov_agents_name ON public.prov_agents USING btree (foaf_name);


--
-- Name: idx_prov_agents_type; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_prov_agents_type ON public.prov_agents USING btree (agent_type);


--
-- Name: idx_prov_entities_attributed; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_prov_entities_attributed ON public.prov_entities USING btree (wasattributedto);


--
-- Name: idx_prov_entities_derived; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_prov_entities_derived ON public.prov_entities USING btree (wasderivedfrom);


--
-- Name: idx_prov_entities_generated; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_prov_entities_generated ON public.prov_entities USING btree (wasgeneratedby);


--
-- Name: idx_prov_entities_type; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_prov_entities_type ON public.prov_entities USING btree (entity_type);


--
-- Name: idx_prov_relationships_object; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_prov_relationships_object ON public.prov_relationships USING btree (object_id, object_type);


--
-- Name: idx_prov_relationships_subject; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_prov_relationships_subject ON public.prov_relationships USING btree (subject_id, subject_type);


--
-- Name: idx_prov_relationships_type; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_prov_relationships_type ON public.prov_relationships USING btree (relationship_type);


--
-- Name: idx_provenance_activities_activity_type; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_provenance_activities_activity_type ON public.provenance_activities USING btree (activity_type);


--
-- Name: idx_provenance_activities_experiment_id; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_provenance_activities_experiment_id ON public.provenance_activities USING btree (experiment_id);


--
-- Name: idx_provenance_activities_processing_job_id; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_provenance_activities_processing_job_id ON public.provenance_activities USING btree (processing_job_id);


--
-- Name: idx_provenance_activities_prov_id; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_provenance_activities_prov_id ON public.provenance_activities USING btree (prov_id);


--
-- Name: idx_provenance_activities_prov_type; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_provenance_activities_prov_type ON public.provenance_activities USING btree (prov_type);


--
-- Name: idx_provenance_entities_derived_from; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_provenance_entities_derived_from ON public.provenance_entities USING btree (derived_from_entity);


--
-- Name: idx_provenance_entities_document_id; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_provenance_entities_document_id ON public.provenance_entities USING btree (document_id);


--
-- Name: idx_provenance_entities_experiment_id; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_provenance_entities_experiment_id ON public.provenance_entities USING btree (experiment_id);


--
-- Name: idx_provenance_entities_generated_by; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_provenance_entities_generated_by ON public.provenance_entities USING btree (generated_by_activity);


--
-- Name: idx_provenance_entities_prov_id; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_provenance_entities_prov_id ON public.provenance_entities USING btree (prov_id);


--
-- Name: idx_provenance_entities_prov_type; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_provenance_entities_prov_type ON public.provenance_entities USING btree (prov_type);


--
-- Name: idx_semantic_shift_experiment; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_semantic_shift_experiment ON public.semantic_shift_analysis USING btree (experiment_id);


--
-- Name: idx_semantic_shift_term; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_semantic_shift_term ON public.semantic_shift_analysis USING btree (term_id);


--
-- Name: idx_semantic_shift_type; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_semantic_shift_type ON public.semantic_shift_analysis USING btree (shift_type);


--
-- Name: idx_term_version_anchors_anchor; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_term_version_anchors_anchor ON public.term_version_anchors USING btree (context_anchor_id);


--
-- Name: idx_term_version_anchors_similarity; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_term_version_anchors_similarity ON public.term_version_anchors USING btree (similarity_score DESC);


--
-- Name: idx_term_version_anchors_version; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_term_version_anchors_version ON public.term_version_anchors USING btree (term_version_id);


--
-- Name: idx_term_versions_corpus; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_term_versions_corpus ON public.term_versions USING btree (corpus_source);


--
-- Name: idx_term_versions_current; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_term_versions_current ON public.term_versions USING btree (is_current) WHERE (is_current = true);


--
-- Name: idx_term_versions_fuzziness; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_term_versions_fuzziness ON public.term_versions USING btree (fuzziness_score);


--
-- Name: idx_term_versions_temporal_period; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_term_versions_temporal_period ON public.term_versions USING btree (temporal_period);


--
-- Name: idx_term_versions_temporal_years; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_term_versions_temporal_years ON public.term_versions USING btree (temporal_start_year, temporal_end_year);


--
-- Name: idx_term_versions_term_id; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_term_versions_term_id ON public.term_versions USING btree (term_id);


--
-- Name: idx_terms_created_by; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_terms_created_by ON public.terms USING btree (created_by);


--
-- Name: idx_terms_research_domain; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_terms_research_domain ON public.terms USING btree (research_domain);


--
-- Name: idx_terms_status; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_terms_status ON public.terms USING btree (status);


--
-- Name: idx_terms_text; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_terms_text ON public.terms USING btree (term_text);


--
-- Name: idx_text_segments_doc_method; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_text_segments_doc_method ON public.text_segments USING btree (document_id, segmentation_method);


--
-- Name: idx_text_segments_job_id; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_text_segments_job_id ON public.text_segments USING btree (segmentation_job_id);


--
-- Name: idx_text_segments_segmentation_method; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX idx_text_segments_segmentation_method ON public.text_segments USING btree (segmentation_method);


--
-- Name: idx_tool_execution_logs_decision_order; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_tool_execution_logs_decision_order ON public.tool_execution_logs USING btree (orchestration_decision_id, execution_order);


--
-- Name: idx_tool_execution_logs_status; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_tool_execution_logs_status ON public.tool_execution_logs USING btree (execution_status);


--
-- Name: idx_tool_execution_logs_tool_name; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_tool_execution_logs_tool_name ON public.tool_execution_logs USING btree (tool_name);


--
-- Name: idx_version_changelog_change_type; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_version_changelog_change_type ON public.version_changelog USING btree (change_type);


--
-- Name: idx_version_changelog_document_version; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX idx_version_changelog_document_version ON public.version_changelog USING btree (document_id, version_number);


--
-- Name: ix_analysis_agents_agent_type; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX ix_analysis_agents_agent_type ON public.analysis_agents USING btree (agent_type);


--
-- Name: ix_analysis_agents_is_active; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX ix_analysis_agents_is_active ON public.analysis_agents USING btree (is_active);


--
-- Name: ix_context_anchors_anchor_term; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE UNIQUE INDEX ix_context_anchors_anchor_term ON public.context_anchors USING btree (anchor_term);


--
-- Name: ix_context_anchors_frequency; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX ix_context_anchors_frequency ON public.context_anchors USING btree (frequency);


--
-- Name: ix_document_processing_index_document_id; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX ix_document_processing_index_document_id ON public.document_processing_index USING btree (document_id);


--
-- Name: ix_document_processing_index_experiment_id; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX ix_document_processing_index_experiment_id ON public.document_processing_index USING btree (experiment_id);


--
-- Name: ix_document_processing_index_processing_id; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX ix_document_processing_index_processing_id ON public.document_processing_index USING btree (processing_id);


--
-- Name: ix_document_processing_index_processing_type; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX ix_document_processing_index_processing_type ON public.document_processing_index USING btree (processing_type);


--
-- Name: ix_document_processing_index_status; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX ix_document_processing_index_status ON public.document_processing_index USING btree (status);


--
-- Name: ix_documents_user_id; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX ix_documents_user_id ON public.documents USING btree (user_id);


--
-- Name: ix_entity_embedding_vector; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX ix_entity_embedding_vector ON public.ontology_entities USING ivfflat (embedding public.vector_cosine_ops) WITH (lists='100');


--
-- Name: ix_experiment_document_processing_experiment_document_id; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX ix_experiment_document_processing_experiment_document_id ON public.experiment_document_processing USING btree (experiment_document_id);


--
-- Name: ix_experiment_documents_v2_document_id; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX ix_experiment_documents_v2_document_id ON public.experiment_documents_v2 USING btree (document_id);


--
-- Name: ix_experiment_documents_v2_experiment_id; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX ix_experiment_documents_v2_experiment_id ON public.experiment_documents_v2 USING btree (experiment_id);


--
-- Name: ix_experiment_orchestration_runs_experiment_id; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX ix_experiment_orchestration_runs_experiment_id ON public.experiment_orchestration_runs USING btree (experiment_id);


--
-- Name: ix_experiment_orchestration_runs_status; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX ix_experiment_orchestration_runs_status ON public.experiment_orchestration_runs USING btree (status);


--
-- Name: ix_experiment_orchestration_runs_user_id; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX ix_experiment_orchestration_runs_user_id ON public.experiment_orchestration_runs USING btree (user_id);


--
-- Name: ix_experiments_term_id; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX ix_experiments_term_id ON public.experiments USING btree (term_id);


--
-- Name: ix_experiments_user_id; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX ix_experiments_user_id ON public.experiments USING btree (user_id);


--
-- Name: ix_extracted_entities_processing_job_id; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX ix_extracted_entities_processing_job_id ON public.extracted_entities USING btree (processing_job_id);


--
-- Name: ix_extracted_entities_text_segment_id; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX ix_extracted_entities_text_segment_id ON public.extracted_entities USING btree (text_segment_id);


--
-- Name: ix_fuzziness_adjustments_adjusted_by; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX ix_fuzziness_adjustments_adjusted_by ON public.fuzziness_adjustments USING btree (adjusted_by);


--
-- Name: ix_fuzziness_adjustments_term_version_id; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX ix_fuzziness_adjustments_term_version_id ON public.fuzziness_adjustments USING btree (term_version_id);


--
-- Name: ix_ontology_mappings_extracted_entity_id; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX ix_ontology_mappings_extracted_entity_id ON public.ontology_mappings USING btree (extracted_entity_id);


--
-- Name: ix_processing_artifact_groups_artifact_type; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX ix_processing_artifact_groups_artifact_type ON public.processing_artifact_groups USING btree (artifact_type);


--
-- Name: ix_processing_artifact_groups_document; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX ix_processing_artifact_groups_document ON public.processing_artifact_groups USING btree (document_id);


--
-- Name: ix_processing_artifact_groups_document_id; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX ix_processing_artifact_groups_document_id ON public.processing_artifact_groups USING btree (document_id);


--
-- Name: ix_processing_artifact_groups_processing_job_id; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX ix_processing_artifact_groups_processing_job_id ON public.processing_artifact_groups USING btree (processing_job_id);


--
-- Name: ix_processing_artifact_groups_status; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX ix_processing_artifact_groups_status ON public.processing_artifact_groups USING btree (status);


--
-- Name: ix_processing_artifact_groups_type; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX ix_processing_artifact_groups_type ON public.processing_artifact_groups USING btree (artifact_type);


--
-- Name: ix_processing_artifacts_document_id; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX ix_processing_artifacts_document_id ON public.processing_artifacts USING btree (document_id);


--
-- Name: ix_processing_artifacts_processing_id; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX ix_processing_artifacts_processing_id ON public.processing_artifacts USING btree (processing_id);


--
-- Name: ix_processing_jobs_document_id; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX ix_processing_jobs_document_id ON public.processing_jobs USING btree (document_id);


--
-- Name: ix_processing_jobs_parent_job_id; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX ix_processing_jobs_parent_job_id ON public.processing_jobs USING btree (parent_job_id);


--
-- Name: ix_processing_jobs_user_id; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX ix_processing_jobs_user_id ON public.processing_jobs USING btree (user_id);


--
-- Name: ix_semantic_drift_activities_activity_status; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX ix_semantic_drift_activities_activity_status ON public.semantic_drift_activities USING btree (activity_status);


--
-- Name: ix_semantic_drift_activities_end_period; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX ix_semantic_drift_activities_end_period ON public.semantic_drift_activities USING btree (end_period);


--
-- Name: ix_semantic_drift_activities_generated_entity; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX ix_semantic_drift_activities_generated_entity ON public.semantic_drift_activities USING btree (generated_entity);


--
-- Name: ix_semantic_drift_activities_start_period; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX ix_semantic_drift_activities_start_period ON public.semantic_drift_activities USING btree (start_period);


--
-- Name: ix_semantic_drift_activities_used_entity; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX ix_semantic_drift_activities_used_entity ON public.semantic_drift_activities USING btree (used_entity);


--
-- Name: ix_semantic_drift_activities_was_associated_with; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX ix_semantic_drift_activities_was_associated_with ON public.semantic_drift_activities USING btree (was_associated_with);


--
-- Name: ix_term_versions_corpus_source; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX ix_term_versions_corpus_source ON public.term_versions USING btree (corpus_source);


--
-- Name: ix_term_versions_is_current; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX ix_term_versions_is_current ON public.term_versions USING btree (is_current);


--
-- Name: ix_term_versions_temporal_end_year; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX ix_term_versions_temporal_end_year ON public.term_versions USING btree (temporal_end_year);


--
-- Name: ix_term_versions_temporal_period; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX ix_term_versions_temporal_period ON public.term_versions USING btree (temporal_period);


--
-- Name: ix_term_versions_temporal_start_year; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX ix_term_versions_temporal_start_year ON public.term_versions USING btree (temporal_start_year);


--
-- Name: ix_term_versions_term_id; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX ix_term_versions_term_id ON public.term_versions USING btree (term_id);


--
-- Name: ix_terms_created_by; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX ix_terms_created_by ON public.terms USING btree (created_by);


--
-- Name: ix_terms_research_domain; Type: INDEX; Schema: public; Owner: ontextract_user
--

CREATE INDEX ix_terms_research_domain ON public.terms USING btree (research_domain);


--
-- Name: ix_text_segments_document_id; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX ix_text_segments_document_id ON public.text_segments USING btree (document_id);


--
-- Name: ix_text_segments_group_id; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX ix_text_segments_group_id ON public.text_segments USING btree (group_id);


--
-- Name: ix_text_segments_parent_segment_id; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX ix_text_segments_parent_segment_id ON public.text_segments USING btree (parent_segment_id);


--
-- Name: ix_text_segments_processing_method; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX ix_text_segments_processing_method ON public.text_segments USING btree (processing_method);


--
-- Name: ix_users_email; Type: INDEX; Schema: public; Owner: postgres
--

CREATE UNIQUE INDEX ix_users_email ON public.users USING btree (email);


--
-- Name: ix_users_username; Type: INDEX; Schema: public; Owner: postgres
--

CREATE UNIQUE INDEX ix_users_username ON public.users USING btree (username);


--
-- Name: term_version_anchors trigger_update_context_anchor_frequency; Type: TRIGGER; Schema: public; Owner: ontextract_user
--

CREATE TRIGGER trigger_update_context_anchor_frequency AFTER INSERT OR DELETE ON public.term_version_anchors FOR EACH ROW EXECUTE FUNCTION public.update_context_anchor_frequency();


--
-- Name: terms trigger_update_terms_updated_at; Type: TRIGGER; Schema: public; Owner: ontextract_user
--

CREATE TRIGGER trigger_update_terms_updated_at BEFORE UPDATE ON public.terms FOR EACH ROW EXECUTE FUNCTION public.update_terms_updated_at();


--
-- Name: provenance_activities update_provenance_activities_updated_at; Type: TRIGGER; Schema: public; Owner: postgres
--

CREATE TRIGGER update_provenance_activities_updated_at BEFORE UPDATE ON public.provenance_activities FOR EACH ROW EXECUTE FUNCTION public.update_updated_at_column();


--
-- Name: provenance_entities update_provenance_entities_updated_at; Type: TRIGGER; Schema: public; Owner: postgres
--

CREATE TRIGGER update_provenance_entities_updated_at BEFORE UPDATE ON public.provenance_entities FOR EACH ROW EXECUTE FUNCTION public.update_updated_at_column();


--
-- Name: analysis_agents analysis_agents_user_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.analysis_agents
    ADD CONSTRAINT analysis_agents_user_id_fkey FOREIGN KEY (user_id) REFERENCES public.users(id);


--
-- Name: app_settings app_settings_user_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.app_settings
    ADD CONSTRAINT app_settings_user_id_fkey FOREIGN KEY (user_id) REFERENCES public.users(id) ON DELETE CASCADE;


--
-- Name: context_anchors context_anchors_first_used_in_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.context_anchors
    ADD CONSTRAINT context_anchors_first_used_in_fkey FOREIGN KEY (first_used_in) REFERENCES public.term_versions(id);


--
-- Name: context_anchors context_anchors_last_used_in_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.context_anchors
    ADD CONSTRAINT context_anchors_last_used_in_fkey FOREIGN KEY (last_used_in) REFERENCES public.term_versions(id);


--
-- Name: document_embeddings document_embeddings_document_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.document_embeddings
    ADD CONSTRAINT document_embeddings_document_id_fkey FOREIGN KEY (document_id) REFERENCES public.documents(id);


--
-- Name: document_processing_index document_processing_index_document_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.document_processing_index
    ADD CONSTRAINT document_processing_index_document_id_fkey FOREIGN KEY (document_id) REFERENCES public.documents(id);


--
-- Name: document_processing_index document_processing_index_experiment_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.document_processing_index
    ADD CONSTRAINT document_processing_index_experiment_id_fkey FOREIGN KEY (experiment_id) REFERENCES public.experiments(id);


--
-- Name: document_processing_index document_processing_index_processing_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.document_processing_index
    ADD CONSTRAINT document_processing_index_processing_id_fkey FOREIGN KEY (processing_id) REFERENCES public.experiment_document_processing(id);


--
-- Name: document_processing_summary document_processing_summary_document_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.document_processing_summary
    ADD CONSTRAINT document_processing_summary_document_id_fkey FOREIGN KEY (document_id) REFERENCES public.documents(id) ON DELETE CASCADE;


--
-- Name: document_processing_summary document_processing_summary_job_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.document_processing_summary
    ADD CONSTRAINT document_processing_summary_job_id_fkey FOREIGN KEY (job_id) REFERENCES public.processing_jobs(id);


--
-- Name: document_processing_summary document_processing_summary_source_document_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.document_processing_summary
    ADD CONSTRAINT document_processing_summary_source_document_id_fkey FOREIGN KEY (source_document_id) REFERENCES public.documents(id);


--
-- Name: document_temporal_metadata document_temporal_metadata_document_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.document_temporal_metadata
    ADD CONSTRAINT document_temporal_metadata_document_id_fkey FOREIGN KEY (document_id) REFERENCES public.documents(id) ON DELETE CASCADE;


--
-- Name: document_temporal_metadata document_temporal_metadata_experiment_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.document_temporal_metadata
    ADD CONSTRAINT document_temporal_metadata_experiment_id_fkey FOREIGN KEY (experiment_id) REFERENCES public.experiments(id) ON DELETE CASCADE;


--
-- Name: document_temporal_metadata document_temporal_metadata_reviewed_by_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.document_temporal_metadata
    ADD CONSTRAINT document_temporal_metadata_reviewed_by_fkey FOREIGN KEY (reviewed_by) REFERENCES public.users(id);


--
-- Name: documents documents_user_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.documents
    ADD CONSTRAINT documents_user_id_fkey FOREIGN KEY (user_id) REFERENCES public.users(id);


--
-- Name: experiment_document_processing experiment_document_processing_experiment_document_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.experiment_document_processing
    ADD CONSTRAINT experiment_document_processing_experiment_document_id_fkey FOREIGN KEY (experiment_document_id) REFERENCES public.experiment_documents_v2(id);


--
-- Name: experiment_documents experiment_documents_document_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.experiment_documents
    ADD CONSTRAINT experiment_documents_document_id_fkey FOREIGN KEY (document_id) REFERENCES public.documents(id);


--
-- Name: experiment_documents experiment_documents_experiment_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.experiment_documents
    ADD CONSTRAINT experiment_documents_experiment_id_fkey FOREIGN KEY (experiment_id) REFERENCES public.experiments(id);


--
-- Name: experiment_documents_v2 experiment_documents_v2_document_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.experiment_documents_v2
    ADD CONSTRAINT experiment_documents_v2_document_id_fkey FOREIGN KEY (document_id) REFERENCES public.documents(id);


--
-- Name: experiment_documents_v2 experiment_documents_v2_experiment_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.experiment_documents_v2
    ADD CONSTRAINT experiment_documents_v2_experiment_id_fkey FOREIGN KEY (experiment_id) REFERENCES public.experiments(id);


--
-- Name: experiment_orchestration_runs experiment_orchestration_runs_experiment_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.experiment_orchestration_runs
    ADD CONSTRAINT experiment_orchestration_runs_experiment_id_fkey FOREIGN KEY (experiment_id) REFERENCES public.experiments(id) ON DELETE CASCADE;


--
-- Name: experiment_orchestration_runs experiment_orchestration_runs_reviewed_by_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.experiment_orchestration_runs
    ADD CONSTRAINT experiment_orchestration_runs_reviewed_by_fkey FOREIGN KEY (reviewed_by) REFERENCES public.users(id);


--
-- Name: experiment_orchestration_runs experiment_orchestration_runs_user_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.experiment_orchestration_runs
    ADD CONSTRAINT experiment_orchestration_runs_user_id_fkey FOREIGN KEY (user_id) REFERENCES public.users(id);


--
-- Name: experiment_references experiment_references_experiment_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.experiment_references
    ADD CONSTRAINT experiment_references_experiment_id_fkey FOREIGN KEY (experiment_id) REFERENCES public.experiments(id);


--
-- Name: experiment_references experiment_references_reference_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.experiment_references
    ADD CONSTRAINT experiment_references_reference_id_fkey FOREIGN KEY (reference_id) REFERENCES public.documents(id);


--
-- Name: experiments experiments_user_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.experiments
    ADD CONSTRAINT experiments_user_id_fkey FOREIGN KEY (user_id) REFERENCES public.users(id);


--
-- Name: extracted_entities extracted_entities_processing_job_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.extracted_entities
    ADD CONSTRAINT extracted_entities_processing_job_id_fkey FOREIGN KEY (processing_job_id) REFERENCES public.processing_jobs(id);


--
-- Name: extracted_entities extracted_entities_text_segment_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.extracted_entities
    ADD CONSTRAINT extracted_entities_text_segment_id_fkey FOREIGN KEY (text_segment_id) REFERENCES public.text_segments(id);


--
-- Name: documents fk_documents_experiment; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.documents
    ADD CONSTRAINT fk_documents_experiment FOREIGN KEY (experiment_id) REFERENCES public.experiments(id) ON DELETE SET NULL;


--
-- Name: documents fk_documents_parent_document_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.documents
    ADD CONSTRAINT fk_documents_parent_document_id FOREIGN KEY (parent_document_id) REFERENCES public.documents(id) ON DELETE CASCADE;


--
-- Name: documents fk_documents_source; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.documents
    ADD CONSTRAINT fk_documents_source FOREIGN KEY (source_document_id) REFERENCES public.documents(id) ON DELETE CASCADE;


--
-- Name: experiments fk_experiments_term_id; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.experiments
    ADD CONSTRAINT fk_experiments_term_id FOREIGN KEY (term_id) REFERENCES public.terms(id);


--
-- Name: provenance_activities fk_provenance_activities_experiment; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.provenance_activities
    ADD CONSTRAINT fk_provenance_activities_experiment FOREIGN KEY (experiment_id) REFERENCES public.experiments(id) ON DELETE SET NULL;


--
-- Name: provenance_activities fk_provenance_activities_processing_job; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.provenance_activities
    ADD CONSTRAINT fk_provenance_activities_processing_job FOREIGN KEY (processing_job_id) REFERENCES public.processing_jobs(id) ON DELETE CASCADE;


--
-- Name: provenance_entities fk_provenance_entities_document; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.provenance_entities
    ADD CONSTRAINT fk_provenance_entities_document FOREIGN KEY (document_id) REFERENCES public.documents(id) ON DELETE CASCADE;


--
-- Name: provenance_entities fk_provenance_entities_experiment; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.provenance_entities
    ADD CONSTRAINT fk_provenance_entities_experiment FOREIGN KEY (experiment_id) REFERENCES public.experiments(id) ON DELETE SET NULL;


--
-- Name: fuzziness_adjustments fuzziness_adjustments_adjusted_by_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.fuzziness_adjustments
    ADD CONSTRAINT fuzziness_adjustments_adjusted_by_fkey FOREIGN KEY (adjusted_by) REFERENCES public.users(id);


--
-- Name: fuzziness_adjustments fuzziness_adjustments_term_version_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.fuzziness_adjustments
    ADD CONSTRAINT fuzziness_adjustments_term_version_id_fkey FOREIGN KEY (term_version_id) REFERENCES public.term_versions(id);


--
-- Name: learning_patterns learning_patterns_derived_from_feedback_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.learning_patterns
    ADD CONSTRAINT learning_patterns_derived_from_feedback_fkey FOREIGN KEY (derived_from_feedback) REFERENCES public.orchestration_feedback(id);


--
-- Name: multi_model_consensus multi_model_consensus_orchestration_decision_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.multi_model_consensus
    ADD CONSTRAINT multi_model_consensus_orchestration_decision_id_fkey FOREIGN KEY (orchestration_decision_id) REFERENCES public.orchestration_decisions(id) ON DELETE CASCADE;


--
-- Name: oed_definitions oed_definitions_term_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.oed_definitions
    ADD CONSTRAINT oed_definitions_term_id_fkey FOREIGN KEY (term_id) REFERENCES public.terms(id) ON DELETE CASCADE;


--
-- Name: oed_etymology oed_etymology_term_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.oed_etymology
    ADD CONSTRAINT oed_etymology_term_id_fkey FOREIGN KEY (term_id) REFERENCES public.terms(id) ON DELETE CASCADE;


--
-- Name: oed_historical_stats oed_historical_stats_term_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.oed_historical_stats
    ADD CONSTRAINT oed_historical_stats_term_id_fkey FOREIGN KEY (term_id) REFERENCES public.terms(id) ON DELETE CASCADE;


--
-- Name: oed_quotation_summaries oed_quotation_summaries_oed_definition_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.oed_quotation_summaries
    ADD CONSTRAINT oed_quotation_summaries_oed_definition_id_fkey FOREIGN KEY (oed_definition_id) REFERENCES public.oed_definitions(id) ON DELETE CASCADE;


--
-- Name: oed_quotation_summaries oed_quotation_summaries_term_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.oed_quotation_summaries
    ADD CONSTRAINT oed_quotation_summaries_term_id_fkey FOREIGN KEY (term_id) REFERENCES public.terms(id) ON DELETE CASCADE;


--
-- Name: oed_timeline_markers oed_timeline_markers_term_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.oed_timeline_markers
    ADD CONSTRAINT oed_timeline_markers_term_id_fkey FOREIGN KEY (term_id) REFERENCES public.terms(id) ON DELETE CASCADE;


--
-- Name: ontologies ontologies_domain_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.ontologies
    ADD CONSTRAINT ontologies_domain_id_fkey FOREIGN KEY (domain_id) REFERENCES public.domains(id);


--
-- Name: ontologies ontologies_parent_ontology_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.ontologies
    ADD CONSTRAINT ontologies_parent_ontology_id_fkey FOREIGN KEY (parent_ontology_id) REFERENCES public.ontologies(id);


--
-- Name: ontology_entities ontology_entities_ontology_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.ontology_entities
    ADD CONSTRAINT ontology_entities_ontology_id_fkey FOREIGN KEY (ontology_id) REFERENCES public.ontologies(id);


--
-- Name: ontology_mappings ontology_mappings_extracted_entity_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.ontology_mappings
    ADD CONSTRAINT ontology_mappings_extracted_entity_id_fkey FOREIGN KEY (extracted_entity_id) REFERENCES public.extracted_entities(id);


--
-- Name: ontology_versions ontology_versions_ontology_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.ontology_versions
    ADD CONSTRAINT ontology_versions_ontology_id_fkey FOREIGN KEY (ontology_id) REFERENCES public.ontologies(id);


--
-- Name: orchestration_decisions orchestration_decisions_created_by_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.orchestration_decisions
    ADD CONSTRAINT orchestration_decisions_created_by_fkey FOREIGN KEY (created_by) REFERENCES public.users(id);


--
-- Name: orchestration_decisions orchestration_decisions_document_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.orchestration_decisions
    ADD CONSTRAINT orchestration_decisions_document_id_fkey FOREIGN KEY (document_id) REFERENCES public.documents(id);


--
-- Name: orchestration_decisions orchestration_decisions_experiment_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.orchestration_decisions
    ADD CONSTRAINT orchestration_decisions_experiment_id_fkey FOREIGN KEY (experiment_id) REFERENCES public.experiments(id);


--
-- Name: orchestration_decisions orchestration_decisions_used_entity_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.orchestration_decisions
    ADD CONSTRAINT orchestration_decisions_used_entity_fkey FOREIGN KEY (used_entity) REFERENCES public.term_versions(id);


--
-- Name: orchestration_decisions orchestration_decisions_was_associated_with_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.orchestration_decisions
    ADD CONSTRAINT orchestration_decisions_was_associated_with_fkey FOREIGN KEY (was_associated_with) REFERENCES public.analysis_agents(id);


--
-- Name: orchestration_feedback orchestration_feedback_orchestration_decision_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.orchestration_feedback
    ADD CONSTRAINT orchestration_feedback_orchestration_decision_id_fkey FOREIGN KEY (orchestration_decision_id) REFERENCES public.orchestration_decisions(id) ON DELETE CASCADE;


--
-- Name: orchestration_feedback orchestration_feedback_researcher_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.orchestration_feedback
    ADD CONSTRAINT orchestration_feedback_researcher_id_fkey FOREIGN KEY (researcher_id) REFERENCES public.users(id);


--
-- Name: orchestration_overrides orchestration_overrides_orchestration_decision_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.orchestration_overrides
    ADD CONSTRAINT orchestration_overrides_orchestration_decision_id_fkey FOREIGN KEY (orchestration_decision_id) REFERENCES public.orchestration_decisions(id) ON DELETE CASCADE;


--
-- Name: orchestration_overrides orchestration_overrides_researcher_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.orchestration_overrides
    ADD CONSTRAINT orchestration_overrides_researcher_id_fkey FOREIGN KEY (researcher_id) REFERENCES public.users(id);


--
-- Name: processing_artifact_groups processing_artifact_groups_document_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.processing_artifact_groups
    ADD CONSTRAINT processing_artifact_groups_document_id_fkey FOREIGN KEY (document_id) REFERENCES public.documents(id);


--
-- Name: processing_artifact_groups processing_artifact_groups_processing_job_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.processing_artifact_groups
    ADD CONSTRAINT processing_artifact_groups_processing_job_id_fkey FOREIGN KEY (processing_job_id) REFERENCES public.processing_jobs(id);


--
-- Name: processing_artifacts processing_artifacts_document_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.processing_artifacts
    ADD CONSTRAINT processing_artifacts_document_id_fkey FOREIGN KEY (document_id) REFERENCES public.documents(id);


--
-- Name: processing_artifacts processing_artifacts_processing_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.processing_artifacts
    ADD CONSTRAINT processing_artifacts_processing_id_fkey FOREIGN KEY (processing_id) REFERENCES public.experiment_document_processing(id);


--
-- Name: processing_jobs processing_jobs_document_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.processing_jobs
    ADD CONSTRAINT processing_jobs_document_id_fkey FOREIGN KEY (document_id) REFERENCES public.documents(id);


--
-- Name: processing_jobs processing_jobs_parent_job_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.processing_jobs
    ADD CONSTRAINT processing_jobs_parent_job_id_fkey FOREIGN KEY (parent_job_id) REFERENCES public.processing_jobs(id);


--
-- Name: processing_jobs processing_jobs_user_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.processing_jobs
    ADD CONSTRAINT processing_jobs_user_id_fkey FOREIGN KEY (user_id) REFERENCES public.users(id);


--
-- Name: prov_activities prov_activities_wasassociatedwith_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.prov_activities
    ADD CONSTRAINT prov_activities_wasassociatedwith_fkey FOREIGN KEY (wasassociatedwith) REFERENCES public.prov_agents(agent_id);


--
-- Name: prov_entities prov_entities_wasattributedto_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.prov_entities
    ADD CONSTRAINT prov_entities_wasattributedto_fkey FOREIGN KEY (wasattributedto) REFERENCES public.prov_agents(agent_id);


--
-- Name: prov_entities prov_entities_wasderivedfrom_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.prov_entities
    ADD CONSTRAINT prov_entities_wasderivedfrom_fkey FOREIGN KEY (wasderivedfrom) REFERENCES public.prov_entities(entity_id);


--
-- Name: prov_entities prov_entities_wasgeneratedby_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.prov_entities
    ADD CONSTRAINT prov_entities_wasgeneratedby_fkey FOREIGN KEY (wasgeneratedby) REFERENCES public.prov_activities(activity_id);


--
-- Name: provenance_chains provenance_chains_derivation_activity_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.provenance_chains
    ADD CONSTRAINT provenance_chains_derivation_activity_fkey FOREIGN KEY (derivation_activity) REFERENCES public.semantic_drift_activities(id);


--
-- Name: semantic_drift_activities semantic_drift_activities_created_by_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.semantic_drift_activities
    ADD CONSTRAINT semantic_drift_activities_created_by_fkey FOREIGN KEY (created_by) REFERENCES public.users(id);


--
-- Name: semantic_drift_activities semantic_drift_activities_generated_entity_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.semantic_drift_activities
    ADD CONSTRAINT semantic_drift_activities_generated_entity_fkey FOREIGN KEY (generated_entity) REFERENCES public.term_versions(id);


--
-- Name: semantic_drift_activities semantic_drift_activities_used_entity_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.semantic_drift_activities
    ADD CONSTRAINT semantic_drift_activities_used_entity_fkey FOREIGN KEY (used_entity) REFERENCES public.term_versions(id);


--
-- Name: semantic_drift_activities semantic_drift_activities_was_associated_with_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.semantic_drift_activities
    ADD CONSTRAINT semantic_drift_activities_was_associated_with_fkey FOREIGN KEY (was_associated_with) REFERENCES public.analysis_agents(id);


--
-- Name: semantic_shift_analysis semantic_shift_analysis_experiment_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.semantic_shift_analysis
    ADD CONSTRAINT semantic_shift_analysis_experiment_id_fkey FOREIGN KEY (experiment_id) REFERENCES public.experiments(id) ON DELETE CASCADE;


--
-- Name: semantic_shift_analysis semantic_shift_analysis_from_definition_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.semantic_shift_analysis
    ADD CONSTRAINT semantic_shift_analysis_from_definition_id_fkey FOREIGN KEY (from_definition_id) REFERENCES public.term_disciplinary_definitions(id) ON DELETE SET NULL;


--
-- Name: semantic_shift_analysis semantic_shift_analysis_from_document_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.semantic_shift_analysis
    ADD CONSTRAINT semantic_shift_analysis_from_document_id_fkey FOREIGN KEY (from_document_id) REFERENCES public.documents(id) ON DELETE SET NULL;


--
-- Name: semantic_shift_analysis semantic_shift_analysis_term_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.semantic_shift_analysis
    ADD CONSTRAINT semantic_shift_analysis_term_id_fkey FOREIGN KEY (term_id) REFERENCES public.terms(id) ON DELETE CASCADE;


--
-- Name: semantic_shift_analysis semantic_shift_analysis_to_definition_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.semantic_shift_analysis
    ADD CONSTRAINT semantic_shift_analysis_to_definition_id_fkey FOREIGN KEY (to_definition_id) REFERENCES public.term_disciplinary_definitions(id) ON DELETE SET NULL;


--
-- Name: semantic_shift_analysis semantic_shift_analysis_to_document_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.semantic_shift_analysis
    ADD CONSTRAINT semantic_shift_analysis_to_document_id_fkey FOREIGN KEY (to_document_id) REFERENCES public.documents(id) ON DELETE SET NULL;


--
-- Name: term_disciplinary_definitions term_disciplinary_definitions_document_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.term_disciplinary_definitions
    ADD CONSTRAINT term_disciplinary_definitions_document_id_fkey FOREIGN KEY (document_id) REFERENCES public.documents(id) ON DELETE SET NULL;


--
-- Name: term_disciplinary_definitions term_disciplinary_definitions_experiment_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.term_disciplinary_definitions
    ADD CONSTRAINT term_disciplinary_definitions_experiment_id_fkey FOREIGN KEY (experiment_id) REFERENCES public.experiments(id) ON DELETE CASCADE;


--
-- Name: term_disciplinary_definitions term_disciplinary_definitions_term_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.term_disciplinary_definitions
    ADD CONSTRAINT term_disciplinary_definitions_term_id_fkey FOREIGN KEY (term_id) REFERENCES public.terms(id) ON DELETE CASCADE;


--
-- Name: term_version_anchors term_version_anchors_context_anchor_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.term_version_anchors
    ADD CONSTRAINT term_version_anchors_context_anchor_id_fkey FOREIGN KEY (context_anchor_id) REFERENCES public.context_anchors(id);


--
-- Name: term_version_anchors term_version_anchors_term_version_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.term_version_anchors
    ADD CONSTRAINT term_version_anchors_term_version_id_fkey FOREIGN KEY (term_version_id) REFERENCES public.term_versions(id);


--
-- Name: term_versions term_versions_created_by_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.term_versions
    ADD CONSTRAINT term_versions_created_by_fkey FOREIGN KEY (created_by) REFERENCES public.users(id);


--
-- Name: term_versions term_versions_term_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.term_versions
    ADD CONSTRAINT term_versions_term_id_fkey FOREIGN KEY (term_id) REFERENCES public.terms(id);


--
-- Name: term_versions term_versions_was_derived_from_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.term_versions
    ADD CONSTRAINT term_versions_was_derived_from_fkey FOREIGN KEY (was_derived_from) REFERENCES public.term_versions(id);


--
-- Name: terms terms_created_by_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.terms
    ADD CONSTRAINT terms_created_by_fkey FOREIGN KEY (created_by) REFERENCES public.users(id);


--
-- Name: terms terms_updated_by_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.terms
    ADD CONSTRAINT terms_updated_by_fkey FOREIGN KEY (updated_by) REFERENCES public.users(id);


--
-- Name: text_segments text_segments_document_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.text_segments
    ADD CONSTRAINT text_segments_document_id_fkey FOREIGN KEY (document_id) REFERENCES public.documents(id);


--
-- Name: text_segments text_segments_group_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.text_segments
    ADD CONSTRAINT text_segments_group_id_fkey FOREIGN KEY (group_id) REFERENCES public.processing_artifact_groups(id);


--
-- Name: text_segments text_segments_parent_segment_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.text_segments
    ADD CONSTRAINT text_segments_parent_segment_id_fkey FOREIGN KEY (parent_segment_id) REFERENCES public.text_segments(id);


--
-- Name: text_segments text_segments_segmentation_job_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.text_segments
    ADD CONSTRAINT text_segments_segmentation_job_id_fkey FOREIGN KEY (segmentation_job_id) REFERENCES public.processing_jobs(id) ON DELETE SET NULL;


--
-- Name: tool_execution_logs tool_execution_logs_orchestration_decision_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.tool_execution_logs
    ADD CONSTRAINT tool_execution_logs_orchestration_decision_id_fkey FOREIGN KEY (orchestration_decision_id) REFERENCES public.orchestration_decisions(id) ON DELETE CASCADE;


--
-- Name: version_changelog version_changelog_created_by_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.version_changelog
    ADD CONSTRAINT version_changelog_created_by_fkey FOREIGN KEY (created_by) REFERENCES public.users(id);


--
-- Name: version_changelog version_changelog_document_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: ontextract_user
--

ALTER TABLE ONLY public.version_changelog
    ADD CONSTRAINT version_changelog_document_id_fkey FOREIGN KEY (document_id) REFERENCES public.documents(id) ON DELETE CASCADE;


--
-- Name: SCHEMA public; Type: ACL; Schema: -; Owner: pg_database_owner
--

GRANT ALL ON SCHEMA public TO ontextract_user;


--
-- Name: TABLE app_settings; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON TABLE public.app_settings TO ontextract_user;


--
-- Name: SEQUENCE app_settings_id_seq; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON SEQUENCE public.app_settings_id_seq TO ontextract_user;


--
-- Name: TABLE document_processing_index; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON TABLE public.document_processing_index TO ontextract_user;


--
-- Name: TABLE document_temporal_metadata; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON TABLE public.document_temporal_metadata TO ontextract_user;


--
-- Name: TABLE documents; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON TABLE public.documents TO ontextract_user;


--
-- Name: TABLE experiments; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON TABLE public.experiments TO ontextract_user;


--
-- Name: TABLE document_version_chains; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON TABLE public.document_version_chains TO ontextract_user;


--
-- Name: TABLE text_segments; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON TABLE public.text_segments TO ontextract_user;


--
-- Name: SEQUENCE documents_id_seq; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON SEQUENCE public.documents_id_seq TO ontextract_user;


--
-- Name: TABLE experiment_document_processing; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON TABLE public.experiment_document_processing TO ontextract_user;


--
-- Name: TABLE experiment_documents; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON TABLE public.experiment_documents TO ontextract_user;


--
-- Name: TABLE experiment_orchestration_runs; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON TABLE public.experiment_orchestration_runs TO ontextract_user;


--
-- Name: TABLE experiment_references; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON TABLE public.experiment_references TO ontextract_user;


--
-- Name: SEQUENCE experiments_id_seq; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON SEQUENCE public.experiments_id_seq TO ontextract_user;


--
-- Name: TABLE extracted_entities; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON TABLE public.extracted_entities TO ontextract_user;


--
-- Name: SEQUENCE extracted_entities_id_seq; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON SEQUENCE public.extracted_entities_id_seq TO ontextract_user;


--
-- Name: TABLE oed_timeline_markers; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON TABLE public.oed_timeline_markers TO ontextract_user;


--
-- Name: TABLE ontology_mappings; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON TABLE public.ontology_mappings TO ontextract_user;


--
-- Name: SEQUENCE ontology_mappings_id_seq; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON SEQUENCE public.ontology_mappings_id_seq TO ontextract_user;


--
-- Name: TABLE processing_artifact_groups; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON TABLE public.processing_artifact_groups TO ontextract_user;


--
-- Name: SEQUENCE processing_artifact_groups_id_seq; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON SEQUENCE public.processing_artifact_groups_id_seq TO ontextract_user;


--
-- Name: TABLE processing_artifacts; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON TABLE public.processing_artifacts TO ontextract_user;


--
-- Name: TABLE processing_jobs; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON TABLE public.processing_jobs TO ontextract_user;


--
-- Name: SEQUENCE processing_jobs_id_seq; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON SEQUENCE public.processing_jobs_id_seq TO ontextract_user;


--
-- Name: TABLE prompt_templates; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON TABLE public.prompt_templates TO ontextract_user;


--
-- Name: SEQUENCE prompt_templates_id_seq; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON SEQUENCE public.prompt_templates_id_seq TO ontextract_user;


--
-- Name: TABLE provenance_activities; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON TABLE public.provenance_activities TO ontextract_user;


--
-- Name: SEQUENCE provenance_activities_id_seq; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON SEQUENCE public.provenance_activities_id_seq TO ontextract_user;


--
-- Name: TABLE provenance_entities; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON TABLE public.provenance_entities TO ontextract_user;


--
-- Name: SEQUENCE provenance_entities_id_seq; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON SEQUENCE public.provenance_entities_id_seq TO ontextract_user;


--
-- Name: TABLE semantic_shift_analysis; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON TABLE public.semantic_shift_analysis TO ontextract_user;


--
-- Name: TABLE term_disciplinary_definitions; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON TABLE public.term_disciplinary_definitions TO ontextract_user;


--
-- Name: SEQUENCE text_segments_id_seq; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON SEQUENCE public.text_segments_id_seq TO ontextract_user;


--
-- Name: TABLE users; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON TABLE public.users TO ontextract_user;


--
-- Name: SEQUENCE users_id_seq; Type: ACL; Schema: public; Owner: postgres
--

GRANT ALL ON SEQUENCE public.users_id_seq TO ontextract_user;


--
-- Name: DEFAULT PRIVILEGES FOR SEQUENCES; Type: DEFAULT ACL; Schema: public; Owner: postgres
--

ALTER DEFAULT PRIVILEGES FOR ROLE postgres IN SCHEMA public GRANT ALL ON SEQUENCES TO ontextract_user;


--
-- Name: DEFAULT PRIVILEGES FOR TABLES; Type: DEFAULT ACL; Schema: public; Owner: postgres
--

ALTER DEFAULT PRIVILEGES FOR ROLE postgres IN SCHEMA public GRANT ALL ON TABLES TO ontextract_user;


--
-- PostgreSQL database dump complete
--

\unrestrict DoZfftHuLcZl42Z2BEYmR0jEblCPc5geIueVggwcJiQtTkSj1x2Sj6S8kbKW1GB

